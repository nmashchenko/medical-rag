[
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 0,
    "text": "Medical follow - up optimization : A Monte - Carlo planning strategy∗ Benoˆıte de Saporta† Aymar Thierry d’Argenlieu‡ R´egis Sabbadin§ Alice Cleynen¶ Abstract Designing patient - specific follow - up strategy is a crucial step towards personalized medicine in cancer . Tools to help doctors deciding on treatment allocation together with next visit date , based on patient preferences and medical observations , would be particularly beneficial . Such tools should be based on realistic models of disease progress under the impact of medical treatments , involve the design of ( multi-)objective functions that a treatment strategy should optimize along the patient ’s medical journey , and include efficient resolution algorithms to optimize personalized follow - up by taking the patient ’s history and preferences into account . We propose to model cancer evolution with a Piecewise Deterministic Markov Process where patients alternate between remission and relapse phases with disease - specific tumor evolution . This model is controlled via the online optimization of a long - term cost function accounting for treatment side - effects , hospital visits burden and disease impact on the quality of life . Op- timization is based on noisy measurements of blood markers at visit dates . This optimization problem is extremely difficult . It has recently been modeled as an infinite dimensional con- tinuous space Markov Decision Process , approximated by a discrete - space problem in order to be solved exactly . Here , instead , we leverage the Partially - Observed Monte - Carlo Planning algorithm to solve the full continuous - time , continuous - state problem , taking advantage of the nearly - deterministic nature of cancer evolution . We show that this approximate solution ap- proach of the exact model performs better than the counterpart exact resolution of the discrete model , while allowing for more versatility in the cost function model , hence a patient - specific follow - up . Our findings in terms of modeling and our efficient simulation - based optimisation approach to produce follow - up strategies can efficiently and easily be adapted to a large number of other diseases , thus being useful to doctors and patients . ∗We acknowledge the support of the French Agence Nationale de la Recherche ( ANR ) , under grant ANR-21 - CE40- 005 ( project HSMM - INCA ) , of European Union ’s Horizon 2020 research and innovation program ( Marie Sklodowska- Curie grant agreement No 890462 ) and the support of MESO@LR - Platform at the University of Montpellier . †IMAG , Univ Montpellier , CNRS , Montpellier , France ‡IMAG , Univ Montpellier , CNRS , Montpellier , France and IP Paris , Palaiseau , France § Univ Toulouse , INRAE - MIAT , Toulouse , France ¶IMAG , Univ Montpellier , CNRS , Montpellier , France and John Curtin School of Medical Research , Australian National University , Canberra , ACT , Australia arXiv:2401.03972v1 [ math . OC ] 8 Jan"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 1,
    "text": ", Palaiseau , France § Univ Toulouse , INRAE - MIAT , Toulouse , France ¶IMAG , Univ Montpellier , CNRS , Montpellier , France and John Curtin School of Medical Research , Australian National University , Canberra , ACT , Australia arXiv:2401.03972v1 [ math . OC ] 8 Jan 2024 \n Contents Introduction Results 2.1 Controlled piecewise deterministic Markov processes form a universal class of versa- tile models for patient follow - up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Cost functions encode the diverse impacts of treatment on the patient ’s quality of life 2.3 Adapted Partially Observed Monte - Carlo Planning is particulartly well suited for controlled PDMPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Following up a patient with adapted POMCP is easy and fast in practice . . . . . . 2.5 Adapted POMCP can be tuned to outperform dynamic programming . . . . . . . . 2.5.1 Study 1 : Impact of the parameters ’ values on POMCP ’s perfomance . . . . . 2.5.2 Study 2 : Adapted POMCP outperforms the dynamic programming approach Discussion Methods 4.1 Datasets , parameters and code availability . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Reminder on controlled PDMPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Reminder on Partially Observed Monte - Carlo Planning . . . . . . . . . . . . . . . . 4.4 Adapted POMCP algorithm to the case of controlled PDMPs . . . . . . . . . . . . . Supporting information 5.1 State of the art . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Supplementary simulation results on POMCP parameters . . . . . . . . . . . . . . . \n Introduction In long - term diseases such as cancer , patients alternate between remission and relapse phases and are monitored along time through non - invasive check - ups such as blood samples [ 24 , 42 ] . Based on these noisy indirect disease measurements of some markers , practitioners must decide on treat- ment allocation , sometimes with little knowledge on the process dynamics ( e.g. aggressiveness of the relapse ) which may"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 2,
    "text": "invasive check - ups such as blood samples [ 24 , 42 ] . Based on these noisy indirect disease measurements of some markers , practitioners must decide on treat- ment allocation , sometimes with little knowledge on the process dynamics ( e.g. aggressiveness of the relapse ) which may differ between patients [ 31 , 39 , 40 ] . Long retrospect of medical practice has allowed the definition of milestones to help practitioners in making follow - up decisions , but automated personalized criteria are yet to be defined to improve individual patient follow - up . The ability to monitor patients in the least invasive manner , according to their personal pref- erences ( more check - ups to enforce relapse detection , less aggressive treatments for better quality of life , etc ) is a crucial step towards better care , but requires fine knowledge of diseases dynamics and reliable prediction algorithms . One of the main requirements for such task is the definition of a universal model adapted to patient - specific parameters that could describe in an exhaustive manner the possible consequences of the practitioner ’s decisions . Mathematical models have been developed to link the tumor markers to tumor sizes [ 30 ] , or to predict evolution of tumor growth from initial measurements [ 32,45 ] , but online adaptive models predicting relapses and automating treatment strategies are still lacking . In particular , such a model should be able to reconcile the continuous time evolution of the disease , continuous values for the markers leading to any possible values within a given range , and the noisy observations at discrete visit dates . A good candidate is the class of Piecewise Deterministic Markov Process ( PDMP ) [ 12,13,37 ] . Indeed , PDMPs are non diffusive hybrid stochastic processes that can handle both continuous and discrete variables and their interactions in continuous time . The only source of stochasticity comes from the jumps of the process . They are thus simple to simulate and easy to interpret . Controlled PDMPs allow contin- uous time dynamics on continuous ( or hybrid discrete and continuous ) state spaces with decisions taken in continuous time [ 14,15 ] . This paper is based on the analysis of a large cohort of Multiple Myeloma ( MM ) patient data from the Intergroupe Francophone du My´elome ( IFM ) 2009 clinical trial [ 2 ] . We assume that there is a single cancer marker that remains at a nominal threshold ζ0 throughout any remission phase , and that at patient relapse its level increases exponentially with multiple possible behaviors until treatment is administered , or a threshold D is reached and the patient dies . To set up the context , we will assume that the study begins at time t0 = 0 when the patient enters her first remission state , and we will denote t1 , t2 , . . . ,"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 3,
    "text": "or a threshold D is reached and the patient dies . To set up the context , we will assume that the study begins at time t0 = 0 when the patient enters her first remission state , and we will denote t1 , t2 , . . . , her visit dates , defined over time by the practitioner , until some time horizon H is reached , or the patient dies . The time lapse between visits may not be constant , so that different patients may have different visit numbers and dates . More precisely , we will assume that at each visit time , the practitioner may choose to schedule the next visit in either 15 , 30 or 60 days . Such decision may be based on the previous and current marker measurements , which we denote Y0 , Y1 , . . . . Note that the exact value of the marker is hidden as measurements are corrupted by noise , and measurements are only collected at visit dates . Together with the next visit date , the practitioner may chose to modify the patient ’s current treatment , fixing it to one of the two available treatments , a and b , or to no treatment at all , denoted ∅. An example of patient follow - up data is presented in Figure 1 a ) . We model the underlying continuous - time dynamics of the patient health by a controlled PDMP ( Xt)0≤t≤H [ 1 , 14 , 15 , 17 , 18 , 35 ] . We propose to optimally control the process , that is to choose online the next treatment and visit date , based on present and past observations and decisions by minimising a cost function which is calibrated to balance the burden of deteriorated quality of life \n * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Time ( days ) Marker value Marker measurements relapse remission H treatment D risk λ disease aggressiveness v treatment efficiency v′ Example of patient follow - up data , PDMP model . a ) Marker values are measured at each patient visits over a certain period of time . Data from the Intergroupe Francophone du My´elome 2009 clinical trial , courtesy of the Centre de Recherche en Canc´erologie de Toulouse . b ) PDMP model , representation of the marker level of a patient . The risk function λ controls the time to relapse , while parameters v and v′ control the aggressiveness of the disease and the efficiency of the treatment respectively . under treatment ( including hospital visits ) with the risk of dying from the disease . Previous work has focused on discretizing this problem in order to solve"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 4,
    "text": "to relapse , while parameters v and v′ control the aggressiveness of the disease and the efficiency of the treatment respectively . under treatment ( including hospital visits ) with the risk of dying from the disease . Previous work has focused on discretizing this problem in order to solve it approximately through Dynamic Programming ( DP ) iterations [ 10 ] . More specifically , the optimal control problem for the PDMP has first been expressed as a Partially Observed Markov Decision Process ( POMDP ) [ 3 ] . This step is simply done by considering decision dates as stages of the POMDP . Note that the time lapse between decisions is thus not constant , the continuous time dynamics is encoded in the specific parametrization of the transition kernel , and the POMDP still has a continuous state space , with continuous observation space . The problem is then classically converted into a fully observed Markov Decision Process ( MDP ) [ 3 , 9 ] on the belief or filter space . The filter process represents the probability distribution of the hidden values of the patient current state given the past and present observations . Second , the state space of the controlled PDMP has been discretized , so that an approximation of the filter process could be computed , charging only finitely many states . This approximate filter is called conditional filter in the sequel . Third , the belief space has been discretized in order to solve the MDP via dynamic programming iterations on a finite space . In the current article , instead of discretizing the state and belief spaces , we use a Monte - Carlo Tree Search approach to ( approximately ) solve the controlled PDMP problem by simulation . More precisely , we propose an adaptation of the Partially Observed Monte - Carlo Planning ( POMCP ) al- gorithm [ 41 ] , originally designed to solve discrete time / finite state and observation spaces POMDP , to the case of controlled PDMP . The novel challenge is that controlled PDMP involve continuous time as well as continuous state and observation spaces . We show empirically that this simulation- based approach outperforms the discretization - based approach both in terms of computation time and quality of returned policies . Thus , this approach is promising for providing an automated decision aiding tool for practitioners . To our knowledge , this is the first method to address this challenging question . Current decision making is typically based on heuristic rules derived from expert clinical knowledge [ 21,29,43 ] . \n Results 2.1 Controlled piecewise deterministic Markov processes form a univer- sal class of versatile models for patient follow - up Despite the discrete - time acquisition of the marker measurements , we choose to model the dynamics of the patient ’s health by a continuous - time controlled Piecewise Deterministic Markov Process ( PDMP ) . The formalism of PDMPs is"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 5,
    "text": "class of versatile models for patient follow - up Despite the discrete - time acquisition of the marker measurements , we choose to model the dynamics of the patient ’s health by a continuous - time controlled Piecewise Deterministic Markov Process ( PDMP ) . The formalism of PDMPs is both light and versatile [ 12,14,36,37 ] . It allows to describe the dynamics of the disease with only three biologically relevant parameters : the disease ’s risk function , λ , that dictates how often the patient is likely to relapse , the aggressiveness of the disease v that dictates how fast the marker level will increase during relapses , and the treatment efficiency v′ that dictates how fast the marker level decreases under treatment . In the formalism of PDMP , this is formulated by an exponential flow Φ which slope parameter ( v or v′ ) depends on patient condition and treatment , the risk function λ , and a transition kernel Q , that dictates how the patient ’s state evolves at relapses , here preventing the marker values from jumping abruptly at patient condition changes . This is illustrated in Fig 1 b ) . We consider a common risk function ( identical for all patients ) which we allow to depend on the time since the last remission date as well as the cancer marker level . We assume that the aggressiveness of the disease can be patient - dependent . It is either high or low and we model this as two different diseases . This leads us to introduce two different treatments , each efficient for one of those diseases and slowing the progression of the other . It is also an option not to treat for a given period . We introduce three variables m , ζ , u , where the mode m corresponds to the overall condition of the patient ( m = 0 : remission , m = 1 : disease 1 , m = 2 : disease 2 , m = 3 : death of the patient ) , ζ ∈[ζ0 , D ] is the level of the marker , where ζ0 is the nominal value and D the death level and u ≥0 is the time since the last change of overall condition ( added for technical reasons to deal with non - constant risk functions ) . The precise definition of the controlled PDMP and its parameters ( λ , Φ , Q ) are given in the Methods section . The complete state of the patient is thus encoded by s = ( m , ζ , u ) . We denote X0 , . . . , Xn the process values at the observation dates t0 , . . . , tn . The overall condition of the patient m , the level of the marker ζ and the relapse dates ( together with the time u since the last change of"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 6,
    "text": ", . . . , Xn the process values at the observation dates t0 , . . . , tn . The overall condition of the patient m , the level of the marker ζ and the relapse dates ( together with the time u since the last change of condition ) are not directly observed and thus can not be used by the clinician to select a treatment . At each visit of the patient to the medical center , we assume that the practitioner receives a noisy observation of the marker level y = ζ + ϵ where ϵ is some Gaussian noise . The practitioner also knows the time t since the beginning of the patient follow - up . The complete observation available to the practitioner is thus encoded by ω = ( y , t ) . The practitioner also has access to an indicator that the patient is still alive as treatment and follow - up stop at the death of the patient . Based on the collection of present and past measurements and decisions , the practitioner selects both a time delay r until the next visit to the medical center and a treatment ℓto hold until this next visit . Note that in our framework measurements are only made at visit dates . A decision is thus a pair d = ( ℓ , r ) , where ℓ∈{∅ , a , b } , and r ∈{15 , 30 , 60 } . Given a fixed arbitrary decision policy , simulating controlled patient trajectories is easy : see Algorithm 1 given in the Methods Section . \n 2.2 Cost functions encode the diverse impacts of treatment on the pa- tient ’s quality of life For the practitioner , controlling the disease is equivalent to choosing the best available treatment as well as the best next visit date in order to minimize its impact on the patient ’s quality of life along time . Defining the impact of treatment on the quality of life is a difficult task as it will typically depend on the treatment ’s side effects , the number of visits , the burden of living with a disease and the remaining life expectancy . This paper proposes a mathematical definition of the impact of the treatment on quality of life in terms of a cost function that takes into account those different aspects . For a decision d = ( ℓ , r ) comprising a treatment allocation ℓand a time to next visit r , and for a current marker level ζ at time tk , and future marker level ζ′ at time tk+1 = tk + r , we define c(ζ , d , ζ′ ) = CV + κ|ζ′ −ζ0|r + βr1{ζ = ζ0,ℓ̸=∅ } + M1{ζ′=D } , ( 1 ) where CV is a visit cost , κ is non - negative scale factor penalizing high marker values , β is a penalty"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 7,
    "text": "r , we define c(ζ , d , ζ′ ) = CV + κ|ζ′ −ζ0|r + βr1{ζ = ζ0,ℓ̸=∅ } + M1{ζ′=D } , ( 1 ) where CV is a visit cost , κ is non - negative scale factor penalizing high marker values , β is a penalty for applying an unnecessary treatment and M is the death cost . This cost function thus takes into account a visit cost , to prevent patients from undergoing too many screening tests , a cost depending on the marker value at the next visit , to encourage treatment and calibrate visit dates , a cost for degradation of quality of life due to treatments , in particular if they are not appropriate , and a cost for dying . Calibrating cost parameters CV , κ , β , and M is a very difficult task , which is allowed to be patient - dependent ( some patients may even express a wish to be sedated rather than undergo very long and painful treatments ) , and treatment strategies are bound to be parameters - dependent . When cast as a controlled PDMP with this cost function , the practitioner ’s problem is math- ematically equivalent to solving a ( continuous state space ) Partially Observable Markov Decision Process ( POMDP ) [ 10,25 ] , which expected value optimisation can be stated as V = inf π∈Π Eπ X0 \" Nπ−1 X n=0 c(Xn , dn , Xn+1 ) # , ( 2 ) where V is called the optimal policy value and represents the lowest possible expected total cost , Π is the set of admissible policies ( yielding decisions depending only on current and past observations ) , Nπ is the patient - specific number of visits within the time - horizon of the study when using policy π , dn = π(Y0 , t0 , . . . , Yn , tn ) is the decision ( ℓ , r ) taken at the n - th visit date tn according to policy π , and ( Yn)n≥0 represents the marker observation process for the controlled - PDMP / POMDP . Solving this problem amounts to computing ( a good approximation of ) the optimal policy value and identifying an admissible policy π∗that reaches ( a value close to ) the minimum . 2.3 Adapted Partially Observed Monte - Carlo Planning is particulartly well suited for controlled PDMPs The Partially Observed Monte - Carlo Planning ( POMCP ) algorithm [ 41 ] is an efficient simulation- based algorithm that has been designed for real - time planning in large finite state - space POMDPs . In this paper we show that even - though it has not been designed to handle continuous state and observation spaces , we can adapt it to solve controlled PDMPs , thanks to their efficient simula- tion property , without resorting to the computation of complex integrals for computing transition probabilities ."
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 8,
    "text": "paper we show that even - though it has not been designed to handle continuous state and observation spaces , we can adapt it to solve controlled PDMPs , thanks to their efficient simula- tion property , without resorting to the computation of complex integrals for computing transition probabilities . \n The objective of POMCP is to reduce the complexity of dynamic programming , which requires the construction of the entire decision tree ( including the probabilities of every possible outcome with every possible decision at every future time - point ) , by sampling the tree in a principled way so as to compute the current optimal action . POMCP is thus an online algorithm , which re - estimates the optimal strategy at each new data acquisition1 . The POMCP algorithm relies on two main properties . The first one is the ability to simulate trajectories , so as to progressively build the decision tree and update filters Θ at every intermediate node h of the tree . Recall that a filter is a probability distribution representing the ( approximate ) distribution of the current hidden state given the observations . The standard POMCP algorithm uses a specific family of simulation - based filters Θp called particle filters specified below . Filters are used to sample sets of plausible states . The second property is the requirement to provide estimates of the expected value of the policy in leaves of the current exploration tree , in order to guide exploration and build the decision policy . In the Methods Section we detail the algorithm ( Algorithm 2 ) and show that POMCP is par- ticularly well suited for controlled PDMPs . We simply point out here why trajectories simulation and policy evaluation are particularly efficient in POMCP , in the case of a controlled PDMP . 1 . Simulation is particularly straightforward with PDMPs [ 15,22,28 ] , requiring only to simulate the jump times and exploit the deterministic behavior between jumps , see Algorithm 1 . In our medical framework , it is made even more simple since only few jumps are allowed . When little knowledge is available about the underlying process , a classic approach is to resort to particle filters Θp [ 16 ] . A particle filter Θp at step n is a discrete uniform probability distribution with finite support Bp ( where Bp may have repeated atoms ) . It is updated at step n + 1 though simulations : states s from Bp are updated through a one - step simulation to a new state s′ , and selected to be added to Bp if the simulated observation is close to the true one . As an alternative filter to compare to , we propose to use a conditional filter Θc derived from the exact filter ( that is the conditional distribution of the hidden state given the observations ) from [ 10 ] . The exact conditional filter is updated"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 9,
    "text": "the true one . As an alternative filter to compare to , we propose to use a conditional filter Θc derived from the exact filter ( that is the conditional distribution of the hidden state given the observations ) from [ 10 ] . The exact conditional filter is updated through a recurrence formula involving ratios of integrals over the state space . By discretizing the state space , one can construct the approximation Θc of the exact filter . Unlike the particle filter that has a dynamically changing support with a uniform mass function , this conditional filter has a fixed support ( the discretized space ) with changing mass functions that are updated through analytical ratios of weighted sums . 2 . To estimate the future expected cost at some node of the tree , POMCP requires to simulate many full trajectories from the current node to a leaf of the tree . This requires to apply an arbitrary strategy to pick actions at every future nodes for which a decision has not yet been optimized . This arbitrary strategy is called a rollout strategy in the POMCP framework . The most naive rollout strategy consists in uniformly randomly selecting decisions from the decision set { ∅ , a , b } × { 15 , 30 , 60 } . We consider instead a mode - based rollout strategy , which consists in choosing action ∅in mode 0 ( no treatment if the simulated patient is in remission ) , action a in mode 1 and b in mode 2 ( most efficient treatment if the simulated patient has relapsed ) and a fixed next visit date of 15 days . This rollout strategy , while not being necessarily optimal ( depending on the cost function it might be optimal not to treat at the beginning of a relapse , or to treat preventively when in remission ) , exploits knowledge of the cost function , 1POMCP does not ” forget ” previously computed strategies , but updates them using new simulated samples after every new observation is received . \n hence yields better estimates of action costs at time t. Note also that this mode - based policy is not applicable for real patients , since their mode is not observed . It is only applicable to simulated patients . This is fine since POMCP ’s rollout strategy is only used through simulations to estimate costs . POMCP has a number of tuning parameters ( number of simulations , number of particles in the filter , exploration vs exploitation rates ) which are described ( as well as the impact of varying their values ) in the following section . 2.4 Following up a patient with adapted POMCP is easy and fast in practice The previous paragraphs set the grounds for optimizing the long - term follow up of patients . In practice , we will assume a patient will enter the follow - up study"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 10,
    "text": "the following section . 2.4 Following up a patient with adapted POMCP is easy and fast in practice The previous paragraphs set the grounds for optimizing the long - term follow up of patients . In practice , we will assume a patient will enter the follow - up study once she enters the remission phase after an initial round of treatment . The practitioner may hence assume that her current state is known , i.e. s0 = ( 0 , ζ0 , 0 ) and the initial value of both the particle and conditional filters is the Dirac mass at s0 . The initial observation is ω0 = ( ζ0 , t0 ) . The adapted POMCP algorithm is run to obtain the optimal decision d0 , which the practitioner can use ( if she decides to ) to allocate treatment and decide on the next visit date t1 . At visit n , the patient will come back for some new marker measurement , so that the n - th ob- servation value ωn = ( yn , tn ) is obtained . The practitioner will have access to her full history , hn = ⟨ω0d0ω1d1 · · · dn−1ωn⟩as well as her last belief filter , Θc n−1 or Θp n−1 . An initial update of the filter is performed , either using the recursion formula for Θc n from Θc n−1 and ωn , or by particle filtering through rejection sampling for Θp n from Θp n−1 and ωn . The adapted POMCP algorithm is then ran to obtain the optimal current decision dn , which the practitioner can use ( or not ) to allocate treatment and decide on the next visit date tn+1 . This is illustrated in Figure 2 . 2.5 Adapted POMCP can be tuned to outperform dynamic program- ming The simulation study presented in this section has been conducted based on real data obtained from the Centre de Recherche en Canc´erologie de Toulouse ( CRCT ) . Multiple myeloma ( MM ) is the second most common haematological malignancy in the world and is characterised by the accumulation of malignant plasma cells in the bone marrow . Classical treatments are based on chemotherapies , which , if appropriate , act fast and efficiently bring MM patients to remission in a few weeks . However almost all patients eventually relapse more than once and the five - year survival rate is around 50 % . We have obtained data from the Intergroupe Francophone du My´elome 2009 clinical trial [ 2 ] which has followed 748 French MM patients from diagnosis to their first relapse on a standardized protocol for up to six years . At each visit a blood sample was obtained to evaluate the amount of monoclonal immunoglobulin protein in the blood , a marker for the disease progression . An example of patient dataset is given in Figure 1 . Based on these data , we calibrated our PDMP model"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 11,
    "text": "years . At each visit a blood sample was obtained to evaluate the amount of monoclonal immunoglobulin protein in the blood , a marker for the disease progression . An example of patient dataset is given in Figure 1 . Based on these data , we calibrated our PDMP model as described in the Methods section , and we performed simulations to evaluate the performance of the POMCP strategy to select the com- bination of treatment and next visit date at each time point of the trajectories ( these time - points being themselves selected by the algorithm ) . The performance of the approach was measured by a Monte - Carlo estimate of the expectation and confidence interval of its value as well as the runtime \n a ) H y0 y1 y2 y3 y4 . . . . . . new marker acquisition ωn = ( yn , tn ) t0 t1 t2 t3 t4 . . . b ) ωn update ζ u ζ u Θn−1 Θn ( hn , Θn ) dn c ) hn ⟨N , V ⟩ hndn ⟨N , V ⟩ c rollout dn hndn ⟨N , V ⟩ hndn 2ωn+1 ⟨N , V ⟩ c rollout ωn+1 hndn 2ωn+1 ⟨N , V ⟩ hndn 2ωn+1 dn+1 ⟨N , V ⟩ c rollout dn+1 hndn 2ωn+1 dn+1 ⟨N , V ⟩ c rollout dn+1 ωn+1 dn Practice of patient follow - up . a ) At each new visit the patient has a new marker measurement , and the practitioner receives a new observation ωn = ( yn , tn ) . b ) The filter is updated with the new observation , either through particle rejection sampling ( particle filter ) or via a recursion formula ( conditional filter ) . c ) The decision tree is partially explored via simulation through an adapted POMCP algorithm using the updated filter . The algorithm returns the optimal decision dn , combination of a time to next visit ( defining tn+1 ) and treatment to allocate ( influencing yn+1 ) . \n of the online computation of a complete trajectory . For each disease parameters ’ configuration 500 simulations were performed to estimate these values . Codes and parameters are available at https://github.com/acleynen/pomcp4pdmp [ 11 ] . 2.5.1 Study 1 : Impact of the parameters ’ values on POMCP ’s perfomance We evaluated the impact of the value of 6 parameters on the performance : ( i ) the filter chosen ( conditional or particle ) , ( ii ) the rollout procedure chosen , ( iii ) the exploitation / exploration tradeoff parameter α′ , ( iv ) the number nsearch of simulations in the online POMCP procedure , ( v ) the number K of initial states to sample from at each of the nsearch simulations , and ( vi ) the internal POMCP precision parameter D to select particles in the particle filter . Those parameters are described at length in"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 12,
    "text": "in the online POMCP procedure , ( v ) the number K of initial states to sample from at each of the nsearch simulations , and ( vi ) the internal POMCP precision parameter D to select particles in the particle filter . Those parameters are described at length in the Methods Section , see Algorithm 2 . The github page [ 11 ] contains tables with results for every sets of parameters ’ values that were tested . In this section we describe the most important results . We performed all parameter comparisons for both the conditional and the particle filters . The conditional particle filter is a discrete probability distribution on a finite fixed support Bc of size 184 with 81 states in condition m = 0 , 31 states in condition m = 1 , 71 states in condition m = 2 and one state in condition m = 3 . The choice of these states is discussed in [ 10 ] . To adapt this filter to the POMCP environment , at each iteration n we start by randomly sampling K states s from Bc with the distribution given by Θc n. For the particle filter , this number K directly corresponds to the number of particles in the filter , hence the size of Bp . Note that for the conditional filter the support Bc does not change over time , whereas for the particle filter Bp keeps the same size but possibly contains different states at each iteration . Mode - based rollout outperforms naive rollout . We found that the uniform rollout procedure ( selecting decisions randomly ) produced very poor results compared to the mode - based rollout procedure and hence we only present results for the mode - based rollout policy here . POMCP is robust to the exploration / exploitation trade - off . We found that the exploita- tion / exploration trade - off parameter had little influence on the overall performance , with the ex- ception of extreme values ( α′ = 0.99 , almost no exploration , and α′ = 0.2 , almost no exploitation , both leading to poorer performance ) . We also tried several adaptive strategies to select the value of α′ depending on the confidence in the belief ( measured in terms of entropy ) which did not improve the results . The following results are therefore discussed for a fixed value of α′ = 0.5 , but the reader may refer to the Supplementary Information Tables 4 and 5 for additional results on these parameters . Increasing the number of exploratory simulations yields the best performance gain . For a fixed number of sampled belief states K and precision D , increasing the number of simulations nsearch improved the performance of the algorithm while decreasing the variance in the simulations for both filter types ( see the top left panel of Figure 3 for K = 500 , D = 0.01 )"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 13,
    "text": "of sampled belief states K and precision D , increasing the number of simulations nsearch improved the performance of the algorithm while decreasing the variance in the simulations for both filter types ( see the top left panel of Figure 3 for K = 500 , D = 0.01 ) . In the case of the conditional filter , the runtime increases linearly , from 103 seconds per trajectory with nsearch = 100 simulations to 104 seconds per trajectory with nsearch = 1000 . In the case of the particle filter , the runtime is 3 times as much for 100 simulations since when nsearch < K additional simulations have \n to be performed to compute the particle filter at time n + 1 . The difference decreases to only 1.2 times the runtime of the conditional filter for nsearch = 1000 . Filter conditional particle Number of simulations ( nsearch ) Value Number of simulations ( nsearch ) impact , K=500 Filter size ( K ) Value Filter size ( K ) impact , nsearch=500 0.001 0.01 0.1 0.2 0.5 Discretization precision Value Discretization impact , nsearch=500 , K=500 0.001 0.01 0.1 0.2 0.5 Discretization precision Value Discretization impact , nsearch=1000 , K=500 Top left : increasing the number of simulations for filters with 500 initial states improves the average trajectory costs . Top right : increasing the number of atoms in the filter improves the performance of the particle filter but not the conditional filter . The particle filter requires a large belief state to achieve high performance . As ex- pected , for a fixed number of exploratory simulations nsearch and precision D , increasing the number K of particles in the particle filter led to a tremendous improvement for the particle filter together with a significantly decreased variance in the trajectory costs , while it had no impact on the con- ditional filter ( see top right panel of Figure 3 for nsearch = 500 , D = 0.01 ) . Similarly , K has no runtime impact for the conditional filter , while it leads to exponential increase of the runtime for the particle filter . POMCP for controlled PDMPs requires one additionnal tuning : the precision of the tree observation nodes . The bottom two panels of Figure 3 illustrate the impact of the preci- sion D for a fixed filter size ( K = 500 ) and two different numbers of simulations ( nsearch = 500 , left , and nsearch = 1000 , right ) . For a smaller number of simulations , decreasing the precision improves the result up to D = 0.1 , and then worsens them again . This tendency is still observed for the particle filter when nsearch increases , while the performance of the conditional filter is optimal with the loosest precision , D = 1 . Those results are the consequence of two factors : as detailed in the \n Methods Section , each simulation creates"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 14,
    "text": "tendency is still observed for the particle filter when nsearch increases , while the performance of the conditional filter is optimal with the loosest precision , D = 1 . Those results are the consequence of two factors : as detailed in the \n Methods Section , each simulation creates a novel node in the tree exploration , where a node is a set of potential future trajectories with their estimated costs . When the precision is very fine , each simulation produces a different future observation and hence the estimation relies on one - step forward simulations which may miss future events . On the other hand , when the precision is very loose , each simulation will build on the previous simulation to explore a step forward , yielding very long trees with few branches . This will also miss the variability of different outcomes . The second factor comes from the way the filters are constructed . Particle filters rely on comparing simulations to observations . When the precision is very loose , almost all simulations will be accepted , creating a strong bias in the belief of the current state , that will propagate from time - point to time - point . As the conditional filter update does not rely on simulations , hence neither on precision , there is no propagation of uncertainty from step to step , and when the number of exploratory simulations nsearch is large enough to guarantee some diversity in the tree exploration , estimating the cost of each decision from longer trajectories will provide better results . Finally , one may note that the gain in using conditional filters is mostly apparent in extreme parameter scenarios , for instance with very low number of particles K , with very high precision rates , etc . Provided the user has enough computing budget , both filters tend to provide very similar results . 2.5.2 Study 2 : Adapted POMCP outperforms the dynamic programming approach In this study we compare the results of three resolution strategies calibrated with their optimal parameters on biological relevant outcomes : the death rate , the Progression - Free Survival ( PFS ) time , that is the time from entry in the study to the first relapse , the time spent under treatment , the number of visits to the hospital , and the cost . Those quantities were normalized so that they range between 0 and 1 , and such that an optimal result is 0 . To do so , death rate was normalized so that a random treatment strategy yields 1 ( here 5 % of patients ) ; the PFS was transformed as 1-(PFS / H ) ( where we recall that H is the study horizon ) , so that a patient who does not relapse has normalized PFS equal to 0 ; the time spent under treatment was normalized by H , the number of visits was normalized as"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 15,
    "text": "transformed as 1-(PFS / H ) ( where we recall that H is the study horizon ) , so that a patient who does not relapse has normalized PFS equal to 0 ; the time spent under treatment was normalized by H , the number of visits was normalized as NVisit−40 160−40 since over the horizon , a visit every 15 days produces 160 visits , whereas a visit every 60 days produces 40 visits ; and finally the cost was normalized as C−v0 Crandom−v0 where v0 is the the best approximation of the optimal value obtained through discretizations in [ 10 ] , and Crandom is the average cost of the random strategy . Here again , we simulated 500 trajectories with each strategy under the same cost parameters . The results are summarized in the Radar plot of Figure 4 , and additional visual information on average trajectory cost are given in the barplots . In the Radar plot representation , a perfect strategy should delimitate the inner circle . Compared strategies are the discretization / DP approach ( DP ) from [ 10 ] that relies on exact resolution by dynamic programming of the discretized POMPD , the adapted POMCP with the conditional filter ( POMCP - Conditional ) and the adapted POMCP with the particle filter ( POMCP- Particles ) . Interestingly , the combination of POMCP with the conditional filter yields the lowest average trajectory cost and the shortest average time spent under treatment , by slightly increasing the number of visits and reducing PFS compared to the DP approach . However , the particle - based POMCP approach ( relying fully on simulations with no other exploitation of the underlying model ) yields cost almost as good as the previous two approaches , with increased number of visits but longest progression - free survival time . Importantly , out of 500 simulations , one of the trajectories \n ended with a patient dying . Nb . Visit cost DeathRate 1−PFS timeTreat DP POMCP−Conditional POMCP−Particles Random DP POMCP−Conditional POMCP−Particles Trajectory cost free survival ( PFS ) , time spent under treatment , average number of visits per patient , and average trajectory cost . An optimal strategy would be the inner - circle . b ) Barplot of trajectory cost for 500 simulations under three main strategies : POMCP with particle filter , POMCP with conditional filter and Dynamic Programming on discretized processes . Discussion In this paper , a mechanistic PDMP model for cancer evolution and treatment has been presented . PDMPs are very flexible tools that allow to model routine screening data with very few parameters with biological meaning . When embedded in a control framework , PDMPs usually suffer from the need to compute intractable integrals and thus resort to several layers of approximations with heavy computational burden . In our case , there are two major difficulties in solving the optimization problem for the controlled PDMP . The first one is"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 16,
    "text": "a control framework , PDMPs usually suffer from the need to compute intractable integrals and thus resort to several layers of approximations with heavy computational burden . In our case , there are two major difficulties in solving the optimization problem for the controlled PDMP . The first one is related to the partial observations of the process , since the practitioner only observes some noisy measurement of the marker at visit dates , and not the overall condition of the patient nor the relapse dates . The second one comes from the continuous state space and continuous time dynamics of the process , which prevent direct use of exhaustive exploration solution strategies such as dynamic programming [ 6 ] . In a previous work [ 10 ] , we have dealt with those difficulties by defining an equivalent fully observable Markov decision process on an enlarged state space through the use of conditional filters , the conditional distributions of the hidden process given the observations . Then we discretized the state space of the original process , in order to obtain finite support filters and discretized again these finite support approximate filters to obtain finitely many ( belief ) states . The fully discretized model can then be solved by dynamic programming . Here we investigated another original solution approach exploiting filter objects under a different ( simulation - based ) dimension reduction strategy . We show that the inherent generator function of the PDMP can be exploited to make use of simulation - based solution strategies such as POMCP \n with excellent performance . Provided the number of simulations is large enough ( either to explore the outcome space , or to construct consistent belief states ) , this approach can even outperform dis- cretization approaches that exploit the knowledge of the underlying model . We have also proposed to combine both approaches , using discretization based conditional filters and simulation based solution stategy , resulting in a more robust algorithm ( in particular less sensitive to the choice of POMCP parameters and with more stable variance ) , but with little performance gain . The main advantage of the discretization / DP approach is that solutions are pre - computed for all new patients . This is especially useful under the assumption that all patients have the same dynamics with the same parameters . Its main drawback is that the model presented here is at maximum complexity for such an approach . In particular , it will become intractable if one wants to take into account more disease markers or more modes and treatments . Conversely , the simulation - based approach can extend to any complexity provided simulations can be performed easily and fast . In addition , one of the main advantages of simulation - based ap- proaches such as that presented here is that cost parameters can be modified with each new patient , yielding a patient - based procedure closer to precision medicine"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 17,
    "text": "provided simulations can be performed easily and fast . In addition , one of the main advantages of simulation - based ap- proaches such as that presented here is that cost parameters can be modified with each new patient , yielding a patient - based procedure closer to precision medicine . This remains quite theoretical , as in practice calibrating cost parameters is a very difficult task , but with experience practitioners may be able to encode personal preferences , such as shorter life with better quality , or longer life at the price of more treatments , etc . In this work , we have adapted the POMCP algorithm to solving a controlled PDMP with known model , too complex to be solved through exact dynamic programming . We made the assumption that the patient - disease model was known , which is a daring assumption . Therefore , one next step of our approach is to extend it by considering an unknown model and applying Reinforcement Learning methods [ 44 ] . A fundamental problem in Reinforcement Learning is the difficulty of deciding whether to select actions in order to learn a better model of the environment , or to exploit current knowledge about the rewards and effects of actions [ 26 ] . This is especially true in disease control problems . Methods 4.1 Datasets , parameters and code availability In order to propose a simulation study as realistic as possible we have used real data to infer the parameters of the design . The data come from the follow - up of 748 multiple myeloma patients registered in the 2009 IFM clinical trial described in the Results Section . An example of data is given in Figure 1 . From this data , we opted for the exponential form of the dynamics in the disease states with boundaries ζ0 = 1 and D = 40 for remission and death levels , simply calibrated as the minimal and maximal values in the data set . Then , as described in the Results Section , 3 parameters had to be calibrated , and all hyperparameters are explicitly given in the github repository [ 11 ] . For the risk function λ , we choose to distinguish the standard relapse ( from remission to disease state ) from the therapeutic escape ( from a disease state under appropriate treatment to the other disease state ) . We then further separate the risk by disease and treatment , so that for any treatment ℓand any state s = ( m , ζ , u ) , one has λℓ(s ) = λℓ m(ζ , u ) , where the form of λℓ m is specified in Table 1 . For the standard relapse , the risk µi for disease i was chosen as piecewise increasing linear functions calibrated such that the risk of relapsing increases until some duration τ1 ( average of standard relapses occurrences ) , then remains constant"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 18,
    "text": "of λℓ m is specified in Table 1 . For the standard relapse , the risk µi for disease i was chosen as piecewise increasing linear functions calibrated such that the risk of relapsing increases until some duration τ1 ( average of standard relapses occurrences ) , then remains constant , and further increases between say τ2 and τ3 years ( to model late or non - relapsing patients ) . This function and corresponding density are illustrated in \n ℓ= ∅ ℓ= a ℓ= b m = 0 λℓ m(ζ , u ) = ( µ1+µ2)(u ) λℓ m(ζ , u ) = µ2(u ) λℓ m(ζ , u ) = µ1(u ) m = 1 λℓ m(ζ , u ) = 0 λℓ m(ζ , u ) = µ′(ζ ) λℓ m(ζ , u ) = 0 m = 2 λℓ m(ζ , u ) = 0 λℓ m(ζ , u ) = 0 λℓ m(ζ , u ) = µ′(ζ ) m = 3 λℓ m(D , u ) = 0 λℓ m(D , u ) = 0 λℓ m(D , u ) = 0 ( m , ζ , u ) , and duration t , Φℓ(m , ζ , u , t ) = ( m , Φℓ m(ζ , t ) , u + t ) is the state of the patient after a time t starting from state s at time 0 if no change of condition or treatment occurred . ℓ= ∅ ℓ= a ℓ= b m = 0 Φℓ m(ζ , t ) = ζ Φℓ m(ζ , t ) = ζ Φℓ m(ζ , t ) = ζ m = 1 Φℓ m(ζ , t ) = ζev∅ 1 t Φℓ m(ζ , t ) = ζe−v′ 1 t = ζeva 1 t Φℓ m(ζ , t ) = ζevb 1 t m = 2 Φℓ m(ζ , t ) = ζev∅ 2 t Φℓ m(ζ , t ) = ζeva 2 t Φℓ m(ζ , t ) = ζe−v′ 2 t = ζevb 2 t m = 3 Φℓ m(ζ , t ) = ζ Φℓ m(ζ , t ) = ζ Φℓ m(ζ , t ) = ζ 0.0000 0.0005 0.0010 0.0015 Time Risk Instant risk 0e+00 2e−04 4e−04 6e−04 Time Density density distribution of event times condition ( similar shapes for standard relapse to disease a ) . For the therapeutic escape , we chose to fit a Weibull survival distribution of the form µ′(ζ ) = ( ˜βζ)˜α , with −1 < ˜α < 0 to account for a higher relapse risk when the marker decreases . We arbitrar- ily chose ˜α = −0.8 and calibrated ˜β = 1000 such that only about 5 % of patients experience a therapeutic escape . The aggressiveness of the disease / treatment efficiency v / v′ may depend on both treatment ℓ and mode m. Specific values are thus denoted by vℓ m / v′ m , see Table"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 19,
    "text": "˜β = 1000 such that only about 5 % of patients experience a therapeutic escape . The aggressiveness of the disease / treatment efficiency v / v′ may depend on both treatment ℓ and mode m. Specific values are thus denoted by vℓ m / v′ m , see Table 2 . After setting aside patients that do not relapse ( about 20 % ) , we first estimated remission and relapse times using maximal slope difference , and then fitted exponential regression models on each segment . We then clustered \n Markov transition kernel Q for the controlled PDMP . matrix Qℓ m(m′ ) for the controlled PDMP . For any treatment ℓand initial state s = ( m , ζ , u ) a jump sends the patient to state s′ = ( m′ , ζ′ , u′ ) sampled from the distribution Q(·|s , ℓ ) . The following constraints are satisfied : ζ′ = ζ , u′ = 0 and m′ is sampled from the discrete distribution Qℓ m. ℓ= ∅ m = 0 Qℓ m(m′ ) = 1(m′∈{1,2 } ) µm′(u ) µ1(u)+µ2(u ) m = 1 Qℓ m(m′ ) = 1(m′=0 ) ( possible only if ζ = ζ0 ) Qℓ m(m′ ) = 1(m′=3 ) ( possible only if ζ = D ) m = 2 Qℓ m(m′ ) = 1(m′=0 ) ( possible only if ζ = ζ0 ) Qℓ m(m′ ) = 1(m′=3 ) ( possible only if ζ = D ) ℓ= a m = 0 Qℓ m(m′ ) = 1(m′=2 ) m = 1 Qℓ m(m′ ) = 1(m′=2 ) ( possible only if ζ > ζ0 ) Qℓ m(m′ ) = 1(m′=0 ) ( possible only if ζ = ζ0 ) m = 2 Qℓ m(m′ ) = 1(m′=3 ) ( possible only if ζ = D ) ℓ= b m = 0 Qℓ m(m′ ) = 1(m′=1 ) m = 1 Qℓ m(m′ ) = 1(m′=3 ) ( possible only if ζ = D ) m = 2 Qℓ m(m′ ) = 1(m′=1 ) ( possible only if ζ > ζ0 ) Qℓ m(m′ ) = 1(m′=0 ) ( possible only if ζ = ζ0 ) patients based on their relapse coefficient and chose the number of clusters using the slope heuristic on residual sum of squares . We obtained two groups and used the average values to obtain v∅ 1 = 0.02 ( 22 % of patients ) and v∅ 2 = 0.006 ( 78 % of relapsing patients ) . We then computed the average of the treatment parameters for each group and obtained v′ 1 = 0.077 and v′ 2 = 0.025 . The data do not present patient relapsing under treatment , and therefore we could not estimate v for therapeutic escapes or inappropriate treatments . Because we assume the aggressiveness in those circumstances should be smaller than under standard relapse , we chose vb 1 = 0.01 and va 2 = 0.003"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 20,
    "text": "data do not present patient relapsing under treatment , and therefore we could not estimate v for therapeutic escapes or inappropriate treatments . Because we assume the aggressiveness in those circumstances should be smaller than under standard relapse , we chose vb 1 = 0.01 and va 2 = 0.003 . By separating the risk λ by disease , the kernel function Q is automatically fitted , since we assume that the marker level does not jump at relapses , and the mode is selected by the risk clocks leading to the jump , whichever rings first , see Table 3 . Finally , we arbitrarily selected a centered Gaussian distribution with noise parameter σ2 = 1 for the observation process . We resorted to extensive simulations study to select cost parameters that seemed reasonable ( very few patients dying over our study horizon , on average not more that a fourth of the follow - up time spent under treatment , etc ) . We arbitrarily fixed the visit cost CV to 1 . We then fixed the death cost M to 110 so that with visits every 15 days and early relapse , a patient would rather die that spend the entire horizon under treatment with numerous visits . We then fixed β = 0.1 so that the penalty of applying an unnecessary treatment would count as 1.5 , 3 or 6 times the visit cost depending on the choice of next visit date r. Finally , we selected κ = 1/6 from extensive simulations so that for low marker observations ( typically when it is hard to decide between relapse or remission with high noise ) it might be preferable to wait for new data acquisition rather than treat by default . All codes and parameters are available at https://github.com/acleynen/pomcp4pdmp [ 11 ] . \n 4.2 Reminder on controlled PDMPs Here is a description of how to simulate a trajectory of a controlled PDMP between two consecutive visits to the medical center . The controlled PDMP parameters are λ , the disease risk function ( distribution of duration until the next jump i.e. condition change ) , Q , the Markov kernel , defining the stochastic transition to the state reached after the next jump and { vℓ m}2 , the parameters of the exponential deterministic behavior of the marker between two jumps , defined from the current mode and treatment applied . Algorithm 1 Simulation of a trajectory between two consecutive decision times of a controlled PDMP . 1 : procedure SimulatePDMP(m , ζ , u , ℓ , r ) 2 : t ←0 3 : v ←vℓ m 4 : while t < r do 5 : S ∼λ 6 : S ←min{S , t∗(m , ζ , u , ℓ ) } 7 : if t + S > r then 8 : return m , ζ exp(vr ) , u + r 9 : else 10 : t ←t + S 11"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 21,
    "text": "do 5 : S ∼λ 6 : S ←min{S , t∗(m , ζ , u , ℓ ) } 7 : if t + S > r then 8 : return m , ζ exp(vr ) , u + r 9 : else 10 : t ←t + S 11 : ζ ←ζ exp(vS ) 12 : u ←u + S 13 : m ∼Q(·|m , ζ , u , ℓ ) 14 : u ←0 15 : v ←vℓ m Procedure SimulatePDMP takes as input an initial position Xt = s = ( m , ζ , u ) with mode m , marker level ζ , time since the last jump u , and a decision d = ( ℓ , r ) with treatment to be applied ℓfor a duration r until the next visit to the medical center and returns the state Xt+r = s′ = ( m′ , ζ′ , u′ ) of the process at time t + r given that treatment ℓwas applied . At line 5 , S ∼λ means that S is sampled from the distribution with risk function λ , which means that it has the following survival function P(S > t ) = e− R t 0 λ(m , ζ exp(vτ),u+τ)dτ . At line 6 , t∗(m , ζ , u , ℓ ) is the ( deterministic ) time to reach either the nominal value ζ0 or the death level D from the current point ( m , ζ , u ) ( if no change of condition or treatment occurs ) . The third variable u representing the time since the last jump allows transitions described in SimulatePDMP to be Markovian , i.e. to be independent of the previous transitions . 4.3 Reminder on Partially Observed Monte - Carlo Planning In this section we give a description of the original POMCP algorithm3 . The algorithm is called iteratively at each observation step n and requires several entries in order to output a decision dn to be used by the operator . The inputs include a set of possible decisions ( denoted A ) , a history 2For the sake of unified notation , we denote here va 1 = v′ 1 , vb 2 = v′ 2 , and v∅ 0 = va 0 = vb 0 = 0 3A Python implementation of the POMCP algorithm can be found here : https://github.com/GeorgePik/POMCP \n hn = ⟨ω0d0ω1 . . . dn−1ωn⟩of successive observations and decisions up to step n , including the current observation ωn , an approximate ( particle ) filter Θp(hn ) with support Bp(hn ) , a simulator G(s , d ) of state and observation at step n + 1 together with their cost given a state s and decision d at step n , a stopping criterion Timeout and an arbitrary Rollout strategy to provide a heuristic evaluation of an history , whenever needed . Algorithm 2 Original POMCP algorithm [ 41 ]"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 22,
    "text": "observation at step n + 1 together with their cost given a state s and decision d at step n , a stopping criterion Timeout and an arbitrary Rollout strategy to provide a heuristic evaluation of an history , whenever needed . Algorithm 2 Original POMCP algorithm [ 41 ] 1 : procedure POMCP(hn ) 2 : repeat 3 : if hn = ∅then 4 : s ∼Θp 5 : else 6 : s ∼Θp(hn ) 7 : Simulate(s , hn ) 8 : until Timeout ( ) 9 : d∗←arg mind V ( hnd ) 10 : V ( hn ) ←mind V ( hnd ) 11 : return d∗ 12 : procedure Rollout(s , h ) 13 : if s.t = H then 14 : return 0 15 : d ∼πrollout(h ) 16 : ( s′ , ω , c ) ∼G(s , d ) 17 : return c+Rollout(s′ , hdω ) 18 : procedure Simulate(s , h ) 19 : if s.t ≥H then return 0 20 : if h ̸∈T then 21 : for all d ∈A do 22 : T ( hd ) ←⟨Ninit , Vinit , ∅⟩ 23 : C ←Rollout(s , h ) 24 : return C 25 : d∗←arg mind V ( hd ) −α q log(N(h ) ) N(hd ) 26 : ( s′ , ω , c ) ∼G(s , d∗ ) 27 : C ←c+Simulate(s′ , hd∗ω ) 28 : Bp(h ) ←Bp(h ) ∪{s } 29 : N(h ) ←N(h ) + 1 30 : N(hd∗ ) ←N(hd∗ ) + 1 31 : V ( hd∗ ) ←V ( hd∗ ) + C−V ( hd∗ ) N(hd∗ ) 32 : return C The POMCP algorithm involves several data structures : • Simulated states s = ( m , ζ , u ) . • Decisions d = ( ℓ , r ) , belonging to a finite decision space A , preferably small . • Observations ω = ( y , t ) . The original algorithm assumes that they belong to a finite observa- tion space Ωof limited size . • Histories h = ⟨ω0d0ω1d1 · · · , dn−1ωnd′ω′d”ω ” . . . ⟩. Histories represent sequences of decisions and observations of variable lengths . They are the concatenation of the sequence of past observations / decisions hn = ⟨ω0d0ω1d1 · · · dn−1ωn⟩plus an arbitrary sequence of future ob- servations / decisions , built using the rollout strategy and a simulation model of the POMCP ( line 28 , h ←hd∗ω ) . • T is a tree data structure rooted at the initial history hn . Each node of T will correspond to an extended history , ended by either a decision or an observation . Each simulation step creates a novel node in the tree , and histories h are attached to T by appending the corresponding novel decision / observation to the parent node ’s history . In addition , we also"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 23,
    "text": "extended history , ended by either a decision or an observation . Each simulation step creates a novel node in the tree , and histories h are attached to T by appending the corresponding novel decision / observation to the parent node ’s history . In addition , we also attach to each node ( now denoted T ( h ) or T ( hd ) ) ( i ) integer numbers N(h ) or N(hd ) where N(h ) corresponds to \n the number of times the history h has been simulated and N(hd ) to the number of times d has been selected after h was encountered , ( ii ) real numbers V ( hd ) corresponding to an estimate of the cost value of hd , that is the expected sum of future costs obtained if the optimal policy is applied after h has been encountered and d selected , until the final decision step and ( iii ) Θp(h ) , called a particle filter , which is a discrete uniform distribution on a set Bp(h ) of states s compatible with the current history h. When Simulate is called with entry a history h that already belongs to T , it updates the values of N(h ) and Bp(h ) as well as the values N(hd ) and V ( hd ) for all the successor nodes4 hd . When Simulate is called with entry a history h which does not yet belong to T ( as is the case initially for hn ) , it appends h as well as all its successor nodes hd to T and initializes their values N(h ) , Bp(h ) , N(hd ) and V ( hd ) . Procedure Simulate is based on a generator function , ( s′ , ω , c ) ∼G(s , d ) that generates a successor ( hidden ) state s′ , an observation ω and an immediate cost c , from decision d applied in current ( hidden ) state s. Repeated calls to G are used to progressively expand T . Simulation sequences and updates are performed , starting from hn , until Timeout ( ) function requires to stop ( generally , after an arbitrary number nsearch of trajectories have been simulated or a fixed amount of time has been spent ) . Then , the decision d∗which maximizes V ( hnd∗ ) is applied to the real - world system , and a real - life observation ω ∈Ωis obtained . The new real - world history becomes hn+1 = hnd∗ω and T is pruned , so that the new tree is rooted5 in hn+1 . POMCP proposes strategies to select the input rollout and filters when the user has no knowledge on the process . A typical rollout strategy may simply involve selecting the decisions randomly from the set A. The following particle filter update procedure , included in procedure Simulate , is suggested :"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 24,
    "text": "POMCP proposes strategies to select the input rollout and filters when the user has no knowledge on the process . A typical rollout strategy may simply involve selecting the decisions randomly from the set A. The following particle filter update procedure , included in procedure Simulate , is suggested : • If h = ∅ , sample s ∼Θp 0 . In the initial step of the algorithm , we simulate random particles from an arbitrary belief state . • If h ̸= ∅ , sample s ∼Θp(h ) where Θp(h ) is the uniform discrete distribution on the finite nonempty set Bp(h ) . Indeed , if h ̸= ∅ , this means that procedure Simulate(s′ , h ) has already been called at least once for some s′ and thus Bp(h ) ̸= ∅(Bp(h ) contains at least s′ ) . • Sample ( s′ , ω , c ) ∼G(s , d∗ ) . This step is performed in line 27 of the POMCP algorithm . • if |ω −ω′| = 0 , update Bp(hd∗ω ) ←Bp(hd∗ω ) ∪{s′ } . As the number of samples increases , the supports of the filters Bp(h ) will contain more and more particles and converge to the empirical distribution P(·|h ) of hidden states given the observed trajectory h. When the state space is finite , Θp(h ) can be seen as an histogram approximation of P(·|h ) . 4.4 Adapted POMCP algorithm to the case of controlled PDMPs It seems natural to apply a POMCP algorithm to optimize a PDMP control strategy in the context of disease control . Indeed , PDMPs have natural generator functions since algorithm SimulatePDMP 4It is a property of the algorithm that whenever h ∈T , hd ∈T as well . 5The interest of pruning T instead of starting with an empty tree in hn+1 is to exploit past simulations in the computation of the next decision . \n can be naturally augmented with an observation simulator and a cost function , in order to obtain a generator function G as described above . We propose three adaptations of the original POMCP algorithm to exploit the particular framework of controlled PDMPs . Rollout policies The original POMCP algorithm describes the possible rollout policies as ad- missible policies , meaning that actions choices should only depend on the history of past actions and observations . Instead , we exploit here the PDMP generator which provides both hidden states and observations . This allows to design rollout policies exploiting hidden states instead of noisy observations . In our medical framework , one may build interesting rollout policies exploiting the hidden mode of the disease to provide good heuristics to the simulation part of POMCP . In practice , we propose to compare the two following rollout policies : 1 . The ( admissible ) uniform policy : πunif(ω = ( y , t ) ) ∼U({∅ , a , b } × { 15 ,"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 25,
    "text": "provide good heuristics to the simulation part of POMCP . In practice , we propose to compare the two following rollout policies : 1 . The ( admissible ) uniform policy : πunif(ω = ( y , t ) ) ∼U({∅ , a , b } × { 15 , 30 , 60 } ) 2 . The ( non - admissible ) mode policy : πmode(s = ( m , ζ , u ) ) = πmode(m ) =      { ∅ , 15 } if m = 0 , { a , 15 } if m = 1 , { b , 15 } if m = 2 . The mode policy , being based on the full observation of the process , is likely to underestimate the real cost of an optimal control policy , which is a useful property for the convergence of a heuristic search method [ 38 ] . In our simulation study we observed that a mode - based rollout policy can be particularly effi- cient . Observation space The time , state and observation spaces of the disease control PDMP model are continuous . This means that the probability of simulating exactly the same history h twice is zero if we apply the POMCP procedure as such . Thus , the tree depth and the size of filters supports Bp(h ) may never exceed 1 . This is particularly annoying since the POMCP algorithm convergence proof only holds when Θ(h ) is close to the true empirical belief state , which ( approximately ) holds when the size of the support Bp(h ) tends to + ∞. Indeed , the POMCP procedure in [ 41 ] requires that Bp(h ) contains at least K particles , when h is non empty . In practice , for the controlled PDMP case , observations are made of pairs ( y , t ) of a continuous - value observed marker level and discrete time of current decision . Therefore , we discretize the observation space into a set of contiguous intervals and group together observations belonging to the same interval . The continuous nature of the model also prevents the exact computation of the filter , hence we resort to the use of particle or conditional filters . The construction of the former is slightly adapted from the initial POMCP algorithm to fit our model . Indeed , as the true process still produces continuous - valued observations , the last action of the particle filter update procedure is modified to updating Bp(hd∗ω ) with s′ only if |ω −ω′| < D ( with D chosen by the user , typically of the same magnitude as the discretization precision ) . It may still happen that this procedure selects only states with wrong mode m at step n ( i.e. Θp(hn ) , which is only an approximation of the true filter , does not contain the true hidden mode"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 26,
    "text": "typically of the same magnitude as the discretization precision ) . It may still happen that this procedure selects only states with wrong mode m at step n ( i.e. Θp(hn ) , which is only an approximation of the true filter , does not contain the true hidden mode m ) , and hence can not generate compatible states at step n + 1 due to diverging dynamics of the process in the different modes . This is not a simple matter of statistical accuracy , but a very practical problem . When this happens , we say that Bp(hn+1 ) is deprived of particles and we can not go on applying POMCP to hn+1 . This problem of particle deprivation can be mitigated by a few modifications of the standard particle filter construction : \n • Assume that Bp(hn+1 ) is empty or too small , whatever the number of simulations of sn ∼ Bp(hn ) followed by a call to G(sn , d∗ n ) we perform . Then , we may go back in the history and resimulate sn−1 ∼Bp(hn−1 ) followed by two successive calls to G : – ( sn , ω′ , c′ ) ←G(sn−1 , d∗ n−1 ) and , provided that |ω′ −ωn| < D , – ( sn+1 , ω′′ , c′′ ) ←G(sn , d∗ n ) , hoping that now , |ω′′ −ωn+1| < D. A particle sn+1 is then added to Bp(hn+1 ) whenever the two above conditions are met . • Assume that Bp(hn+1 ) is non - empty but still too small ( |Bp(hn+1)| ≪K ) after the previous step was applied a large number of times . We may perform particle revigoration by resampling particles from Bp(hn+1 ) and duplicate them . • Finally , when everything fails , we may perform a large number of sampled transitions from Bp(hn ) ( e.g. 1000 × K ) and keep the K particles s′ in the generated samples(s′ , ω′ , c′ ) with minimal distance |ωn+1 −ω′| , with some arbitrary distance definition . In our experimental studies , we applied these three modifications in turn , whenever needed , until we got belief states Bp(hn ) of cardinality at least K. Filters We propose to modify the original particle filter of POMCP to incorporate the conditional filter . Hence the algorithm is modified as follows : starting from an initial arbitrary belief filter Θc for h0 = ∅as in the original POMCP , when the new history becomes hn+1 = hnd⋆ω , we compute the new filter Θc n+1 as a deterministic function of Θc n and d⋆ , ω ( see [ 10 ] for its specific form ) and sample K particles from Θc n+1 to generate a set of plausible hidden states . Depending only on the current belief and the new observation ( and not on simulations ) , this filter does not suffer from the propagation of approximations"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 27,
    "text": "10 ] for its specific form ) and sample K particles from Θc n+1 to generate a set of plausible hidden states . Depending only on the current belief and the new observation ( and not on simulations ) , this filter does not suffer from the propagation of approximations and particle deprivation . Moreover , the computational burden of the simulations is replaced by the computation of weighted sums which are particularly efficient in matrix programming languages . Supporting information 5.1 State of the art Artificial Intelligence in medicine The use of artificial intelligence methods in medicine has recently exploded , as was shown for example in [ 27 ] . Yet the vast majority of these works focus on diagnosis and prognosis , rather than treatment and follow - up . There are a few studies related to treatment of diseases , though . [ 7 ] provides a review of approaches to cancer treatment focused on medical decision support . In the context of drug design for cancer treatment , ( deep ) Reinforcement Learning ( RL ) approaches have been proposed recently [ 33,34 ] , but with the disadvantage of being black - box approaches , preventing the practitioner from access to an explainable model of disease evolution and treatment . Uncertainty quantification in models of assisted decision making have been proposed to alleviate this problem [ 4 ] . In the same line of work , [ 5 ] advocate learning of mechanistic models of cancer evolution / treatment in order to help decision making . However , the latter approach requires a large amount of data prior to applying any treatment action , in order to learn a model that may prove only partially valid as decisions influence the disease dynamics . The authors advocate that the approach may be rendered more efficient if learning phases are interleaved with actual decision phases . \n An alternative view of controlled PDMPs Controlled PDMPs can be modelled as continuous space POMDPs , in the way we proposed in this paper . However , they can also be seen as a particular subclass of continuous - time ( Partially Observed ) Semi - Markov Decision Processes [ 23 ] . Fully - observed continuous - time Semi - Markov Decision Processes extend Markov Decision Processes by including random continuous durations of state transitions and by considering that decisions can only be made at transition times . Several reinforcement learning solution approaches have been proposed , both in the fully - observed [ 8,19 ] and partially - observed [ 20 ] cases to solve these problems . An alternative approach to ours could be to cast the PDMP model of cancer treatment into the continuous - time SMDP model and look for specializations of the existing simulation based solution algorithms . 5.2 Supplementary simulation results on POMCP parameters Here we provide raw results for a series of parameter we tried to tune to optimize the POMCP"
  },
  {
    "source": "2401.03972v1.pdf",
    "chunk_index": 28,
    "text": "the PDMP model of cancer treatment into the continuous - time SMDP model and look for specializations of the existing simulation based solution algorithms . 5.2 Supplementary simulation results on POMCP parameters Here we provide raw results for a series of parameter we tried to tune to optimize the POMCP algorithm , but that did not seem to bring additional improvement in our framework . Table 4 shows the impact of the trade off parameter α′ in a few scenarios for the particle filter . set , n = 500 trajectories were simulated . The Value column is the average cost of the trajectories over the n trajectories , and ˆσ its empirical variance . We also recorded the runtime of optimizing each trajectory ( duration column ) . Filter πrollout nsearch K α′ Value 1.96ˆσ/√n duration duration s.d particle πmode 0.2 161.93 14.79 particle πmode 0.5 156.01 13.01 particle πmode 0.8 165.63 13.51 particle πmode 0.99 147.24 6.17 particle πmode 0.2 141.10 9.83 particle πmode 0.5 141.56 4.72 particle πmode 0.8 134.98 4.44 particle πmode 0.99 133.57 3.56 particle πmode 0.2 146.30 8.27 particle πmode 0.5 146.82 11.91 particle πmode 0.8 145.87 13.01 particle πmode 0.99 140.91 6.35 particle πmode 0.2 135.99 4.00 particle πmode 0.5 132.88 4.25 particle πmode 0.8 136.14 8.08 particle πmode 0.99 129.42 5.06 To allow adaptive selection of the trade off parameter c we tried three dynamic procedures to exploit or explore more depending on our trust in the current patient state . To do so , we define the state entropy as Et = P2 m=0 pm log(pm ) , pj = P s=(m , ζ , u)∈B 1{m = j } and Emax = log(1/3 ) and consider the following : • entropy : αt = Et / Emax \n • rev - entropy : αt = 1 −Et / Emax • rev - entropy-2 : αt = 1 −Et/2Emax . However , none of those procedures improved the results , as illustrated in selected examples in Table 5 . For each parameter set , n = 500 trajectories were simulated . The Value column is the average cost of the trajectories over the n trajectories , and ˆσ its empirical variance . We also recorded the runtime of optimizing each trajectory ( duration column ) . Filter nsearch α′ Value 1.96ˆσ/√n duration conditional entropy 138.63 5.06 conditional rev - entropy 131.94 3.70 conditional rev - entropy-2 133.75 3.70 particles entropy 142.17 9.88 particles rev - entropy 143.27 10.34 particles rev - entropy-2 135.28 3.82 conditional entropy 131.78 4.70 conditional rev - entropy 131.41 3.45 conditional rev - entropy-2 132.73 3.56 particles entropy 133.39 3.74 particles rev - entropy 135.64 3.74 particles rev - entropy-2 131.89 4.24"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 0,
    "text": "VALD - MD : Visual Attribution via Latent Diffusion for Medical Diagnostics Ammar Adeel Siddiqui1 , * , Santosh Tirunagari1 , Tehseen Zia2 , and David Windridge1 1Middlesex University , London , UK 2COMSATS University , Islamabad , Pakistan * Corresponding author . Email : ammaradeel7@gmail.com ABSTRACT Visual attribution in medical imaging seeks to make evident the diagnostically - relevant components of a medical image , in contrast to the more common detection of diseased tissue deployed in standard machine vision pipelines ( which are less straightforwardly interpretable / explainable to clinicians ) . We here present a novel generative visual attribution technique , one that leverages latent diffusion models in combination with domain - specific large language models , in order to generate normal counterparts of abnormal images . The discrepancy between the two hence gives rise to a mapping indicating the diagnostically - relevant image components . To achieve this , we deploy image priors in conjunction with appropriate conditioning mechanisms in order to control the image generative process , including natural language text prompts acquired from medical science and applied radiology . We perform experiments and quantitatively evaluate our results on the COVID-19 Radiography Database containing labelled chest X - rays with differing pathologies via the Frechet Inception Distance ( FID ) , Structural Similarity ( SSIM ) and Multi Scale Structural Similarity Metric ( MS - SSIM ) metrics obtained between real and generated images . The resulting system also exhibits a range of latent capabilities including zero - shot localized disease induction , which are evaluated with real examples from the cheXpert dataset . Introduction Medical imaging has become increasingly important in modern medical settings for patient stratification , assessing disease progression , evaluating treatment response , and grading disease severity1 . However , medical image diagnosis tends to involve far more than simple disease detection . Visual Attribution ( VA ) is the detection , identification and visualization of evidence of a particular class or category of images2 . It is a specific part of explainability of learned models i.e using visualization techniques to investigate the decisions made by a model , and attribute the decisions to distinct parts of an image . This opens the model to interpretation , a key aspect of XAI ( Explainable AI ) machine learning research , especially in relation to deep learning models3 . As it manifests , in medical imaging , VA is the process of educing evidence for medical conditions in relation to different parts of an image , such as pathological , psychological or disease - related effects4567 . As such , VA differs from the straightforward detection or segmentation of pathological regions in standard medical machine vision . These detected or segmented parts of the image are thus crucial biomarkers , and may serve as additional diagnostic and prognostic evidence8 . Such models base their decisions on locally or globally perceived evidence components , and it is thus in these terms that the"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 1,
    "text": "standard medical machine vision . These detected or segmented parts of the image are thus crucial biomarkers , and may serve as additional diagnostic and prognostic evidence8 . Such models base their decisions on locally or globally perceived evidence components , and it is thus in these terms that the VA aspects of the models must be visually and semantically interpretable9 . In clinical practice , these findings may then be used to diagnose and select treatment options , which may be surgical intervention , prescription of drugs etc . Interpretability is also key for scientific understanding of the system as a whole , and VA knowledge may thus sit on top of the explicit output of the model ( for example , VA - based delineation of those regions affected by a tumor , typically extending significantly beyond the segmented tumor region itself ) . VA knowledge factors may also relate to the safety of the application , or to the ethics and a priori biases of the data , highlighting incomplete or mismatched objectives being optimized by the model10 . A lack of interpretability of one or more of these examples may lead to complete or partial system failure , the model failing to achieve some aspect of the complex targets provided by the user / clinician , or optimization of an objective different to that intended . Model explainability is hence of critical interest in the medical imaging domain , having been identified as crucial to increasing the trust of medical professionals in the automated diagnostic domain1 . Visual attribution consequently provides a way to increase the confidence between the system , patient and clinician , leading to fewer misinformed results11 . It may also serve to decrease cognitive load on the clinicians and medical practitioners via automated localization and segmentation of areas of interest1213 . However , it is important to consider the specific requirements and safety - criticalities of the application when developing a VA model ( methods that directly manipulate images in the pixel space typically have to gain the acceptance of diagnosticians as part of their work process14 ) , and use - case flexible human - in - the - loop models are therefore to be preferred arXiv:2401.01414v1 [ eess . IV ] 2 Jan 2024 \n in the general case . 0.1 Generative Visual Attribution The most recent techniques in visual attribution involve variants of deep neural networks ( DNNs ) , which tackle the problem in different ways , though typically centred on classification or segmentation1516 . The need for VA is especially acute for DNNs in a clinical setting due to their intrinsic high complexity and low interpretability , often termed ‘ black boxes’1718 . However , DNNs , uniquely amongst machine learning VA approaches have the capacity to act in a generative manner . They hence have the capacity to mimic the actual clinical practice of a radiologist or practitioner , typically trained via the difference between healthy"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 2,
    "text": "often termed ‘ black boxes’1718 . However , DNNs , uniquely amongst machine learning VA approaches have the capacity to act in a generative manner . They hence have the capacity to mimic the actual clinical practice of a radiologist or practitioner , typically trained via the difference between healthy and non - healthy disease manifestations . As a result , the diagnosis of a condition or disease may be implicitly explained in terms of abnormalities of non - healthy tissue in relation to a hypothetical healthy version of the same tissue19 . Generative DNN - based machine learning therefore leads to the state - of - the - art strategy of generative visual attribution ( developed in part by the authors ) that leverages generative methods for counterfactual normal generation , in which abnormal images are translated into their normal counterparts for observation by a clinician . These methods hence perform visual attribution map generation via heatmaps taking the difference between the observed image of a patient and its healthy counterfactual201921 . Previously , such techniques have used a specific DNN generative mechanism , Generative Adversarial Networks or GANs to carry out this mapping ( cf the techniques ANT - GAN19 and VANT - GAN20 ) . This attribution process exploits the underlying properties of GANs to directly model the differences present between the normal and abnormal clinical images , as well as capture the complete structure of the individual classes in a learned latent representation . GANs in general have the advantage of requiring relatively fewer abnormal examples22 than standard supervised learning while still capturing underlying features of the surrounding areas of the higher density information regions . ( Examples of these overlooked regions might be micro tumors in other parts of an organ that may not , in themselves , have a highly significant effect on the supervised decision boundary2 ; it has been shown , especially for medical imaging DNNs , that such models typically disregard a significant fraction of these regions , which are essentially background evidence in relation to the underlying pathological condition23 ) . However , GANs , while powerful , have faults that have led to the very recent development of a new state - of - the - art generative mechanism : visual diffusion . Diffusion models are typically able to operate at higher resolutions and image qualities than GANs . They are also superior to GANs in not suffering from ‘ mode collapse ’ arising from the adversarial process of distinguishing real from generated images reaching a convergence ( Nash equilibrium ) in which critical image classes are omitted24 . Diffusion models have been used for counterfactual generation as Diff - SCM21 , and similar192526 . In this work , we shall use visual diffusion for counterpart normal generation . Our approach hence uses counterfactual generation with diffusion models directed at visual attribution in the medical imaging domain in a manner that builds on the conceptual foundations of generative visual"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 3,
    "text": "- SCM21 , and similar192526 . In this work , we shall use visual diffusion for counterpart normal generation . Our approach hence uses counterfactual generation with diffusion models directed at visual attribution in the medical imaging domain in a manner that builds on the conceptual foundations of generative visual attribution laid out in VANT - GAN20 . In doing so , we will aim to increase the interpretability of the model by using multi - modal ( text and image ) inputs . We hence leverage prior control and conditioning techniques to reliably steer the mapping process in an interpretable manner utilising text prompts and control images . We achieve this by training domain - specific language and vision models on relevant medical imaging data allowing the generation of visual attribution maps for specific medical conditions , which can be quantitatively measured using relevant metrics in the domain . As well as improving reliability , trustworthiness and utility with respect to the previous techniques of generative visual attribution , the approach of utilizing diffusion models in combination with domain - adapted large language models with enhanced controllability and conditioning potentially also opens horizons to applications such as post - surgery simulation of ageing , disease etc by leveraging natural language instructions , as well as a host of additional ‘ zero - shot ’ latent use - case capabilities . 0.1.1 Diffusion Generative Models Diffusion models consist of an autoencoder , which encodes the image into a latent space , and a diffusion process in which stochastic perturbations are performed incrementally in the latent space , such that a DNN can learn the reverse denoising process capable of transforming random noise images into images from the trained domain ( a process which may be guided by a suitable language model to introduce linguistic priors in the image generation ) . Depending on the autoencoder , the images generated by diffusion models are typically of relatively high resolution ( compared with GANs ) and the textual conditioning may include a wide range of textual encoders trained on specific domains , e.g. in the medical domain BioBERT27 , RadBERT28 and PubmedCLIP29 . Such language encoders can hence be used to condition the generation in a much more flexible way than other generative models , in particular GANs . Other approaches use the metadata in the datasets to help learn models that take into account age , gender , intracranial and ventricular volume etc in parallel with image conditioning such as RoentGen30 and LDM+DDIM31 for synthetic image generation . This meta - information can then be used to measure correlation among real images . 2/18 \n This ability to guide diffusion models via external semantic model make them potentially very powerful and relevant to visual attribution , especially in the medical imaging domain . 0.2 Proposed Methodological Approach The current research builds upon a particular conception of generative visual attribution set out in20 in the context of GAN generative models . In"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 4,
    "text": "via external semantic model make them potentially very powerful and relevant to visual attribution , especially in the medical imaging domain . 0.2 Proposed Methodological Approach The current research builds upon a particular conception of generative visual attribution set out in20 in the context of GAN generative models . In particular , it seeks to build on the notion of counterpart normal generation , but enriched via the use of visual diffusion and large language models . We thus leverage domain - adapted language components combined with conditional generation to modify the latent diffusion in a manner suited to medical VA . The approach hence combines domain - adapted large language and vision models to enable broad medical understanding to be brought to bear on the problem of counterpart normal generation , enabling generative visual attribution useful to understanding and pinpointing visual evidence in the form of generated counterfactuals and visual maps . Additionally , the representative power of the domain adapted large language model alongside the image - domain representation of the vision model ensures that medical image concepts are grounded in medical language , such that counterfactual generation may be prompted via complex ( natural language ) text prompts including , potentially , location and intensity of disease or condition , or else constrained to the specific organs within a medical scan . Note that the vision model is not directly trained on such morphological concepts beforehand ( e.g. the concept of an organ or the boundaries of an organ ) , yet is able to extrapolate from the combined multimodal knowledge using the data from the language and visual domain to discover these concepts latently . Lastly , the model proposed shows zero - shot generation capabilities on disease concepts that are out of the training data distribution , but which also appear qualitatively valid in the generated counterfactuals . This is presumably the result of exploiting the different extrapolate capabilities of the respective vision and language models in a synergistic manner . The model thus latently encompasses the ‘ rules of biology ’ in generating counterfactuals , e.g not generating extra lung scar tissue where it could not exist , outside of the chest cavity , irrespective of the language prompt . This strengthens our argument for using latent diffusion models for visual attribution , since no direct perturbations are made in pixel space and neither is the model trained on synthetic data . We also need only use a dataset with a modest amount of images and basic one - word labels , relying on the text encoder ( pretrained on domain - specific data , e.g. radiology reports ) to supply additional linguistic concept relations . The contributions of the study are as follows : 1 . We illustrate the use of the visual diffusion pipeline for jointly fine - tuning the combination of a domain - adapted text encoder and a vision encoder with a modest amount of real medical scans and text"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 5,
    "text": "relations . The contributions of the study are as follows : 1 . We illustrate the use of the visual diffusion pipeline for jointly fine - tuning the combination of a domain - adapted text encoder and a vision encoder with a modest amount of real medical scans and text prompts for conditional scan generation ( we thus eliminate the need for synthetic data ) . 2 . We generate visually valid counterfactuals ( non - healthy to healthy and vice versa ) with minimal perturbations to the original real image guided by text prompts that employ complex natural language medical imaging concepts . 3 . We explore the interpolation of knowledge in the text and vision domains using the composite text / vision models , evaluating the validity of the interpolations in the respective language and vision domains via their reflection into the other . 4 . Using the generated counterfactuals , we generate visual maps by subtracting the generated counterfactual from the original image for visual attribution in the medical imaging domain , thereby enhancing diagnostic explainability in the manner of VANT - GAN ( motivating the use of these models in safety - critical diagnostic applications in which visual explanation is critical for highlighting different areas of interest ) . 5 . We show zero - shot generation capabilities in the visual domain for inducing diseases in healthy or non - healthy scans prompted by complex text prompts including medical imaging concepts using the text encoder . 6 . Finally , we indicate the potential for future studies using such a combination of vision and language concepts for visual attribution using conditional generation . 1 Related Work in Generative Visual Attribution 1.1 Generation of activation maps Generative visual attribution includes a variety of classes of approach , each of which tackle the explainability problem in different ways . The particular class emphasised here , exemplified in a192 and20 , seek to generate complete or partial counterfactuals of the abnormal ( i.e. diseased ) image , and generate implicitly or explicitly a discrepancy map between the two . These maps are then visualized to highlight the attributing parts of the normal or abnormal image . 3/18 \n The ANT - GAN19 approach hence leverages GANs to generate normal or healthy - looking images from abnormal or unhealthy images and finds the difference between the two . These are then used to highlight local and global features from the image which otherwise might have been overlooked . The work in2 learns a map generating function from the training data . This function then generates an instance specific visual attribution map highlighting the features unique for a class . The VANT - GAN20 approach generates VA maps directly from unhealthy images , which can then be used to generate healthy - looking images from unhealthy images . ( This latter anticipates that the direct map modelling learns why the image is unhealthy and captures the appropriate local and global visual"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 6,
    "text": "VANT - GAN20 approach generates VA maps directly from unhealthy images , which can then be used to generate healthy - looking images from unhealthy images . ( This latter anticipates that the direct map modelling learns why the image is unhealthy and captures the appropriate local and global visual attributes of the disease ) . Charachon32 generates a range of adversarial examples and tracks the gradient across the stable generation of the original image and the adversarial example . By mapping these gradients to image space , visual attribution maps are generated to find differences between the counterfactuals and the original image . 1.2 Generation of complete counterfactuals The second ( more common ) class of generative visual attribution works generate complete subject / image counterfactuals , which are used for diagnostic findings and may or may not be used for explicit subtraction of images for highlighting the differences between the normal and generated counterfactual . STEEX33 uses region - based selection of images and counterfactuals are generated only using semantic guidance . The regions are thus hoped to be meaningful ( such as selecting a traffic signal with a green light and generating a counterfactual for a stop light within a complex image of a traffic junction ) . The counterfactuals are generated using semantic synthesis GAN , and the generation is constrained to keep the other regions unchanged . The Singla14 approach is a similar approach which uses perturbations in the original image controlled by a parameter . A counterfactual is generated for the perturbation such that the posterior probability of the image changes to the desired value of the parameter in the interval [ 0 , 1 ] . Cutting edge methods of image generation , such as diffusion models , have significantly improved the resolution and quality of generated images . These models have been utilized in counterfactual generation techniques for the latter class of techniques such as Diff - SCM34 , \" What is healthy\"21 and other similar techniques2535 . Diffusion models based generative VA techniques include36 , which use noise encoding with reversed sampling and perform guidance using a class label and task - specific network . This combination is then denoised with a sampling scheme to generate a class conditional counterfactual . Unsupervised Medical Image Translation with Adversarial Diffusion Models26 use a combination of diffusive and non diffusive models in an adversarial setup , to perform nosing and transformation operations with the noised latents of the image to translate between two modalities of MRI scans , using class conditioning , such as transforming a T1 contrast image to T2 . Diffusion Models for Medical Anomaly Detection25 use a weakly supervised setup for generating healthy counterfactuals of brain tumor images . The approach uses the noised latents from the diffusion model of the image and perform classifier guided denoising of the latent to produce a healthy image without a tumor . The What is Healthy?21 work similarly encodes the image into noised latents"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 7,
    "text": "generating healthy counterfactuals of brain tumor images . The approach uses the noised latents from the diffusion model of the image and perform classifier guided denoising of the latent to produce a healthy image without a tumor . The What is Healthy?21 work similarly encodes the image into noised latents , using an unconditional model . The decoding of the latent can be done via class label or unconditionally , to generate a counterfactual of the starting input image . A heatmap of the region containing the lesion is then produced by taking the difference between the reconstructed healthy and starting image . The guidance is performed without a downstream classifier using conditional attention mechanism techniques . In both of these broad classes of generative VA approach there is noticeable absence of a linguistic , natural language explanation or conditioning mechanism easily with which a domain expert could engage ‘ in the loop ’ ( e.g. communicating with the system in domain specific terminologies via precise relational instructions for counterfactual generation ) . Such techniques require the use of classifier guidance for conditional descent of gradients mapping between the latent parameter space and the image space ( for example , using weakly supervised decoding strategies or hyperparametric perturbation of the image towards a healthy looking counterfactual ) . Furthermore , such techniques focus on regions of high information density , in most cases leaving the broad structure of the image remain changed . ( An example would be a tumor causing exogenous pressure in the brain such that the surrounding tissue is displaced ; this structural deformity would not be visually reversed by the above techniques , but rather just the tumor mass removed , and the unhealthy tissue converted into healthy tissue via transformations of pixel level features characteristic of the affected region ) . 2 Diffusion Models Diffusion models are probabilistic models which learn a data distribution by reversing a gradual noising process through sampling . Denoising thus proceeds from as assumed starting point of x(t ) , where x(t ) is considered the final noisy version of the input x ( which , being assumed to be equivalent to pure noise , can be treated as an easily sampled latent space ) . The model thus learns to denoise x(t ) into progressively less noisy versions x(t −1),x(t −2 ) .. until reaching a final version x(0)24 , representing a sample from the domain distribution . In transforming a ( typically uniformly or Gaussian sampled ) latent space into an observational domain , the process is thus one of generative machine learning , with the denoiser typically a deep neural network of learned parameter weights . The latest approaches , however , use the reweighted variant of the evidence lower bound , 4/18 \n which estimates the gaussian noise added in the sample x(t ) , using a parametrized function θ(x(t),t ) rather than a denoised version of input x37 : LDM = Ex , ε∼N ("
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 8,
    "text": "latest approaches , however , use the reweighted variant of the evidence lower bound , 4/18 \n which estimates the gaussian noise added in the sample x(t ) , using a parametrized function θ(x(t),t ) rather than a denoised version of input x37 : LDM = Ex , ε∼N ( 0,1),t h ∥ε −εθ ( xt , t)∥2 i ( 1 ) with εθ ( xt , t ) estimated via the diffusion model , such that the objective function is the difference between the predicted ( latent parameter instantiation ) noise and the actual noise instantiation ( t is an arbitrary time step uniformly sampled from 1 , . . . , T and Ex denotes the expected value over all examples x in the dataset ) . 2.1 Latent Diffusion models To lower computational demands , latent diffusion models first seek to learn an appropriate latent space , one which , when decoded , is perceptually equivalent to the image space ( a key assumption of latent diffusion is thus that noise perturbation of image and latent spaces are not intrinsically incompatible with regard to the generative process ) . Denoting the encoder by E , E hence learns to map images x ∈Dx into a spatial latent code z = E(x ) . The essential mechanism of latent diffusion is then as indicated previously going forward - i.e. seeking to learn a model to correctly remove noise from an image , though this time in the latent space . The decoder D ( which is usually a DNN ) learns to map the latent codes back to images , such that D(E(x))p ≈qx . The objective function for the latent diffusion model now becomes LLDM : = EE ( x),ε∼N ( 0,1),t h ∥ε −εθ ( zt , t)∥2 i ( 2 ) where z(t ) is the latent noised to time step t3738 . 2.2 Conditioning using a domain - specific encoder In the following , the noise prediction function εθ ( xt , t ) is implemented using a time - conditioned Unet model39 , which can also be conditioned on class labels , segmentation masks , or outputs of a jointly trained domain specific encoder . Let y be the condition input and T(θ ) be a model which maps the condition y to an intermediate representation T(θ)(y ) which is then mapped to the intermediate layers of the UNet via a cross - attention layer40 . The objective function for the class - conditional variant of latent diffusion thus becomes : LLDM : = EE ( x),y , ε∼N ( 0,1),t h ∥ε −εθ ( zt , t , τθ(y))∥2 i ( 3 ) 2.3 Image Priors In the above , any arbitrary image can be considered an instantiation of the generative latent parameters . Thus , instead of commencing from pure noise ( i.e. purely stochastic latent parametric instantiantion ) , the latent diffusion process can instead be initiated from a given"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 9,
    "text": ") 2.3 Image Priors In the above , any arbitrary image can be considered an instantiation of the generative latent parameters . Thus , instead of commencing from pure noise ( i.e. purely stochastic latent parametric instantiantion ) , the latent diffusion process can instead be initiated from a given image , via application of the appropriate Stochastic Differential Equations ( SDEs ) , as a form of prior conditioning in the image space . The given image ( which may or may not be in the training data distribution , but which is presumed to lie within the manifold of natural images ) , is firstly perturbed with Gaussian noise ( ’ lifting out the image manifold ’ ) . This noise is then removed progressively via the learned denoiser , which effectively acts to reproject the guide image back into the manifold of natural images ; This may be thought of as a short random walk within the manifold of a given metric distance . More formally , if x(0 ) ∼p0 is a sample from the data distribution , the forward SDE produces x(t ) for t ∈(0,1 ] via Gaussian diffusion . Given x(0 ) , x(t ) is distributed as : x(t ) = α(t)x(0)+σ(t)z , z ∼N(0,I ) ( 4 ) where the magnitude of the noise z is defined by the scalar function σ(t ) : [ 0,1 ] →[0,∞ ) . The magnitude of the data x(0 ) is defined by the scalar function α(t ) : [ 0,1 ] →[0,1 ] . The probability density function of x(t ) as a whole is denoted pt . The usually considered SDE are of two types . One is Variance Exploding SDE , where α(t ) = 1 for all t and σ(1 ) is a large constant , which makes p1 close to N(0,σ2(1)I ) . The second type is the Variance Preserving SDE , satisfying α2(t)+σ2(t ) = 1 for all t with α(t ) →0 as t →1 , so that p1 equals to N(0,1)41 . Image synthesis is then performed via a reverse SDE4243 from the noisy observation of x(t ) in order to recover x(0 ) , given knowledge of the noise - perturbed score function ∇xlog pt(x ) . The learned score model as sθ(x(t),t ) , the learning objective for time t is : Lt = Ex(0)∼pdata , z∼N ( 0,I ) h ∥σtsθ(x(t),t)−z∥2 i ( 5 ) 5/18 \n with sθ(x(t),t ) a parametrized score model to approximate ∇xlog pt(x ) ; the SDE solution can be approximated with the Euler - Maruyama method41 . The update rule from ( t + ∆t ) to t is : x(t ) = x(t + ∆t)+ \u0000σ2(t)−σ2(t + ∆t ) \u0001 sθ(x(t),t)+ q σ2(t)−σ2(t + ∆t)z ( 6 ) A selection can be made on a discretization of the time interval from 1 to 0 and after the initialization x(0 ) ∼N ( 0,σ2(1)I ) , Equation"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 10,
    "text": "t is : x(t ) = x(t + ∆t)+ \u0000σ2(t)−σ2(t + ∆t ) \u0001 sθ(x(t),t)+ q σ2(t)−σ2(t + ∆t)z ( 6 ) A selection can be made on a discretization of the time interval from 1 to 0 and after the initialization x(0 ) ∼N ( 0,σ2(1)I ) , Equation 4 can be iterated to produce an image x(0)41 . 2.4 Additional Control Priors Additional conditioning mechanisms can be introduced to add further control to the generation e.g. ControlNet44 adds intermediate layers to the feature maps at each step of the downscaling operation while transitioning from image to latent space . Thus it becomes possible to add a task - specific image - conditioning mechanism to the model : L = Ez0,t , ct , cf , ε∼N ( 0,1 ) [ ∥ε −εθ ( zt , t , ct , cf))∥2 \u0003 ( 7 ) Where given an image z0 , noised latents zt are produced by progressively adding gaussian noise to the initial image after time steps t. Given the time step t , text prompts ct , and task specific conditions cf , the model learns a network to predict the added noise εθ . Some examples of task - specific image based conditioning include Canny edge maps , Semantic Segmentaion , Sketch - based guidance , and human pose44 etc . The conditioning mechanisms of input text , image priors , depth and segmentation maps can thus be used in combination with each other , complementing or adding to the image generation for further generative control as required on a task - by - task basis . 3 Methodology In the following , we indicate normal medical images by In and abnormal images by Ia. We make the assumption that In and Ia are sampled from distributions pn(I ) and pa(I ) respectively . Additionally , we assume that the differences between an abnormal image and its corresponding normal image ( from the same patient ) are only the characteristic disease markers or indicators of diagnostically relevant abnormality , and no other structural differences are present . In this setup , given an input abnormal image Ia , we wish to produce a visual attribution map M(Ia i ) that contains all the features that differentiate an abnormal image Ia i from its normal counterfactual In i , such that mapping is decomposed M(Ia i ) = Ia i −In i in common with the VANT - GAN20 strategy for visual attribution , albeit in a visual diffusion rather than GAN - based context . To generate the normal counterpart In i we use a conditioned stable diffusion model which combines a text and an image condition or input of the forms set out in sections 2.2 and 2.4 via the loss functions delineated in equations 5 and 7 . Using an image to image synthesis setting similar to SDEdit41 , we initiate with the abnormal image as the guide x(g ) = Ia i and"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 11,
    "text": "condition or input of the forms set out in sections 2.2 and 2.4 via the loss functions delineated in equations 5 and 7 . Using an image to image synthesis setting similar to SDEdit41 , we initiate with the abnormal image as the guide x(g ) = Ia i and add Gaussian noise to form the noised latents zt = x(g)(t0 ) ∼N ( x(g);σ2(t0)I ) which are then used to produce x(0 ) via application of equation 6 , conditioned on Tθ(y ) , where Tθ is a domain adapted text encoder which maps the conditional prompt y to an intermediate representation Tθ(y ) . Hence the normal corresponding image In i = x(0 ) is synthesized as the denoised version of εθ(zt , t , Tθ(y ) ) . The mask M(Ia i ) is then explicitly produced by subtracting the generated normal counterpart from the abnormal image . The network architecture is depicted in Figure 1 . The conditioned latent diffusion model pipeline that we utilise in the following experiments deploys an initial encoder / de- coder network of the form of a variational autoencoder ( VAE ) , a time - conditioned Unet model39 conditioned on a domain- specific encoder in the textual domain ( specifically a Bert based model trained on radiology reports called RadBERT28 ) and , finally , an additional system fine tuning detailed below . We use an image - to - image conditioning mechanism paralleling that of SDEdit41 , with the model taking two inputs , an image and corresponding text prompt to generate the counterfactual image from which the VA map is derived . 4 Experiments We firstly evaluate counterfactual generation – the generation of healthy counterparts to unhealthy scans – via an investigation of its qualitative impact i.e. the overall visual plausibility of the generated counterpart . Following this , we seek to quantitatively analyze the generative perturbation of the tested unhealthy scans in order to determine the utility of the method in its primary mode of VA application . Finally , we explore the latent capacity of the trained system to carry out a series of zero - shot counterfactual generation exercises , in particular : localized disease induction and the induction of diseases from outside the training data in relation to input healthy scans . 6/18 \n encoder ( ε ) to form the encoded image latents Z and passed through the diffusion process to form noised latents of the image ZT after incremental t steps . The fine - tuned conditional U - net denoises the latents into the conditioned latent Z , decoded by the VAE decoder D into the final generated counterfactual xn , from which a map M(xn ) is generated explicitly 4.0.1 Training Details The pretrained latent diffusion model CompVis / stable - diffusionv1 - 4 and the Bert based model RadBERT are obtained from Huggingface https://huggingface.co/StanfordAIMI/RadBERT . These were jointly fine - tuned using a single Quadro RTX 8000 at bf16"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 12,
    "text": "from which a map M(xn ) is generated explicitly 4.0.1 Training Details The pretrained latent diffusion model CompVis / stable - diffusionv1 - 4 and the Bert based model RadBERT are obtained from Huggingface https://huggingface.co/StanfordAIMI/RadBERT . These were jointly fine - tuned using a single Quadro RTX 8000 at bf16 precision , with batch size = 2 , at a resolution of 512x512px . The models were fine - tuned on the diffusers library using an approach for binding a unique identifier to a specific subject via a class - specific prior preservation loss , Dreambooth ? , with 1200 training steps used for the Normal class , after which 500 training steps are applied for each of the non - healthy classes , namely Lung Opacity , COVID-19 , and Viral Pneumonia , making a total number of training steps of 2700 . The greater preponderance of the normal class ameliorates the intrinsic imbalance in dataset , with model convergence inherently slower for the X - ray image domain , being out of the initial distribution . The learning rate was 5e-05 and , for sampling , the PNDM scheduler strength is set at 0.55 with Guidance Scale=4 found to be most effective across all classes for counterfactual generation . The COVID-19 Radiography Database45 contains 10192 normal , 3616 COVID-19 , 4945 Lung Opacity and 1345 Viral pneu- monia chest x - ray images . The dataset is obtained from https://www.kaggle.com/datasets/tawsifurrahman/ covid19 - radiography - database . The model is fine - tuned on the images using their respective labels as text prompts i.e Normal chest scan , Lung Opacity , Viral Pneumonia , and COVID 19 . 4.1 Qualitative Evaluation of Healthy Counterpart Generation Example images from the disease COVID-19 Radiography Database and their generative healthy counterparts are given in middle column are examples of the generated healthy counterfactuals obtained via latent space diffusion , with RadBERT - guided textual - conditioning via a conditional prompt “ normal chest x - ray ” . A total of 75 diffusion inference steps are used with image conditioning strength=0.85 and guidance scale=7.5 . ( The former indicates the level of constraint on changes to the original 7/18 \n ( a ) Lung Opacity Instances ( b ) Generated Normal ( c ) Generated Healthy Tissue via difference 8/18 \n ( a ) COVID 19 ( b ) Generated Normal ( c ) Difference ( d ) Lung Opacity ( e ) Generated Normal ( f ) Difference ( g ) Viral Pneumonia ( h ) Generated Normal ( i ) Masked Difference input image and the latter is the weight given to the textual encoder conditioning in the generation of the image , ranging over [ 0,1 ] and [ 0,9 ] , respectively ) . Side - by - side inspection of the generated healthy counterfactuals ( as per fig . 2 ) suggests that , as required , only minimal perturbation is made to the original image with"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 13,
    "text": "the image , ranging over [ 0,1 ] and [ 0,9 ] , respectively ) . Side - by - side inspection of the generated healthy counterfactuals ( as per fig . 2 ) suggests that , as required , only minimal perturbation is made to the original image with respect to healthy pixels -i.e . localized image sites without structural medical defects . ( In the top row , the medical structural defect in the original image is due to a lung opacity , and characterized via a relatively complex interaction between the imaging modality and subject manifesting as ‘ gaps ’ in the corresponding portions of the lung scan ) . The healthy / non - healthy discrepancy maps in all of these cases are obtained via masked subtraction of the original image from the generated image ( the ground truth segmentation masks correspond to the broad area of interest – i.e. the complete lung ) . The generated healthy tissue is thus a subset of the mask and is shown in the final column of fig . 2 for the respective cases . In the context of a VANT - GAN20 - based approach , this highlighted material constitutes the diagnostic counterfactual visual attribution , i.e. the selection of material relevant to the diagnosis of the unhealthy condition . Healthy counterfactual generation was performed for the complete datasets in the three unhealthy classes , i.e Lung opacity , Viral Pneumonia and COVID , examples of which are given in fig . 3 for the three classes ( all of the generated healthy counterfactuals from this experiment can be found on https://huggingface.co/ammaradeel/diffusionVA ) . Visual inspection indicates that the generated counterfactuals are , in general , visually plausible with minimal perturbation made to the unhealthy image overall . Moreover , the 9/18 \n healthy counterpart generation does not appear to unnecessarily affect aspects of the images unrelated to the medical condition , the model selectively making changes to the unhealthy regions in a structurally plausible manner , e.g. generating missing portions of the lung without generating extraneous lung material where it would be expected to normally exist ( e.g. in the abdominal cavity ) . 4.2 Quantitative Evaluation of Healthy Counterpart Generation 4.2.1 Fréchet Inception Distance ( FID ) Measures For quantitative evaluation on the COVID19 dataset , Fréchet Inception Distance ( FID)46 was calculated for the generated healthy counterfactuals for each class in order to measure the general level of plausibility , and also to assess how distant the generated counterpart normal distribution is from that of the healthy and diseased image sets . Image Set 1 ImageSet 2 Frechet Inception Distance Lung Opacity Generated Healthy 27.8 Lung Opacity Real Healthy 46.9 Relative Absolute Difference 19.1 Viral Pneumonia Generated Healthy 37.63 Viral Pneumonia Real Healthy 97.6 Relative Absolute Difference 59.97 COVID 19 Generated Healthy 32.2 COVID 19 Real Healthy 38.2 Relative Absolute Difference 6.0 FID scores are calculated with default characterisations i.e activations of the pool3 layer"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 14,
    "text": "Lung Opacity Real Healthy 46.9 Relative Absolute Difference 19.1 Viral Pneumonia Generated Healthy 37.63 Viral Pneumonia Real Healthy 97.6 Relative Absolute Difference 59.97 COVID 19 Generated Healthy 32.2 COVID 19 Real Healthy 38.2 Relative Absolute Difference 6.0 FID scores are calculated with default characterisations i.e activations of the pool3 layer of the InceptionV3 model with 2048 dimensions ( the particular implementation deployed is sourced from the Pytorch FID package47 ) . A lower FID would indicate that distribution of the two image sets are similar . Obtained results ( cf Table 1 ) indicate that the real healthy and the generated healthy counterfactuals have relatively similar distributions , with the exception of the Viral Pneumonia class , which has a significantly larger absolute relative difference in FID scores . ( An “ ImageSet \" here indicates randomly - sampled images of a real class or a generated class . E.g. In the first row of Table 1 , ImageSet 1 is Lung Opacity , referring to all images of the Lung Opacity class from the original dataset , while ImageSet 2 contains all generated healthy images corresponding to ImageSet1 . ImageSet 1 and ImageSet2 in the second row correspond to the images of the Lung Opacity and Healthy classes of the original dataset respectively ) . Relative differences between generated healthy and real healthy images are presented in Table 2 for respective classes ( with FID measured as µh −µg 2 + Tr(Σh + Σg −2(ΣhΣg)1/2 ) for the two continuous multivariate Gaussian distributions parametrised ( µg , Σg ) and ( µh , Σg ) applied to activations of the pool3 layer of the InceptionV3 model ) . The relative differences highlighted in Table 2 are overall indicative of good fidelity ( By way of baseline , FID differences using unconditioned stable diffusion without any training or fine - tuning can reach values 275.0 in the Roentgen30 study ) . Image Set 1 ImageSet 2 Frechet Inception Distance Real Healthy Generated Healthy from the Lung Opacity class 60.60 Real Healthy Generated Healthy from the Viral Pneumonia class 110.72 Real Healthy Generated Healthy from the Viral COVID19 class 45.11 The overall visual soundness of the generated images , as validated via the absolute and relative FID scores obtained for each of the classes , is thus broadly consistent with the previous qualitative interpretation that tested image distributions are minimally perturbed in order to transform them into healthy counterfactuals , while refraining from making changes to the healthy local regions of the image ( the scores of the COVID19 class are the closest in this respect among the tested disease conditions , with a relative absolute difference of 6.0 in FID scores between real and generated images . 10/18 \n The scores for the viral pneumonia class appear to be in a large part attributable to the relatively larger magnitude of fundamental structural differences between healthy and viral pneumonia images in the training set : in particular , the viral pneumonia"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 15,
    "text": "scores between real and generated images . 10/18 \n The scores for the viral pneumonia class appear to be in a large part attributable to the relatively larger magnitude of fundamental structural differences between healthy and viral pneumonia images in the training set : in particular , the viral pneumonia image set mostly had scans from children and infants , while the healthy class was of adult majority . ( This data bias would break the basic assumption that differences between class image sets is due only to structural defects of disease ) . 4.2.2 SSIM and MS - SSIM Measures As a further quantitative measure of the relationship between diseased image and generated healthy counterfactuals , we adopt the Structural Similarity ( SSIM ) and Multi Scale Structural Similarity Metric ( MS - SSIM)48 metrics , calculated between the unhealthy images and their respective generated counterparts , and averaged across classes . The Structural Similarity index49 quantifies the differences between a processed / distorted image x and a reference image y , combining the three key comparisons : luminance l(x , y ) , contrast c(x , y ) and structure s(x , y ) . The SSIM(x , y ) between two signals or images x and y is then given as : SSIM(x , y ) = [ l(x , y)]α · [ c(x , y)]β · [ s(x , y)]γ , where α , β and γ are weighting variables , used to control the relative importance of the three factors . We use the general form of the measure where α = β = γ = 1 and C3 = C2/2 : SSIM(x , y ) = ( 2µxµy + C1)(2σxy + C2 ) \u0000µ2x + µ2y + C1 \u0001\u0000σ2x + σ2y + C2 \u0001 ( 8) with mean intensities µ and standard deviations σ , estimating the signal contrast . The Multi - Scale Structural Similarity50 ( MS - SSIM ) is an extension of SSIM incorporating image details at differing resolutions , progressively downsampling x and y signals using a low - pass filter in factors of 2 . The j - th contrast and structure comparisons are respectively denoted as cj(x , y ) and sj(x , y ) ( the luminance comparison Eq.12 is made at only the largest scale ( i.e. original size ) at scale M. The Multiscale SSIM is then defined : MS - SSIM(x , y ) = [ lM(x , y)]αM · M ∏ j=1 [ cj(x , y)]βj [ sj(x , y)]γ j ( 9 ) MS - SSIM and SSIM metric values are interpreted as measuring the extent of structural similarity between the generated counterfactuals and unhealthy real images : a priori , the structure of unhealthy images should not change significantly overall in terms of their broad morphology , but only the requisite minimal perturbations should be made . A low structural similarity indicates larger perturbations to the unhealthy image , and a"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 16,
    "text": "counterfactuals and unhealthy real images : a priori , the structure of unhealthy images should not change significantly overall in terms of their broad morphology , but only the requisite minimal perturbations should be made . A low structural similarity indicates larger perturbations to the unhealthy image , and a higher structural similarity indicates smaller overall perturbation : in the extreme cases , 0 would indicate no structural similarity , and 1 would indicate identity of the images . The SSIM and the MS - SSIM measures for the respective disease classes are as depicted in Table 3 , and appear consistent with this prior assumption , with only small variation between tested disease classes . Image Set 1 Image Set 2 MS - SSIM SSIM COVID Generated Healthy 0.830 0.798 Lung Opacity Generated Healthy 0.813 0.780 Viral Pneumonia Generated Healthy 0.802 0.768 4.3 Latent Capacity of the Model For Open - Ended Visual Analysis The implicit coupling of a Language Model ( LM ) with a stochastic image parameterization model embodied by our approach raises the question of whether other use cases are made possible within a VA context , closer to the goal of arbitrary open - ended counterfactual querying of medical data ( e.g. in which a medical practitioner might , as part of the diagnostic chain of evidence , ask : “ What would this scan look like if the patient were X years older and suffered from condition Y ? \" ) . Thus we seek to establish the presence of Latent Capabilities within the model : i.e. capabilities not explicit instilled at training time . We conduct two sets of ( qualitative and quantitative ) experiments to evaluate this latent capacity , namely : Zero - shot Induction of Non - Healthy Counterparts and Localized Disease Induction . 4.3.1 Zero - shot Induction of Non - Healthy Counterparts Despite our model being trained for healthy counterpart generation , we may consider instead a reverse of this process , i.e. the induction of a specific disease within healthy scans using the same experimental pipeline . In particular , we can consider the capacity to induce disease via the latent language capacity of the model . 11/18 \n ( a ) Real Normal ( b ) Model Induced Carcinoma ( c ) Real example with expert markings51 As an instance of this , the trained model was prompted in the generative setting for “ carcinoma \" in relation to a healthy image . The result is shown in Figure 4 alongside the real healthy scan and a separate real - case carcinoma . It is clear that the induced disease is visually comparable to that of the real case despite it ’s absence from the training set . We propose that this capability arises as a result of a the internal correlation of the domain - adapted text encoder to that of the visual domain via the visual model , given that the domain - adapted"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 17,
    "text": "the real case despite it ’s absence from the training set . We propose that this capability arises as a result of a the internal correlation of the domain - adapted text encoder to that of the visual domain via the visual model , given that the domain - adapted text encoder is trained on the full panoply of Radiology reports . To evaluate this in more detail we examine a less localised condition : Cardiomegaly . Zero - shot evaluation : Cardiomegaly The disease cardiomegaly ( enlargement of the heart ) was not present in the training data ; to evaluate zero shot induction in this context , we take real images from the small version of the Chexpert52 dataset ( from https://www.kaggle.com/ datasets / ashery / chexpert ) . Thus , 8060 images of positively identified cases of cardiomegaly were used as the reference image set for real cardiomegaly . Correspondingly , for each of the healthy images from the COVID 19 database , an induced version was generated by the model with the prompt “ Cardiomegaly \" . FID scores between the real cases of cardiomegaly from the Chexpert dataset and the generated images are given in Table 4 . Image Set 1 Image Set 2 FID Real Cardiomegaly Generated Cardiomegaly 52.08 Real Healthy Generated Cardiomegaly 17.71 The FID scores in Table 4 indicate that the generated cardiomegaly images do not have a large distance ( using the 275.0 baseline of the Roentgen30 study ) from the real images from which they were generated , suggesting appropriate perturbations were made and the generations were reasonably close to the real cardiomegaly set from the Chexpert dataset . Interestingly , while generation across different settings of the visual diffusion hyperparameters Strength & Guidance - scale did not have a very significant difference on FID scores evaluated across the full range of image sets , visual differences for individual images could be more significant , as highlighted in Figure 5 for two different settings of the respective hyperparameters . This is presumably due to the different aspects specific to individual patient image ( such as the prior health of the patient , structural variances due to age , recording equipment , size etc ) acting to mimic hyperparametric variation , which primarily appears to affect the opacity of the induced material for hyperparameter settings ranges consistent with good image generation ( in general , the Strength hyperparameter give scope for larger perturbation from the original image during diffusion , while Guidance - scale determines the intensity of text prompt conditioning ; optimal settings of these parameters are inherently disease - specific given the wide variation in the amount of pixel opacity needing to be added in the disease induction setting of the pipeline ) . ( For an additional comparison baseline , we include results for induction of disease that are within the training set , namely viral pneumonia and COVID19 . Results are given in figure 6 ) ."
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 18,
    "text": "opacity needing to be added in the disease induction setting of the pipeline ) . ( For an additional comparison baseline , we include results for induction of disease that are within the training set , namely viral pneumonia and COVID19 . Results are given in figure 6 ) . 12/18 \n ( a ) Real Normal ( b ) Strength=0.6,Guidance scale=6 ( c ) Real Normal ( d ) Strength=0.9 , Guidance scale=7 13/18 \n ( a ) Real Normal ( b ) Induced viral pneumonia ( c ) Difference ( d ) Real Normal ( e ) Induced COVID19 ( f ) Difference ( a ) Real Normal ( b ) Induced lung opacity on the right ( c ) Difference ( d ) Real Normal ( e ) Induced lung opacity on the left ( f ) Difference 14/18 \n 4.3.2 Localized Disease Induction Finally , a key requirement of counterfactual visual attribution is sensitivity to both exogenous and endogenous aspects of disease : we define the endogenous visual aspects of disease as those attributes intrinsic to diagnosis , and the exogenous aspects as free - parameters associated with diseased tissue that are not themselves directly implicated in diagnosis . An example might be a tumor identified via its texture characteristics ( endogenous ) , but which is otherwise located arbitrarily within a particular organ ( so that location within the diseased organ is effectively an exogenous free variable within a VA context ) . We therefore illustratively test our model in regard to its latent capability to induce disease in specific locations through the simple expedient of conditioning on positionally - indicative text . The results may be seen in figure 7 for the case of localized lung opacity ( lung opacity being chosen because it is both diffuse and generally specific to one or other lung ) . The respective condition texts are “ large lung opacity on the left ” and “ large lung opacity on the right ” . 5 Conclusion In this work , we present a novel generative visual attribution technique for improving explainability in the medical imaging domain , leveraging a fusion of vision and large language models via the stable diffusion pipeline , built on foundational generative VA concepts from the VANT - GAN20 approach . The model developed generates normal counterparts of scans affected by different medical conditions in order to provide a subtractive salience map between the real affected regions and the generated normal scans , thereby providing insight into those regions relative to diagnosis ( and which is thus distinct from straightforward segmentation of diseased regions typically associated with machine medical diagnostics ) . It does so in a manner potentially synonymous with , and therefore assistive to , the inference process of human medical practitioners . The pre - trained domain - adapted text and vision encoder are jointly fine - tuned using a modest number of image and one - word text training examples"
  },
  {
    "source": "2401.01414v1.pdf",
    "chunk_index": 19,
    "text": "so in a manner potentially synonymous with , and therefore assistive to , the inference process of human medical practitioners . The pre - trained domain - adapted text and vision encoder are jointly fine - tuned using a modest number of image and one - word text training examples from the medical imaging domain for image - to - image generations . The generation capabilities include the induction of different medical conditions in healthy examples induced with varying severity . Inputs to the text encoder support advanced medical domain language and terminology , with the capacity for specifying particular topological locations in organs . By harnessing the model ’s learned multimodal knowledge from the domain - adapted text encoder and the vision model , out - of - training data distribution or zero - shot generations can be made for unseen medical conditions . In the medical diagnostics domain , future work will address the possibility of addressing complex disease - interactions , for example , providing simulation of the composite effects of age , lifestyle choices , and differing underlying disease conditions . The modest data requirement may also prove helpful for few - shot learning in relation to rare diseases or those with limited examples ( for example , neonatal medical scans ) ."
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 0,
    "text": "TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG Primitive Geometry Segment Pre - training for 3D Medical Image Segmentation Ryu Tadokoro∗1,2 tadokororyuryu@gmail.com Ryosuke Yamada∗1,3 ryosuke.yamada@aist.go.jp Kodai Nakashima1,3 nakashima.kodai@aist.go.jp Ryo Nakamura1,4 ryo.nakamura@aist.go.jp Hirokatsu Kataoka1 hirokatsu.kataoka@aist.go.jp 1 National Institute of Advanced Industrial Science and Technology , Japan 2 Tohoku University , Japan 3 University of Tsukuba , Japan 4 Fukuoka University , Japan Abstract The construction of 3D medical image datasets presents several issues , including requiring significant financial costs in data collection and specialized expertise for anno- tation , as well as strict privacy concerns for patient confidentiality compared to natural image datasets . Therefore , it has become a pressing issue in 3D medical image segmen- tation to enable data - efficient learning with limited 3D medical data and supervision . A promising approach is pre - training , but improving its performance in 3D medical image segmentation is difficult due to the small size of existing 3D medical image datasets . We thus present the Primitive Geometry Segment Pre - training ( PrimGeoSeg ) method to enable the learning of 3D semantic features by pre - training segmentation tasks using only primitive geometric objects for 3D medical image segmentation . PrimGeoSeg per- forms more accurate and efficient 3D medical image segmentation without manual data collection and annotation . Further , experimental results show that PrimGeoSeg on Swi- nUNETR improves performance over learning from scratch on BTCV , MSD ( Task06 ) , and BraTS datasets by 3.7 % , 4.4 % , and 0.3 % , respectively . Remarkably , the perfor- mance was equal to or better than state - of - the - art self - supervised learning despite the equal number of pre - training data . From experimental results , we conclude that effective pre - training can be achieved by looking at primitive geometric objects only . Code and dataset are available at https://github.com/SUPER-TADORY/PrimGeoSeg . Introduction 3D medical image analysis using deep learning is expected to enhance diagnostics and im- prove patient outcomes through the more accurate detection and visualization of geometric structures inside the human body . For example , 3D medical image segmentation estimates © 2023 . The copyright of this document resides with its authors . It may be distributed unchanged freely in print or electronic forms . ∗These authors contributed equally to this work . arXiv:2401.03665v1 [ cs . CV ] 8 Jan 2024 \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG the location and category of human organs from computed tomography ( CT ) and magnetic resonance imaging ( MRI ) images . More accurate segmentation of 3D medical images re- quires a large amount of training data and rich semantic annotation . However , training data collection is difficult due to the high imaging costs and stringent privacy protections . In addition , the annotation process requires expert knowledge of medical science . In order to solve the above problems ,"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 1,
    "text": "a large amount of training data and rich semantic annotation . However , training data collection is difficult due to the high imaging costs and stringent privacy protections . In addition , the annotation process requires expert knowledge of medical science . In order to solve the above problems , there have been many studies in terms of pre- training methods toward more data - efficient learning under limited training data conditions . In particular , self - supervised learning ( SSL ) has emerged as a promising approach for pre- training in 3D medical image segmentation [ 3 , 5 , 6 , 8 , 12 , 21 , 24 , 25 , 26 , 28 , 29 , 30 , 31 , 32 ] , as it learns 3D structural features and reduces manual annotation costs by designing and learning a pre - text task on unsupervised data . Chen et al . [ 5 ] achieved state - of - the - art per- formance on the Multi - Atlas Labeling Beyond the Cranial Vault ( BTCV ) dataset [ 18 ] and Medical Segmentation Decathlon ( MSD ) [ 1 ] dataset by merging existing 3D medical im- age datasets and pre - training three pseudo tasks . Nevertheless , pre - training methods for 3D medical image segmentation have lagged compared with other 3D object recognition tasks because of the small scale of pre - training datasets . Therefore , an alternative pre - training ap- proach is needed to address dataset construction issues in 3D medical image segmentation . Formula - driven supervised learning ( FDSL ) [ 13 , 14 ] has been proposed as a synthetic pre - training method without real data and human annotations , which automatically generates synthetic data and supervised labels based on a specific principle rule of the real world . Therefore , a significant advantage of FDSL is that the properties of the synthetic data can be designed considering fine - tuning tasks different from real data . Furthermore , as much as possible , FDSL can reduce dataset issues related to real data , such as social bias and personal information protection . Recently , Nakashima et al . [ 20 ] reported that Vision Transformer ( ViT ) tends to focus on the outlines of an object on images in pre - training . Inspired by the above insight , Kataoka et al . [ 15 ] proposed a Radial Counter DataBase ( RCDB ) that has improved the complexity of outlines and pre - training performance . Furthermore , Yamada et al . [ 27 ] proposed a Point Cloud Fractal DataBase ( PC - FractalDB ) based on fractal geometry to improve performance by designing 3D object detection pre - training . From these insights , we hypothesize that we can design segmentation tasks using only primitive geometric objects to achieve an effective pre - training method for 3D medical segmentation ."
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 2,
    "text": "PC - FractalDB ) based on fractal geometry to improve performance by designing 3D object detection pre - training . From these insights , we hypothesize that we can design segmentation tasks using only primitive geometric objects to achieve an effective pre - training method for 3D medical segmentation . ••• ••• SSL ( Tang et al . [ 23 ] ) PrimGeoSeg ( Ours ) Pre - text task ( e.g. Rotation ) ( a ) Pre - training method ( b ) Comparison of PrimGeoSeg and state - of - the - art SSL Ground truth Tang et al . [ 23 ] PrimGeoSeg ( Ours ) Fine - tuning @BTCV Dice Avg . 82.0 ( +0.4 ) 81.6 Synthetic data pre - training Pre - training data num . : 5,000 3D medical image pre - training Pre - training data num . : 5,050 Pre - train ( Encoder - Decoder ) Pre - train ( Encoder only ) More Precise Segmentation ! ! ! The present study proposes a prim- itive geometry segmentation ( Prim- GeoSeg ) for 3D medical image seg- mentation by automatically generating pre - training data and expressing se- mantically supervised labels as an as- sembly of primitive geometric objects in 3D space , as shown in Figure 1 . We generate a primitive geometric object from independent laws in the xy - plane and z - axis directions . We also con- struct a pre - training dataset by arrang- ing multiple primitive geometric ob- jects in 3D space , overlapping each ob- ject . We designed this generation pro- cess to consider two aspects of the in- ternal structure of the human body ; ( i ) the variability among individuals and ( ii ) the comTADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG plexity with ambiguous boundaries between organs . The experimental results found that primitive geometric objects only are sufficient to learn the necessary 3D structural represen- tations for achieving a superior pre - training effect for 3D medical image segmentation . The contributions of this work are as follows : ( i ) We propose PrimGeoSeg as a pre- training method that enables pre - training by segmentation without real data collection and manual annotation . ( ii ) We show that pre - training both UNETR and SwinUNETR with Prim- GroSeg outperform state - of - the - art SSL accuracy on BTCV and MSD in 3D medical image segmentation . Notably , the number of synthetic pre - training data was almost equal ( see Fig- ure 1 ) . In addition , PrimGeoSeg also demonstrates remarkable data efficiency , performing as well with only 30 % of the BTCV data as it does when learning from scratch with 100 % of training data . ( iii ) Our proposed method for pre - training synthetic data can reduce problems such as the privacy of 3D medical images . Related Works"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 3,
    "text": "as well with only 30 % of the BTCV data as it does when learning from scratch with 100 % of training data . ( iii ) Our proposed method for pre - training synthetic data can reduce problems such as the privacy of 3D medical images . Related Works Pre - training for 3D medical image segmentation . In 3D medical image segmentation , SSL has been attracting attention for its ability to achieve highly accurate segmentation results by pre - training unsupervised 3D medical images [ 3 , 4 , 5 , 6 , 8 , 12 , 24 , 25 , 26 , 28 , 29 , 30 , 31 , 32 ] . Even in transformer - based models that achieve higher accuracy than conventional CNN - based models for 3D medical images [ 9 , 10 ] , SSL has shown substantial accuracy improvements . Chen et al . [ 5 ] improved performance on UNETR through pre - training via masked image modeling , which masks a portion of 3D medical images . In addition , Tang et al . [ 25 ] achieved state - of - the - art results using the SwinUNETR [ 9 ] on BTCV [ 18 ] and MSD [ 1 ] datasets by pre - training three pre - text tasks including image inpainting , 3D rotation prediction , and contrastive learning . As shown above , SSL can improve the performance of 3D image medical segmentation . However , SSL improvements may be limited by training data available , as SSL performance is often dependent on the amount of training data . We thus believe that the performance of the pre - training of 3D medical image segmentation will be further improved by solving the dataset construction issues . Formula - driven supervised learning ( FDSL ) . Recently , large - scale pre - training has made tremendous developments in computer vision [ 2 , 7 , 17 ] , and among its methods , FDSL can perform large - scale pre - training without real data and manual annotation [ 11 , 13 , 14 , 15 , 16 , 20 , 23 ] . Specifically , pre - training data and its label are automatically generated from math- ematical formulations based on real - world principles , such as fractal geometry and Perlin noise . Kataoka et al . [ 15 ] , proposed RCDB inspired that ViT pays attention to the outer con- tours of the fractal region when pre - training with the Fractal Database ( FractalDB ) . RCDB pre - trained model surpasses the ImageNet pre - trained model on ViT despite not learning nat- ural images . More recently , Yamada et al . [ 27 ] proposed the PC - FractalDB for pre - training in 3D object detection using 3D point clouds . They concluded that one factor for success in pre - training is initializing"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 4,
    "text": "on ViT despite not learning nat- ural images . More recently , Yamada et al . [ 27 ] proposed the PC - FractalDB for pre - training in 3D object detection using 3D point clouds . They concluded that one factor for success in pre - training is initializing not only the backbone network but also the entire model . Based on these findings , we hypothesize that synthetic pre - training through the same segmentation task , similar to the fine - tuning task , will have a greater effectiveness in 3D medical image segmentation using the transformer - based model . Furthermore , we think that learning from synthetic pre - training data , rather than from 3D medical images , can effec- tively address several issues commonly associated with 3D medical data usage . These issues include societal bias , privacy concerns , and copyright infringement . This paper notably extends the experiments in [ 22 ] and provides new contributions of our proposed method . \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG SR z - axis Similarity ratio along z - axis Primitive object generation 3 . Concave 4 . Convex 1 . Pillar 2 .   Cone z - axis SR SR：Similarity ratio z - axis z - axis z - axis SR SR SR Assembled object generation Place the shape while allowing for overlapping    Primitive object Assembled object SR = 2D primitive shapes Ellipse , 3 - poly , 4 - poly , 5 - poly ,    6 - poly ,    7 - poly , 8 - poly , 9 - poly xy - plane rule : z - axis rule : GeoSeg . We generate an assembled object by arranging randomly multiple primitive objects generated from the individual xy - plane and z - axis rules . PrimGeoSeg : Primitive Geometry Segment Pre - training In this section , we introduce PrimGeoSeg method , which is the pre - training strategy of gener- ating primitive geometric objects and performing segment pre - training for downstream tasks in 3D medical image segmentation . We generate an assembled object as pre - training data for PrimGeoSeg based on the design concept of the property of 3D medical images by ( i ) the variability among individuals and ( ii ) the complexity with ambiguous boundaries between organs . The pre - training dataset consisting of assembled objects and supervised labels de- noted by D = { ( Si , mi)}N i=1 , where Si ∈RW×H×D is an assembled object , mi ∈LW×H×D is a corresponding segmentation mask , and N is the number of pre - training data for PrimGeoSeg . L is a set of integers denoting the segmentation label . As shown in Figure 2 , the generation procedure of an assembled object is composed of two steps : ( i ) primitive object generation and ( ii ) arrangement of primitive objects ."
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 5,
    "text": "training data for PrimGeoSeg . L is a set of integers denoting the segmentation label . As shown in Figure 2 , the generation procedure of an assembled object is composed of two steps : ( i ) primitive object generation and ( ii ) arrangement of primitive objects . ( i ) First , we set a class of each primitive object based on xy - plane and z - axis rules . Moreover , we generate a primitive object based on randomly determined parameters regarding the num- ber of vertices in the xy - plane and the similarity ratio along the z - axis . ( ii ) Second , in the arrangement of primitive objects , we generate an assembled object Si and its corresponding segmentation mask mi by arranging multiple primitive objects in 3D space . Finally , we repeat ( i ) – ( ii ) steps N times to automatically construct pre - training dataset D for PrimGeoSeg . 3.1 Pre - training Data Generation Primitive object generation . Each primitive object is generated by stacking xy - plane slices , with the similarity ratio of each slice varying along the z - axis . The generation process of a primitive object is based on two rules : the xy - plane rule , which dictates the shape of the slices , and the z - axis rule , which controls the changing rate of the similarity ratio along the z - axis . We define a class of a primitive object considering combing the xy - plane and z - axis rules . We set the maximum 32 classes consisting of eight classes in the xy - plane rule and four classes in the z - axis rule ( see Figure 2 ) . The size of the primitive object along the z- axis , denoted as zmax , is randomly selected from a uniform distribution , zmax ∼U(10,50 ) . For each t in the range 0 ≤t ≤zmax , a slice Pt is generated . The similarity ratio of the slice at z = t is determined by a function f(z = t ) according to the z - axis rule . The z - axis rule \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG consists of four classes : { ‘ concave ’ , ‘ convex ’ , ‘ pillar ’ , ‘ cone ’ } in this paper . We define qz by randomly selecting from the above four classes . Here , the function f(z ) represents the similarity ratio of the slices along with the z - axis direction as shown below ; f(o1,o2,o3,z ) = \u001a o1 + ( o2 −o1 ) z zc ( 0 ≤z ≤zc ) o2 + ( o3 −o2 ) z−zc zmax −zc ( zc < z ≤zmax ) ( 1 ) where zc ∼U(3,zmax −3 ) and o1 , o2 , and o3 are defined as"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 6,
    "text": "f(o1,o2,o3,z ) = \u001a o1 + ( o2 −o1 ) z zc ( 0 ≤z ≤zc ) o2 + ( o3 −o2 ) z−zc zmax −zc ( zc < z ≤zmax ) ( 1 ) where zc ∼U(3,zmax −3 ) and o1 , o2 , and o3 are defined as certain values when qz was selected . For instance , in the ‘ pillar ’ class , all parameters are set to 1 : o1 = o2 = o3 = 1 . In the ‘ cone ’ class , we use o1 = 0 , o2 = z0 zmax , and o3 = 1 . For ‘ concave ’ , o1,o3 ∼U(0.8,1 ) , and o2 ∼U(0.2,0.5 ) . For ‘ convex ’ , o1,o3 ∼U(0.2,0.5 ) , and o2 ∼U(0.8,1 ) . Choosing a z - axis rule determines the values of o1 , o2 , and o3 , defining the unique function f(z ) . We generate the slice Pt using both f(z ) and the xy - plane rule . The xy - plane rule is defined by a set of shape definitions : { ‘ ellipse ’ , ‘ 3 - poly ’ , ‘ 4 - poly ’ , ‘ 5 - poly ’ , ‘ 6 - poly ’ , ‘ 7- poly ’ , ‘ 8 - poly ’ , ‘ 9 - poly ’ } , where w - poly represents a w - sided polygon . One shape definition qxy is selected from the above eight rules . To define the size of the slice Pt , we set parameters Rmin = 15 and Rmax ∼U(30,80 ) . If qxy is defined as a polygon , the slice Pt forms a closed shape bounded by edges in the set E(t ): V = \b ( rk cosθk , rk sinθk ) | 1 ≤k ≤Cxy ( 2 ) E(z = t ) = n f(t ) \u0010 vk + s(v(k+1 ) mod Cxy −vk ) \u0011 | vk ∈V,1 ≤k ≤Cxy,0 ≤s ≤1 o ( 3 ) where rk ∼U ( Rmin , Rmax ) , θk ∼U \u0010 2k Cxy π , 2(k+1 ) Cxy π \u0011 is polar coordinates and Cxy is the num- ber of vertices . We have tolerated the alignment of three adjacent vertices in a straight line as acceptable noise . If Kxy is defined as ‘ ellipse \" , Pt is a closed shape that satisfies the equation x2 ( a f(t))2 + y2 ( bf(t))2 = 1 , where a , b ∼U(Rmin , Rmax ) is the major and minor axes of the ellipse . By integrating the slices Pt from z = 0 to zmax along the z - axis , we create a single primitive object Il . By repeating M times , we generate a set of primitive objects G = { Il}M l=1 . Here , M is the number of primitive objects to be positioned in assembled"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 7,
    "text": "= 0 to zmax along the z - axis , we create a single primitive object Il . By repeating M times , we generate a set of primitive objects G = { Il}M l=1 . Here , M is the number of primitive objects to be positioned in assembled object Si . Arrangement of primitive objects . As shown in Figure 3 , we place a collection of M prim- itive objects from set G into a 3D volume F ∈RW×H×D in descending order of their volume . ment of primitive objects . Initially , the elements of the primitive objects set G are sorted in descending order according to the vol- umes of the primitive objects and are re - indexed as l′ to reflect the sorted order . We set a condition regard- ing overlaps for the arrangement of primitive objects . When placing the object Ij in 3D volume F , we de- fine the area already occupied by the objects { Il′ } j−1 l′=1 as A j , and the area that Ij occupies as B j. The con- dition for overlap stipulates that the volume overlap ratio , represented as O(A j∩Bj ) O(Bj ) , should be less than a threshold r. If this condition is met , we proceed with the placement . In this context , r denotes the maximum overlap ratio of the shapes , and O is a function representing the volume of the occupied region . The placement procedure for each primitive object comprises two main steps : ( 1 ) position selection and ( 2 ) arrangement . ( 1 ) position selection : randomly select a position of the center of the primitive object in the 3D volume F. ( 2 ) arrangement : placing the primitive object based on the over- lap condition shown in Figure 3 . If the overlap condition is met at the position from ( 1 ) , we \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG place the primitive object . If not , we revert to ( 1 ) . This ( 1 ) and ( 2 ) process is repeated up to a maximum of Max_iter (= 100 ) . If the ( 1 ) and ( 2 ) process fails after 100 iterations , the primi- tive object is rejected . We repeat the placement process for each object Il′ a total of M times , proceeding sequentially from l′ = 1 to l′ = M. The outcome of arranging the contours of the primitive objects is denoted as the assembled object Si . The outcome of placing the primitive objects filled in the interior is denoted as mi . In this study , we set the intensity values of the contours for the primitive objects within the assembled object to a fixed value , Intensity . We iteratively generate pairs ( Si , mi ) for N times , thereby automatically"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 8,
    "text": "in the interior is denoted as mi . In this study , we set the intensity values of the contours for the primitive objects within the assembled object to a fixed value , Intensity . We iteratively generate pairs ( Si , mi ) for N times , thereby automatically creating the pre - training dataset D for PrimGeoSeg . Combining various random parameters increases the diversity of geometric shapes within each class . We call this intra - class diversity of shapes instance augmentation . Please refer to the Supplementary Materials for the parameters and values required to generate the pre - training data for PrimGeoSeg . 3.2 Hypothesis and Motivation of PrimGeoSeg Reasons for independent rules in the xy - plane and z - axis : 3D medical images are created by constructing of xy slices and reconstructing them along the z - axis . We thus generate primitive objects by stacking slice images on the xy - plane according to the z - axis rule . Why is each xy - plane and z - axis rule defined as described above ? : 3D general object recognition recognizes diverse and complex 3D objects in the real world . On the other hand , 3D medical image segmentation recognizes only a limited number of 3D objects within the human body ’s internal anatomy . Therefore , in PrimGeoSeg , we considered that a certain number of primitive objects should be sufficient for 3D image segmentation . Why introduce the overlap when arranging primitive objects ? The internal structure of a human being is such that blood vessels can penetrate the interior of organs . In this case , 3D medical images are represented as if the blood vessels overlap a part of the organ region . Therefore , we introduce the overlap when arranging multiple primitive objects to create an assembled object in which the part region that overlaps more closely resembles the internal structure of the human body . Experiments 4.1 Experimental Settings Datasets . In this experiment , we evaluate the effectiveness of PrimGeoSeg using several datasets : BTCV [ 18 ] , MSD [ 1 ] , and the 2021 edition of the Multi - modal Brain Tumor Seg- mentation Challenge ( BraTS ) . BTCV has 30 samples for organ segmentation , and we split the BTCV training data in an 8:2 ratio for offline evaluation as in [ 5 ] . MSD has the lung ( Task06 ) with 63 samples , and the spleen ( Task09 ) has 41 samples for organ segmentation . In the MSD , we focused on the lung ( Task06 ) and the spleen ( Task09 ) due to computational resource constraints , and we split the training data in an 8:2 ratio for offline evaluation as in [ 28 ] . BraTS has 1,251 samples for brain tumor segmentation , we performed segmentation of three types of tumors : whole tumor ( WT ) , tumor"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 9,
    "text": "due to computational resource constraints , and we split the training data in an 8:2 ratio for offline evaluation as in [ 28 ] . BraTS has 1,251 samples for brain tumor segmentation , we performed segmentation of three types of tumors : whole tumor ( WT ) , tumor core ( TC ) , and enhancing tumor ( ET ) , splitting the training data in an 8:2 ratio for offline evaluation , as follows [ 9 ] . Architectures . We utilized the prominent transformer - based models , UNETR [ 10 ] and Swi- nUNETR [ 9 ] , as architectures for 3D medical image segmentation . Both UNETR and Swi- nUNETR have demonstrated their state - of - the - art performance on test leaderboards for the \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG BTCV and MSD , surpassing the capabilities of conventional CNN - based models . Implementation details . PrimGeoSeg executes the segmentation task with assembled ob- jects Si ∈R96×96×96 as input data and the mask mi ∈R96×96×96 as ground truth in pre- training . During pre - training of PrimGeoSeg , we used 96 × 96 × 96 patches , a batch size of 8 , a learning rate of 0.0001 , and a weight decay of 0.00001 , optimizing the dice loss . We employ AdamW [ 19 ] with a warmup cosine scheduler for training . For the number of iterations , when the pre - training data is { 5K,50 K } , the iterations are set to { 100K,375 K } . Concerning the pre - training data for PrimGeoSeg , constructing a dataset of 5,000 objects requires less than two hours on a 400GiB CPU memory system , and the storage used is un- der 3 GB . The pre - training process on NVIDIA A100 GPUs takes up to five GPU days for 100,000 iterations . For fine - tuning on BTCV , MSD , and BraTS , we follow the conditions of the hyperparameters on each fine - tuning dataset . For specific hyperparameter settings , we refer readers to the respective conventional research . Specifically , for SwinUNETR in BTCV and BraTS , refer to [ 25 ] ; and for UNETR in BTCV and MSD , consult [ 10 ] . In addition , please see [ 10 ] in terms of MSD . All experiments for downstream tasks are conducted using a dice similarity coefficient ( Dice ) as an evaluation metric . For more detailed settings , please refer to the supplementary materials . 4.2 Fundamental Experiments ( see Table 1 ) Fundamental experiments aim to clarify the effectiveness of PrimGeoSeg pre - trained model . We focus on five aspects : ( a ) effects of volumetric shapes , ( b ) effects of the number of classes , ( c ) effects of instance augmentation ( IA ) , ( d ) effects of overlap"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 10,
    "text": "the effectiveness of PrimGeoSeg pre - trained model . We focus on five aspects : ( a ) effects of volumetric shapes , ( b ) effects of the number of classes , ( c ) effects of instance augmentation ( IA ) , ( d ) effects of overlap , and ( e ) effects of dataset size . We pre - trained UNETR using 2,500 data of PrimGeoSeg for all experiments . Effects of volumetric shapes : This fundamental experiment ( a ) aims to compare the effec- tiveness of pre - training between planar shapes and volumetric shapes . We compare the effects of pre - training when arranging planar shapes and volumetric shapes in 3D space as shown in Fig- ure 4 . Dice metrics by 7.78 points compared to planar shapes . This result demonstrates that incorporating volumetric information leads to an improved performance of the pre - training for 3D medical image segmentation . Effects of the number of classes : This fundamental experiment ( b ) aims to investigate the validity of each xy - plane and z - axis rule of our generation method . Table 1b shows that both xy and z rules contribute to the effective pre - training . Moreover , the pre - training effect improves as the number of classes increases . For example , a maximum performance difference of +2.4 points was observed at 1 class and 32 classes . This result shows that the pre - training effect is enhanced even for primitive shapes when increasing diversity in the shape of the class . We clarify that the diversity of shapes in the xy - plane and z - axis directions in the 3D structure are both important factors in improving the pre - training effect . Effects of instance augmentation ( IA ): This fundamental experiment ( c ) aims to examine the pre - training effect of our proposed IA method , as it considers individual primitive object variations similar to 3D medical images . As demonstrated in Table 1c , IA improves +3.31 points compared to the performance when not using IA pre - training performance . Also , while IA improves accuracy by 3.31 points , class diversity enhances 2.4 points ( Table 1b ) . This result suggests that intra - class shape diversity holds equal or greater importance than inter - class shape diversity in pre - training for 3D medical image segmentation . \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG shapes , ( b ) Effects of the number of classes , ( c ) Effects of instance augmentation ( IA ) , ( d ) Effects of overlap , and ( e ) Effects of the number of pre - training data . ( a ) Shapes Dice Planar 69.11 Volumetric 76.89 ( b ) Classes Dice xy:1 , z:1 75.12 xy:1 , z:4 76.00 xy:8 , z:1 76.89 xy:8 , z:4"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 11,
    "text": "( d ) Effects of overlap , and ( e ) Effects of the number of pre - training data . ( a ) Shapes Dice Planar 69.11 Volumetric 76.89 ( b ) Classes Dice xy:1 , z:1 75.12 xy:1 , z:4 76.00 xy:8 , z:1 76.89 xy:8 , z:4 77.52 ( c ) IA Dice w/o IA 74.21 w IA 77.52 ( d ) Overlap Dice w/o overlap 77.52 w overlap 78.14 ( e ) Dataset size Dice 0.8 K 77.39 2.5 K 78.14 50 K 80.86 Effects of overlap : This fundamental experiment ( d ) aims to investigate the pre - training effect of overlapping among 3D primitive objects . Because we hypothesize that overlaps between primitive objects could assist pre - training performance by considering , for example , fuzzy boundaries and overlapping regions within the human body . Table 1d shows that overlapping 3D volumetric shapes led to a higher accuracy of +0.62 points . This result suggests that the overlap between primitive objects contributes toward improving the pre- training effect of 3D medical image segmentation . Effects of dataset size : One of the key advantages of PrimGeoSeg is its ability to generate primitive geometric objects automatically , which enables easily scaling of the pre - training dataset . As demonstrated in Table 1e , there is a positive correlation between the amount of pre - training data and the effectiveness of pre - training , where more data leads to better pre- training outcomes . Note that this experiment is limited to a certain amount of data size due to computational resource constraints . The above experimental results revealed that volumetric shape extensibility , the number of classes , IA , overlap between primitive objects , and data scalability in PrimGeoSeg con- tribute to pre - training effects . The above fundamental experiments offered valuable insights into the key elements essential for pre - training in 3D medical image segmentation . 4.3 Organ and Tumor Segmentation ( see Table 2 and Figure 5 ) In this section , we verify the effectiveness of PrimGeoSeg on organ segmentation ( BTCV and MSD Task09 ) and tumor segmentation ( MSD Task06 and BraTS ) . We employed [ 5 , 25 ] of the state - of - the - art SSL only because of limited computational resources and non - integrality of various conditions such as test data , architecture and input size . BTCV . In Table 2a , we compare the fine - tuning results of our proposed method , learning from scratch , and the recent state - of - the - art SSL [ 5 , 25 ] , respectively . PrimGeoSeg showed an overall higher recognition performance than Scratch for each class . Even when utiliz- ing an equivalent volume of pre - training data as with the SSL , we observed performance improvements : UNETR increased by 1.6 points and SwinUNETR by 0.4 points in average Dice score"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 12,
    "text": ". PrimGeoSeg showed an overall higher recognition performance than Scratch for each class . Even when utiliz- ing an equivalent volume of pre - training data as with the SSL , we observed performance improvements : UNETR increased by 1.6 points and SwinUNETR by 0.4 points in average Dice score . Moreover , as detailed in Section 4.2 , the performance of UNETR continued to improve as we increased the volume of pre - training data . It is worth noting that with only synthetic 3D pre - training data , the performance of our proposed method is superior to that of the baseline . In addition , Figure 5 shows several examples of SwinUNETR output results in BTCV . In areas that are over or under - segmented by Scratch and SSL , PrimGeoSeg is able to segment more accurately . Thus , we speculate that the distinct contours of PrimGeoSeg allow for the acquisition of more accurate 3D structual features during pre - training . MSD . Table 2b shows the accuracy when fine - tuning to MSD ( Task06 and Task09 ) . For lung segmentation , both UNETR and SwinUNETR , initialized by PrimGeoSeg improved \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG GeoSeg on BTCV , MSD , and BraTS in comparison to the previous SSL . The best value for each fine - tuning dataset is in bold . ( a ) Comparison of performance in BTCV . Pre - training PT Num Type Avg . Spl RKid LKid Gall Eso Liv Sto Aor IVC Veins Pan rad lad UNETR Scratch – 73.0 90.2 91.1 90.7 47.0 63.8 95.3 76.5 85.1 82.1 67.9 72.3 46.1 40.8 Chen et al . [ 5 ] 0.8 K SSL 75.8 95.2 95.5 93.8 51.9 52.3 98.8 80.0 87.8 82.7 66.1 68.9 60.8 51.3 PrimGeoSeg 0.8 K FDSL 77.4 88.9 94.0 93.8 59.8 65.7 95.4 79.3 88.3 82.6 69.9 76.8 58.5 53.3 PrimGeoSeg 50 K FDSL 80.9 95.7 94.2 94.1 61.9 69.6 96.7 85.5 89.5 84.4 74.7 81.9 64.3 58.7 SwinUNETR Scratch – 78.3 92.3 93.2 93.8 55.9 61.3 94.0 77.0 87.5 80.4 74.2 76.1 68.8 63.6 Tang et al . [ 25 ] 5 K SSL 81.6 95.3 93.2 93.0 63.6 74.0 96.2 79.3 90.0 83.3 76.1 82.3 69.0 65.1 PrimGeoSeg 5 K FDSL 82.0 95.7 94.4 94.4 61.0 75.5 96.7 83.3 89.1 85.6 75.2 84.3 67.9 62.4 ( b ) Comparison of performance in MSD . UNETR SwinUNETR Pre - training Type Lung Spleen Lung Spleen Scratch – 52.5 95.0 63.5 96.3 Tang et al . [ 25 ] SSL – – 65.2 96.5 PrimGeoSeg FDSL 62.2 96.3 67.9 96.6 ( c ) Comparison of performance in BraTS . UNETR SwinUNETR Pre - training Type Avg . ET WT TC Avg . ET WT TC Scratch – 88.1 84.8 91.3 88.1 90.0 86.8 92.9 90.3 PrimGeoSeg FDSL 88.7 85.6 91.8 88.9 90.3 87.0 92.9 91.0 dashes indicate more accurately identified areas ."
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 13,
    "text": ") Comparison of performance in BraTS . UNETR SwinUNETR Pre - training Type Avg . ET WT TC Avg . ET WT TC Scratch – 88.1 84.8 91.3 88.1 90.0 86.8 92.9 90.3 PrimGeoSeg FDSL 88.7 85.6 91.8 88.9 90.3 87.0 92.9 91.0 dashes indicate more accurately identified areas . accuracy compared to training from scratch by 9.7 points and 4.4 points , respectively . In ad- dition , PrimGeoSeg on SwinUNETR showed a 2.7 points accuracy improvement compared to SSL . For spleen segmentation , when using PrimGeoSeg on UNETR and SwinUNETR , PrimGeoSeg exhibited accuracy improvements from Scratch of 1.3 points and 0.3 points , respectively . PrimGeoSeg on SwinUNETR had a 0.1 points accuracy improvement com- pared to SSL . From this result , PrimGeoSeg demonstrates superior pre - training performance without depending on a specific dataset . BraTS . We verify the effectiveness of PrimGeoSeg for tumor segmentation . Due to the diffi- culty in conducting a fair comparison with other pre - training methods , we primarily focused on comparing PrimGeoSeg with Scratch . The BraTS results shown in Table 2c , indicate that PrimGeoSeg improves accuracy by approximately 0.5 points for ET , WT , and TC , respec- tively . Interestingly , although PrimGeoSeg is designed considering the key elements of the human body ’s internal structure , it has been proven effective for brain tumor segmentation . This suggests that PrimGeoSeg is capable of acquiring a 3D structural representation . \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG 4.4 Pre - training Effect on Limited Training Data ( see Figure 6 ) In 3D medical image segmentation , achieving accurate recognition using a small number of 3D medical images is considered ideal . Figure 6 illustrates the results of limited training data in BTCV . Specifically , this experiment used training data ( 10 % [ 2 samples ] , 30 % [ 6 samples ] , 70 % [ 15 samples ] ) to compare the performances of Scratch and PrimGeoSeg . As shown in Figure 6 , the accuracy improvements of PrimGeoSeg over Scratch were { 19.9 , 20.3 , 17.9 } , respectively . Even more surprising , PrimGeoSeg uses only 30 % of the training data , yet it achieves a performance comparable to that of Scratch , which uses 100 % of the training data . This result demonstrates that PrimGeoSeg is beneficial , even with limited training data . Therefore , we consider it a promising pre - training approach to handling limited training data for 3D medical image segmentation . Conclusion This paper demonstrated the effectiveness of pre - training the proposed PrimGeoSeg , result- ing in significant accuracy improvements compared to training from scratch for organ and tumor segmentation . Our proposed method also showed equal or superior performance to self - supervised learning . The findings through experimental results are described below ; The effect of the intra -"
  },
  {
    "source": "2401.03665v1.pdf",
    "chunk_index": 14,
    "text": "proposed PrimGeoSeg , result- ing in significant accuracy improvements compared to training from scratch for organ and tumor segmentation . Our proposed method also showed equal or superior performance to self - supervised learning . The findings through experimental results are described below ; The effect of the intra - class diversity . We observed that despite organs in the human body being essentially identical , the size and shape of these organs differ from person to person . In light of this observation , we experimented with a variety of 3D object types and shapes while building PrimGeoSeg . The results demonstrated that increasing the diversity of 3D objects contributes to the enhancement of medical image segmentation performance ( see Table 1c ) . The effect of spatial overlap of 3D objects . We also focused on the fact that the anatomical structure of the human body is complex and the boundaries between different tissues and or- gans are ambiguous , resulting in overlapping regions . Through our exploratory experiments on overlap ( see Table 1d ) , it became clear that incorporating a certain amount of overlap can help improve performance . While we empirically confirmed the performance enhancement due to shape pre - training , further analytical justification is required . Although the current focus is on addressing data scarcity in segmentation tasks , we plan to investigate the applicability of our method to other domains , such as 3D medical image classification and registration , in future research . \n TADOKORO , YAMADA , NAKASHIMA , NAKAMURA , KATAOKA : PRIMGEOSEG Acknowledgement . Computational resource of AI Bridging Cloud Infrastructure ( ABCI ) provided by the National Institute of Advanced Industrial Science and Technology ( AIST ) was used . We want to thank Hideki Tsunashima , Hiroaki Aizawa , Shinagawa Seitaro , and Takuma Yagi for their helpful research discussions ."
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 0,
    "text": "Text2MDT : Extracting Medical Decision Trees from Medical Texts Wei Zhu1a , Wenfeng Lia , Xing Tianc , Pengfei Wanga , Xiaoling Wanga , Jin Chenb , Yuanbin Wua , Yuan Nic , Guotong Xiec aDepartment of Computer Science and Technology , East China Normal University , Shanghai , China bUniversity of Kentucky , Kentucky , United States cPingan Health Tecnology , Shanghai , China Abstract Knowledge of the medical decision process , which can be modeled as medical decision trees ( MDTs ) , is critical to build clinical decision support systems . How- ever , the current MDT construction methods rely heavily on time - consuming and laborious manual annotation . In this work , we propose a novel task , Text2MDT , to explore the automatic extraction of MDTs from medical texts such as med- ical guidelines and textbooks . We normalize the form of the MDT and create an annotated Text - to - MDT dataset in Chinese with the participation of medical experts . We investigate two different methods for the Text2MDT tasks : ( a ) an end - to - end framework which only relies on a GPT style large language models ( LLM ) instruction tuning to generate all the node information and tree structures . ( b ) The pipeline framework which decomposes the Text2MDT task to three sub- tasks . Experiments on our Text2MDT dataset demonstrate that : ( a ) the end - to- end method basd on LLMs ( 7B parameters or larger ) show promising results , and successfully outperform the pipeline methods . ( b ) The chain - of - thought ( COT ) prompting method Wei et al . ( 2022 ) can improve the performance of the fine- tuned LLMs on the Text2MDT test set . ( c ) the lightweight pipelined method based on encoder - based pretrained models can perform comparably with LLMs with model complexity two magnititudes smaller . Our Text2MDT dataset is open- sourced at https://tianchi.aliyun.com/dataset/95414 , and the 1Corresponding Author . Email : wzhu@stu.ecnu.edu.cn . Address : Department of Computer Science and Technology , East China Normal University , No . 3663 , Zhongshan Road , Putuo Dis- trict , Shanghai , China , 200050 . Preprint submitted to AIIM January 5 , 2024 arXiv:2401.02034v1 [ cs . CL ] 4 Jan 2024 \n source codes are open - sourced at https://github.com/michael-wzhu/ text2dt . Keywords : Medical decision trees , medical information extraction , pre - training language models , large language models . 1 . Introduction As a typical application of artificial intelligence in the medical field , clini- cal decision support systems ( CDSS ) have been widely concerned by researchers Tsumoto ( 1998 ) ; Fotiadis et al . ( 2006 ) ; Machado et al . ( 2017 ) . CDSS can suggest experienced doctors of all the options and problems to be considered when mak- ing decisions , help inexperienced medical students to"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 1,
    "text": "been widely concerned by researchers Tsumoto ( 1998 ) ; Fotiadis et al . ( 2006 ) ; Machado et al . ( 2017 ) . CDSS can suggest experienced doctors of all the options and problems to be considered when mak- ing decisions , help inexperienced medical students to learn clinical knowledge , or give medical advice to patients without medical background IoannisVourgidis et al . ( 2018 ) . The core of building a CDSS is the knowledge of medical decision processes , which are rules that link given conditions to medical decisions Abra- ham ( 2005 ) and are usually modeled as medical decision trees ( MDTs ) . However , existing methods for constructing MDTs rely on manual tree construction by med- ical experts Saibene et al . ( 2021 ) , which is time - consuming , laborious , and can not absorb the latest research timely . All these hinder the construction , dissemina- tion , maintenance of large - scale CDSS Nohria ( 2015 ) . There is an unmet need to explore automated pipelines to precisely extract MDTs from vast and rapidly growing medical knowledge sources . It is computationally challenging to automatically extract MDTs for the fol- lowing reasons : 1 ) the current MDT lacks a normalized and structured form , lead- ing to ambiguity in understanding medical decision knowledge and therefore hin- ders automated knowledge extraction ; 2 ) the NLP community lacks a benchmark dataset for training and validating MDT extraction tasks ; and constructing such data is challenging in that annotating medical decision trees requires in - depth do- main knowledge ; 3 ) existing methods for medical information extraction are not directly applicable for MDT extraction . In this work , Text2MDT is defined as an automated task to explore the au- tomatic extraction of MDTs from medical texts such as medical guidelines and textbooks . To this end , we structure and normalize a specific tree structure to model medical decision knowledge . As shown in Figure 1 , the knowledge of a medical decision process embedded in the medical text can be modeled as a bi- nary decision tree consisting of condition nodes ( orange diamond ) and decision nodes ( blue rectangle ) . The triplets with their logic relationships in the nodes repclinical guideline . English translation are provided in brackets . resent the conditional judgment to be performed or the decisions to be made based on the previous conditional judgment . Suppose the result of the conditional judg- ment is ” Yes ” ( ” No ” ) , go to the left ( right ) branch for the following conditional judgment or decision . Once a decision is made , the medical process is terminated . In summary , the new MDT reflects the triplets in the text that represent medi- cal knowledge and connects this information to form a complete decision - making process . We construct the first"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 2,
    "text": "judgment or decision . Once a decision is made , the medical process is terminated . In summary , the new MDT reflects the triplets in the text that represent medi- cal knowledge and connects this information to form a complete decision - making process . We construct the first Text - to - MDT ( Text2MDT ) becnhmark dataset with 500 Text2MDT pairs and 3,232 triplets for automatically extracting MDTs from medical texts . Medical guidelines and textbooks , which are referential for clinical decision making , are used as knowledge sources . Trigger words and templates customized by medical experts are used to locate medical text fragments that contain knowledge for clinical decision making . Finally , well - trained annotators and medical experts complete the annotation manually . With the constructed Text2MDT benchmark , we conduct a systematic eval- uation of different pretrained model based methods . The first cohort of meth- ods we consider is from the pipeline framework , in which the Text2MDT task is decomposed into three subtasks : triplet extraction , node grouping and tree as- sembling . Note that the existing encoder - based information extraction research can not be directly applied to deal with our novel Text2MDT task in a end - to- end ( end2end ) fashion , they can be applied in each subtask . We also consider generation - based methods for the pipeline framework . The second cohort of meth- ods are all end2end methods . For the end2end framework , we mainly consider uti- lizing the generation capabilities of the pretrained generative LMs , especially the current large language models ( LLMs ) . Notably , the chain - of - thought Wei et al . ( 2022 ) ( COT ) style reasoning is investigated , which demonstrates to be beneficial . Experiments on our Text2MDT benchmark show promising results . In summary , the main contributions of this work are : • We propose a well - defined novel task Text2MDT aiming to automatically extract MDTs from medical text . • We construct the first Text2MDT benchmark dataset with the help medical practitioners . • Both the pipelined and end2end models are investigated , including encoder- based methods and LLM fine - tuning . The experiments show that LLMs can perform strongly on our Text2MDT benchmark , however the encoder - based models can also perform comparably by utilizing a series of models dealing with different subtasks in the pipeline . • The Text2MDT dataset and source codes are openly available2 , to facilitate future research . 2 . Related Work 2.1 . Medical natural language processing The developments in neural networks and natural language processing has advanced the field of medical natural language processing ( MedNLP ) Zhou et al . 2https://github.com/michael-wzhu/text2dt \n ( 2021 ) ; Hahn and Oleynik ( 2020 ) ; Zhu et al . ( 2021b ) . In the pre - BERT era , firstly , RNNs like"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 3,
    "text": "language processing has advanced the field of medical natural language processing ( MedNLP ) Zhou et al . 2https://github.com/michael-wzhu/text2dt \n ( 2021 ) ; Hahn and Oleynik ( 2020 ) ; Zhu et al . ( 2021b ) . In the pre - BERT era , firstly , RNNs like LSTM / GRU are used for processing sequential medical data such as text and speech Beeksma et al . ( 2019 ) . Convolutional networks are also used for medical text classificaiton Hughes et al . ( 2017 ) . The techniques of Graph neural networks are also explored for diagnose recommendations Li et al . ( 2020 ) . In this period , many different model architectures are specially designed for better per- formances on a specific MedNLP task Zhu et al . ( 2021b , c ) ; Zhang et al . ( 2021 ) . Since BERT Devlin et al . ( 2018 ) , the pretrained language models ( PLMs ) become the deafult solution for MedNLP . In this stage , researchers become less interested in modifying the model architecture , but instead trying to pretrain or further pre- train a PLM from the open domain to the medical domain Guo et al . ( 2021a ) ; Zhu ( 2021b ) ; Gu et al . ( 2020 ) . With the wide study of LLMs , the field of MedNLP is also being revolutionized . There are already works on adapting LLM back- bones to the medical domain question answering Zhu and Wang ( 2023 ) . And Zhu et al . ( 2023 ) propose PromptCBLUE , a prompt learning based benchmark dataset for examing the LLMs ’ ability in MedNLP tasks . This work can also serve as a testbed for the current commercial or open - sourced LLMs , since the complexity of our novel task will pose great challenges for them . 2.2 . Information extraction from medical texts Information Extraction ( IE ) is a research topic of long history that aims to extract structured knowledge or factual information from unstructured texts Yang et al . ( 2022 ) . The field of IE includes a wide range of tasks , such as named entity recognition Das et al . ( 2022 ) ; Landolsi et al . ( 2023 ) , relation extraction ( RE ) Zhu et al . ( 2020 ) ; Li et al . ( 2022 ) , event extraction Hsu et al . ( 2022 ) , aspect - level sen- timent analysis CHENG et al . ( 2023 ) . Since the raise of pre - trained models like BERT Devlin et al . ( 2018 ) , the performances on IE tasks have advanced greatly Zhu ( 2021b ) . But one has to have different model structures for different fine- grained IE tasks , for instance , the SOTA nested NER models Zhang et al . ( 2022b )"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 4,
    "text": "et al . ( 2018 ) , the performances on IE tasks have advanced greatly Zhu ( 2021b ) . But one has to have different model structures for different fine- grained IE tasks , for instance , the SOTA nested NER models Zhang et al . ( 2022b ) are different from those of discontinuous NER tasks Zhang et al . ( 2022c ) . Re- cently , there is a trend that all the IE task should be solved by a unified paradigm , that is , Seq2Seq generation . Yan et al . ( 2021a ) proposes the framework of Bart- NER which solves all types of NER tasks with a BART model Lewis et al . ( 2019 ) . UIE Lu et al . ( 2022 ) takes a step ahead and proposes to use prompts and a uni- fied structural language to deal with many types of IE tasks with a single model checkpoint . Medical information extraction is an important research field , and it has broad applications like medical search engine , automatic electronic health record analysis , online health consultation , and medical knowledge graph construction Sun et al . ( 2020 ) ; Guo et al . ( 2021b ) ; Zhu et al . ( 2019 ) ; Zhou et al . ( 2019 ) ; Zhu et al . ( 2021b , a ) ; Zhang et al . ( 2023a ) . Compared with open - domain IE tasks , the IE tasks are known for their complexity . For example , discontinuous or nested en- tities are common in the medical field . And knowledge in the medical domain may be too complex to be expressed as triplets Zhu et al . ( 2023a ) . For example , Jiang et al . ( 2019 ) introduced the role of “ condition ” and argued that a fact triplet is established based on some conditional triplets in the biomedical field . In the CMedCausal Li et al . ( 2023b ) task , a triplet may be the result of a subject con- ducting certain behaviour , expressing the causal relations . With the rise of LLMs , the research field of IE and medical IE is also under revolution . In this work , we compliment the existing literature by constructing the challenging Text2MDT task , where not only triplets have to be extracted , but also they need to arranged into nodes of a binary tree to express a complex medical decision process . 2.3 . Text2Tree modeling There are a rich history of NLP tasks that aim to extract tree structures from a given text . The most fundamental task in NLP is syntax analysis , which aims to express the syntactic structure of a sentence into a syntactic tree Zhang ( 2020 ) . Parsing often relies on specific grammars , which are used to refine the output structures of syntax and"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 5,
    "text": "given text . The most fundamental task in NLP is syntax analysis , which aims to express the syntactic structure of a sentence into a syntactic tree Zhang ( 2020 ) . Parsing often relies on specific grammars , which are used to refine the output structures of syntax and semantics . Two of the most popular grammars are con- stituent parsing and dependency parsing . Text2Tree are also seen in many appli- cation scenarios . Math word problems ( MWPs ) Zhang et al . ( 2022d ) ; Zhao et al . ( 2023b ) extracts mathematical expressions from the unstructured texts , and try to improve the neural networks ’ capabilities in math problem solving by asking the model to understand the tree structure . Semantic parsing Kamath and Das ( 2018 ) , the task of transforming the unstructured text into a SQL query , has promising application potentials in areas like dialogue systems , search engine , business in- telligence . Our Text2MDT task is novel compared to the literature in the following sense : ( a ) Text2MDT focus on extracting medical decision trees from unstructured medical texts . ( b ) our task has a different granularity with the existing Text2Tree tasks , since each node in our task consists of one or more triplets . ( c ) the tree structure , or the links among different nodes , have different meanings with the existing Text2Tree tasks . In terms of the model architectures for the existing Text2Tree methods , we have seen a trend of idiosyncratic models to more unified model architectures . The field of syntactic analysis has seen many different model architectures , such as recursive neural network Socher et al . ( 2011 ) , CRF Sutton and McCallum \n ( 2010 ) , transition - based models like Fernandez Astudillo et al . ( 2020 ) ; Zhang et al . ( 2016 ) , graph - based models Pei et al . ( 2015 ) . With the rise of pre - trained encoder models Devlin et al . ( 2019 ) , a series of works apply the pre - trained mod- els like BERT to enhance the performances on the Text2Tree tasks . For example , Dozat and Manning ( 2017 ) proposes to install the biaffine module on top of a pre- trained BERT for the dependency parsing task . This method models the relations among token pairs as a table - filling task and decode the tree structures of the en- tire input sequence in one forward pass . With the advances of generative language models , many works apply the pretrained sequence - to - sequence ( Seq2Seq ) mod- els or GPT style models to Text2Tree tasks Wang et al . ( 2018 ) ; Zhong et al . ( 2017 ) . Since the generative models generate sequences that ignore the constraints of the tree , a series of"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 6,
    "text": "sequence - to - sequence ( Seq2Seq ) mod- els or GPT style models to Text2Tree tasks Wang et al . ( 2018 ) ; Zhong et al . ( 2017 ) . Since the generative models generate sequences that ignore the constraints of the tree , a series of approaches Xie and Sun ( 2019 ) ; Yu et al . ( 2018 ) are devoted to add constraints for tree - structured decoders by utilizing the structural information or syntactic rules . In this work , we contribute to the existing literature by conduct- ing a systematic evaluation of the encoder - based and generation - based methods , especially open - sourced generative LMs with different scales . 3 . Problem Definition 3.1 . Text2MDT Task As shown in Figure 1 , the Text2MDT task focuses on extracting the MDT from a given text containing the medical decision process from medical guidelines or textbooks . We denote a medical text with n text words as X = [ x1 , x2 , ...... , xntext ] , the goal of Text2MDT is to generate the pre - order sequence of the nodes in the MDT T = [ N1 , N2 , ...... , Nnnode ] . The pre - order sequence of the nodes in the MDT can uniquely represent this tree , which we explain in detail in Section 3.2 . 3.2 . Medical Decision Tree Node structure Nodes in a MDT consist of three parts : role , triplets , and logical relationship between triplets . We denote a node by Node = { Role , Triplets , Logical Rel } , Role = 3 or 2 , Triplets = ( t1 , t2 , ... , tntri ) , Logical Rel = AND , OR or NULL , ( 1 ) \n where : ( a ) Role denotes the role of the node . Role = 3 means that the node is a condition node describing certain statuses of patients ( presented as diamond- shaped nodes in Figure 1 ) , while Role = 2 means that the node is a deci- sion node demonstrating how to treat the patients given certain conditions . ( b ) Triplets = ( t1 , t2 , ... , tntri ) denotes the collection of triplets extracted from the given text , where each triplet t = ( sub , rel , obj ) consists of a subject sub , a re- lation rel , and a object obj . These triplets are used to describe medical contents , either a patient ’ medical condition or status , or a medical decision representing the medical procedure to treat the patients . ( c ) Logical Rel denotes the logical relationship ( and/or relation ) among the Triplets in a node . Note that l = null if and only if the number of triplets ntri in the node is less or equal to 1 ."
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 7,
    "text": "procedure to treat the patients . ( c ) Logical Rel denotes the logical relationship ( and/or relation ) among the Triplets in a node . Note that l = null if and only if the number of triplets ntri in the node is less or equal to 1 . Tree structure . A medical decision tree represents the structured process for decision making of physicians . As depicted in Figure 1 , medical professionals need to identify the condition of patients , and make the according decisions . Sometimes , medical conditions are complex so that one may have to differentiate many levels of conditions before they can make a valid medical decision . Therefore , we define a MDT as a binary tree consisting of condition and decision nodes , where non- leaf nodes are called conditional nodes , and leaf nodes are decision nodes . For the condition node , when the conditional judgment result is ” Yes ” ( ” No ” ) , it will go to the left ( right ) branch for the next condition judgment or decision . It should be noted that each condition node has left and right child nodes . If the subsequent operation that needs to be done after the result of the condition judgment is ” Yes ” ( ” No ” ) is not reflected in the text , a decision node without triplets is used as the left ( right ) child node . After this operation , a decision tree can be represented by a preorder sequence of its nodes . cision process embedded in the medical text above can be modeled by the MDT below : 1 ) Firstly , the condition ” whether valproic acid is applicable for patients with generalized tonic - clonic seizures ” is determined , and if the result is ” Yes , ” i.e. , valproic acid is applicable , then go to the left branch and make the corre- sponding decision , i.e. , valproic acid is used for treatment ; 2 ) if the result is ” No , ” that is , valproic acid is not applicable , next go to the right branch and make an- other conditional judgment , i.e. , the condition ” whether the patient has myoclonic seizures or suspected juvenile myoclonic epilepsy ” is determined , and go to dif- ferent branches according to the result . \n 4 . Text2MDT Dataset 4.1 . Data Collection We choose clinical practice guidelines and clinical medicine textbooks as our data sources . Clinical practice guidelines are systematically developed multidis- ciplinary clinical guidelines that help clinicians , patients , and other stakeholders make appropriate management , selection , and decisions about specific clinical is- sues . Clinical medicine textbooks are the primary means medical students acquire medical knowledge and can be used as a reference for clinical decision - making . We collected more than 100 clinical guidelines published by authoritative medical"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 8,
    "text": "make appropriate management , selection , and decisions about specific clinical is- sues . Clinical medicine textbooks are the primary means medical students acquire medical knowledge and can be used as a reference for clinical decision - making . We collected more than 100 clinical guidelines published by authoritative medical institutions about 30 clinical departments from 2011 to 2021 and undergraduate clinical medical textbooks published by People ’s Health Publishing House3 to build our dataset . Since medical texts are long and contain rich and various medical knowledge , we used section - based filtering and trigger / template - based filtering to locate seg- ments of medical texts that have knowledge of medical decision process based on the analysis of medical texts and the help of specialized doctors . First , we selected the chapters with a high density of medical decision knowledge , such as ” Treatment ” , ” Drug Selection ” and ” Medical Solutions ” in the source data . Then , we analyzed and summarized the structure and pattern of the medical decision text construct templates and trigger words for medical decision knowledge . We filtered the text based on the template and triggers to obtain the text fragments containing the knowledge of the medical decision process . Annotators of our dataset include 4 annotators and 2 medical experts . All the annotators have linguistic knowledge and are instructed with detailed and formal annotation principles for at least two hours , including understanding the medical decision - making process , the judgment of logical relationships , and the annotation specifications of triplets and decision trees . Two well - trained annotators firstly in- dependently annotated each text and revised the initial annotation after discussion . Medical experts will examine the revised annotation to avoid errors or omissions and make the final decision . It is discarded if an annotation can not be agreed upon in the discussion or is ambiguous . Furthermore , we calculate the Cohen ’s Kappa Cohen ( 1960 ) to measure the agreements between two annotators . The result of triplet annotation is 0.83 , which indicates a high degree of consistency ; the result of MDT annotation is 0.37 , which indicates a degree of consistency . 3http://www.pph166.com/. \n Tree Depth Amount Proportion 26.80 % 60.40 % 12.80 % Relation Name Amount Proportion clinical feature 42.51 % therapeutic drug 28.15 % medical option 17.36 % usage or dosage 6.87 % forbidden drug 2.57 % basic information 2.54 % 4.2 . Data Statistics are 500 text - tree pairs in the Text2MDT dataset ; the decision tree depth is 2 to 4 , the average number of nodes per tree is 3.76 , and the average number of triplets per tree is 6.46 . There are 1896 nodes in the dataset , including 934 decision nodes , 962 conditional nodes , 476 “ or ” nodes , 367 “ and ” nodes , and 1053 “ null ” nodes ."
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 9,
    "text": "is 3.76 , and the average number of triplets per tree is 6.46 . There are 1896 nodes in the dataset , including 934 decision nodes , 962 conditional nodes , 476 “ or ” nodes , 367 “ and ” nodes , and 1053 “ null ” nodes . Text2MDT dataset has 6 relationships , where the relationship ” prohibited drug ” only accounts for 2.57 % percent of the total number of triplets , so the dataset exhibits long - tailed distributions . It should be noted that the triplet extraction in Text2MDT has the problem of SingleEntityOverlap ( SEO ) , i.e. , triplets in medical text share a single entity . 4.3 . Manual Evaluation of Medical Decision Trees To evaluate the quality of the annotated medical decision tree and whether it can help make medical decisions , we invited 10 medical practitioners and 10 people without a medical background to complete the following two evaluation tasks : 1 ) We observed the subjects ’ performance ( accuracy and time spent ) in answering medical decision problems of similar difficulty under different settings ( with medical texts or decision trees as a reference ) . 2 ) We asked subjects to \n evaluate the ability of medical texts and decision trees to represent the medical decision process ( completeness , readability , helpfulness ) . Most of the subjects could answer the decision - making questions more accu- rately or faster with the help of the MDTs and thought that our annotated MDTs are more readable and helpful for understanding the knowledge of the medical de- cision process while providing a comprehensive representation of decision knowl- edge in medical texts . This demonstrates the quality of our annotation and the strength of the decision tree in terms of expressive power . Besides , we provide a detailed manual evaluation of MDT in the Appendix . grouping and tree assembling . 5 . Methods of modeling Text2MDT In this section , we will elaborate on our proposed methods for modeling the task of Text2MDT . First , we will present each module of the pipeline framework for Text2MDT . Then , we will discuss the end - to - end framework . 5.1 . Pipelined framework steps : triple extraction , node grouping , and tree assembling . \n 5.1.1 . Triplet Extraction The first step is to extract all the triplets representing either decisions or conditions from medical texts with a unified triplet extraction model TEModel ( ): { t1 , ... , tntri } = TEModel ( [ x1 , ...... , xntext ] ) , ( 2 ) where ti = ( si , ri , oi ) is the i - th triplet in the text , representing a part of a decision or a condition . si and oi are two entity spans from the given text , and ri is a relation between the two entities and is one of the"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 10,
    "text": "ri , oi ) is the i - th triplet in the text , representing a part of a decision or a condition . si and oi are two entity spans from the given text , and ri is a relation between the two entities and is one of the relation types presented in Table 2 . Triplet extraction is widely studied task Zhu ( 2021a ) ; Gao et al . ( 2023 ) ; Zhu ( 2021c ) ; Zhu et al . ( 2021c ) , and there are many recent works that can be utilized to complete this subtask . One line of work is based on semantic encoders like BERT Devlin et al . ( 2019 ) and a table - filling module Dozat and Manning ( 2016 ) ; Zhang et al . ( 2023b ) . The representative methods in this direction is : CASREL Wei et al . ( 2020 ) , TPLinker Wang et al . ( 2020 ) and UNIRE Wang et al . ( 2021 ) . For com- pleteness , we now demonstrate how UNIRE Wang et al . ( 2021 ) applies a biaffine module to complete the entity mention detection and relation classification tasks simultaneously . With a given sentence input X , a pre - trained encoder like BERT or RoBERTa will encode the semantic information and provide hidden representations for X ′. Denote the hidden vector corresponding each token xi as hi ∈Rd . Denote the set of entity types as Ke , and the set of relation types as Kr . UNIRE targets at identifying the label li , j of each token pair ( i , j ) . That is , if the token pair ( i , j ) is classified as an entity type ke ∈Ke , we will consider the text span starting from the i - th token and ending at the j - th token as an entity of type ke . And if the token pair ( i , j ) is classified as an relation type kr ∈Kr , and token i and j are the starting tokens of two entity mentions , we will consider that these two entities have a relation of type kr . To complete the two tasks with a single calculation step , the UNIRE construct a biaffine module which maps each token pair ( i , j ) to a probability distribution of dimension K = |Ke| + |Kr| + 1 : 4 P(li , j ) = Biaffine ( hi , hj ) , ( 3 ) where Biaffine ( ) is given by Biaffine(h1 , h2 ) = hT 1 Uh2 + W ( h1 ⊕h2 ) , ( 4 ) 4Adding 1 for the null type . \n Since we need to calculate the scores for K categories , U is a d × K × d tensor , and W is a 2d ×"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 11,
    "text": ") = hT 1 Uh2 + W ( h1 ⊕h2 ) , ( 4 ) 4Adding 1 for the null type . \n Since we need to calculate the scores for K categories , U is a d × K × d tensor , and W is a 2d × K tensor.5 Since the above method is analogeous as filling in a ntext × ntext sized table , we often refer to the biaffine method as the table - filling method . Denoting the ground truth of li , j as yi , j , then the training objective is the summation of cross - entropy loss at each of L = − |ntext|2 |ntext| X i=1 |ntext| X j=1 log P ( li , j = yi , j ) . ( 5 ) After the above BERT - based biaffine model is trained , the inference procedure follows UNIRE Wang et al . ( 2021 ) . 5.1.2 . Node grouping Given the medical text X = [ x1 , ...... , xntext ] and the triplets { t1 , ... , tntri } extracted from this text , we now need to group these triplets into different groups , i.e. , nodes , with relation l ∈(and , or , null ) ( a triple constitutes a group if it has the null relation with other triples ) . These groups will be the main components of nodes of the MDT . Now we will demonstrate the model for this subtask : node - grouping biaffine ( NG - Biaffine ) , which is to adapt the idea of biaffine model to the node group- ing task . Note that if a triple belongs to a node with relation l ∈KNG ( where KNG = and , or , null is the set of the logical relations among triplets . ) , it will have relation l with any other triplet within the group and null relation with other triplets in the other groups . Thus , the key step for node grouping is to deter- mine the relationships among the triplets , which can be conveniently modeled by a table - filling task similar to Equation 3 . Denote the augmented text input as X ′ = [ X , [ t ] , t1 , ... , [ t ] , tntri ] , where [ ] denotes the text concatenation operation . Note that we add a special token [ t ] before each triplet . A pre - trained encoder like BERT or RoBERTa will encode the semantic information and provide hidden representations for X ′ , and obtain the semantic representation of triplet ti by tak- ing the hidden vector corresponding the special token right before ti ( denoted as h(ti ) ) . Then a biaffine module will handle the classification task for each triplet pair ( ti , tj ) by calculating its probability P(lti , tj ) distribution over"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 12,
    "text": "triplet ti by tak- ing the hidden vector corresponding the special token right before ti ( denoted as h(ti ) ) . Then a biaffine module will handle the classification task for each triplet pair ( ti , tj ) by calculating its probability P(lti , tj ) distribution over all the relation categories . 5Note that in the BERT biaffine NER Yu et al . ( 2020 ) , two feed forward layers are designated to transform the two features passing to the biaffine module . However , we find that dropping the two feed forward layers will not result in significant performance changes . \n During inference , we will consider a score based decoding procedure for re- solving possible conflicts . For each triplet pair ( ti , tj ) , its label lti , tj is obtained by choosing the relation category that receives the highest probability mass . And denote the probability mass of lti , tj as mti , tj . During inference , we first calculate mti , tj and lti , tj for each triplet pair ( ti , tj ) in a single forward pass . And we rank lti , tj by mti , tj . The relation lti , tj that receives the highest mti , tj value will first be established , and any conflicting relation predictions with lower scores will be rejected . Here , a conflict arises when a triplet ti has the and relation with tj , but also has the or relation with another triplet tj′. Then we will establish the relation prediction with the second highest probability mass that has not been discarded . Repeting the above procedures till all the triplets are included in the established relations , and we will have the complete prediction for node grouping . The logical relation for each node will be the relation type among the triplets inside the node . 5.1.3 . Tree assembling Note that in the above procedure , we already has the nodes in the deci- sion tree . To assemble the nodes to a medical decision tree , one has to assign a role ( condition or decision ) to each node , and determine whether a pair of nodes are connected . Considering the node ’s role as the node ’s named entity label , and whether a pair of nodes are connected in the decision tree as a directional rela- tion , the tree assembling task can also be regarded as a joint task of entity type classification and relation extraction . We now elaborate on the model details for tree assembling . Denote each un- classified node as Nodei ( i = 1 , 2 , ... , nnode ) . We formulate each node as a text sequence by concatenating the logical relation name , role label name , and triplets ’ text contents , and we augment the text input X to X ′ = ["
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 13,
    "text": "Nodei ( i = 1 , 2 , ... , nnode ) . We formulate each node as a text sequence by concatenating the logical relation name , role label name , and triplets ’ text contents , and we augment the text input X to X ′ = [ X , [ n ] , Nodei , ... [ n ] , Nodennode ] , where [ ] denotes the text concatenation operation . Note that we add a special token [ n ] before each node . After being encoded with a pre - trained text en- coder , we can obtain h(Nodei ) , the hidden states of the special token [ n ] right before each node . h(Nodei ) is considered as the semantic representation of Nodei . A simple linear layer can operate as the node type prediciton module , and a biaffine module will handle the relation classification task for each node pair ( Nodei , Nodej ) . During decoding , we employ the strategy described in Dozat and Manning ( 2016 ) to resolve conflicting predictions . We will refer to the above model as TreeAssemble - Biaffine . \n 5.2 . LLM - based pipeline framework With the recent advances in generative language models Zhu et al . ( 2023 ) ; Zhu and Tan ( 2023 ) ; Zhao et al . ( 2023a ) , there is an unbreakable trend in the NLP field that transforms all the NLP tasks to the task of response generation given an prompt ( or called instruction ) . Yan et al . ( 2021b ) and Zhang et al . ( 2023a ) propose to tackle the varous NER tasks to a unified generation framework . PromptCBLUE Zhu et al . ( 2023 ) conducts a thorough investigation on how the recent large lan- guage models perform on different medical text processing tasks , both under the in - context learning setting and instruction fine - tuning setting . Motivated by the above works , we now formulate each subtask of Text2MDT to a prompt - response generation task . In the Appendix , we present the prompt template and response format for each subtask in the pipeline framework . Note that for the generative LMs like LlaMA-2 to excel at each of the three tasks , we need to construct the designated datasets for each subtask , so that LMs can be finetuned . The details of constructing the instruction finetuning datasets are presented in Section 6 . 5.3 . End - to - end framework For the end2end framework , due to the complexity of this task , it is challeng- ing for the encoder - based models to deal with the Text2MDT task in an end2end fashion . Thus , we mainly utilize the generative LMs for the end2end framework . Note that since this task is complex , it is natural that the idea of chain -"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 14,
    "text": "is challeng- ing for the encoder - based models to deal with the Text2MDT task in an end2end fashion . Thus , we mainly utilize the generative LMs for the end2end framework . Note that since this task is complex , it is natural that the idea of chain - of - thought ( COT ) Wei et al . ( 2022 ) could be benefical for boosting the performances of the generative LMs . COT asks a generative LM to think step by step , either by it- self ( zero - shot COT ) , or to mimick the thinking steps of demonstrations ( few - shot COT ) . In this task , we construction a series of different COT - style prompts and responses . Thus , for the end2end framework , we consider the following variations : • direct generation ( Generation ) , in which a LM is asked to directly generate the entire MDT given the text inputs . The prompt and response templates are presented in the Appendix . • COT - style generation . Due to the complexity of our Text2MDT task , one can consider the following variations6 : 6See the Appendix for prompt and response templates for the series of generation methods . \n – COT - Generation-1 , which decompose the Text2MDT task exactly as the pipeline framework , and ask the LM to first generate the extracted triplets , then node grouping , and then tree assembly , in a single gener- ation run before generating the end - of - sentence token . – COT - Generation-2 decompose the task into more fine - grained subtask . It asks the model to generate entities , triplets , node assignments , node roles , and finally the entire tree . – COT - Generation-3 asks the LM to first extract triplets and then gener- ate the whole MDT . – COT - Generation-4 decompose the triplet extraction subtask by asking the LM to first extract entities , and then generate the triplets , and fi- nally generate the whole MDT . 6 . Experiments 6.1 . Evaluation Metrics 6.1.1 . Metrics for the triplet extraction subtask As described in Section 5 , the most fundamental step of Text2MDT is to ex- tract triples from the given text documents . Following Zhu et al . ( 2023 ) and Zhu ( 2021a ) , we adopt the triplet precision , recall and F1 scores as evaluation met- rics . These metrics of triplet extraction are instance - level strict performance met- rics . Here , an instance means a complete piece of information extracted from the given document . In our triplet extraction subtask , an instance consists of a head entity mention , a tail entity mention , and the relation label name between these two entities . And strict means that the model predicts an instance correctly if and only if it correctly predicts the all"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 15,
    "text": ". In our triplet extraction subtask , an instance consists of a head entity mention , a tail entity mention , and the relation label name between these two entities . And strict means that the model predicts an instance correctly if and only if it correctly predicts the all the components of the instance . 6.1.2 . Metrics for the node grouping subtask Following Wang and Cer ( 2012 ) , we now define an edit distance based met- ric to evaluate how models perform in the node assignment task . According to Equation 1 , one can express a predicted node N pred to a tuple . N pred = ( Rolepred , tpred , ... , tpred ntri , Logical Relpred ) . ( 6 ) Note that we treat each triplet in the same level with the node role label and the logical relation label . And denote a node in the ground truth as N gt = ( Rolegt , tgt 1 , ... , tgt ntri , Logical Relgt ) . ( 7 ) \n Treating each element in the N pred and N gt tuples as indivisible , one can calculate the edit distance between N pred and N gt . In this scenario , the editing operations include inserting and deleting elements , and each operation has a cost of 1 . Now we concatenate all the nodes in the node grouping prediction into a single tu- ple NG Tuppred . Since we does not require the model to assign orders to each node in the node grouping step , we consider all the permutation m of nodes in the ground truth MDTgt , and we concatenate the nodes in each permutation ( de- noted as NG Tupgt , m ) . And the edit distance between the whole node assignment prediction and the ground truth node assignment is defined as the minimum edit distance between the predicted node grouping and a permutation of the ground truth node grouping : NG ED(NG Tuppred , MDTgt ) = min m∈Permute(MDTgt ) ED(NG Tuppred , NG Tupgt , m ) , ( 8) where ED(x , y ) denotes the edit distance between tuple x and tuple y. Since the edit distance score NG ED is an un - normalized metric , it is in - suitable for model comparisons . Thus , we now define the Levenshtein ratio Navarro ( 2001 ) ( denoted as NG LR ) for the node grouping subtask : NG LR(NG Tuppred , MDTgt ) = NG ED(NG Tuppred , MDTgt ) max(len(NG Tuppred ) , len(NG Tupgt , m∗ ) ( 9 ) where len denotes the tuple length , and m∗is the MDTgt ’s permutation that ob- tains the lowest edit distance with the prediction : m∗= argmin m∈Permute(MDTgt ) ED(NG Tuppred , NG Tupgt , m ) . ( 10 ) 6.1.3 . Metrics for the tree assembling subtask To properly evaluate a model ’s"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 16,
    "text": "tuple length , and m∗is the MDTgt ’s permutation that ob- tains the lowest edit distance with the prediction : m∗= argmin m∈Permute(MDTgt ) ED(NG Tuppred , NG Tupgt , m ) . ( 10 ) 6.1.3 . Metrics for the tree assembling subtask To properly evaluate a model ’s performance in constructing medical decision trees from text , we adopt the following three evaluation metrics : • The accuracy of decision tree extraction ( TreeAcc ) . For this metric , the instance is the entire medical decision tree consisting of a series of nodes connected as a binary tree of a certain structure , and each node contains three components , logical relation , role and triplets . A decision tree pre- dicted by a model is correct when it is precisely the same as the ground truth . Thus , this metric is a very strict metric . \n • F1 score of decision paths ( DP - F1 ) . We define a decision path in a medi- cal decision tree as a path from the root node to a leaf node . Thus , in DPF1 , an instance is a decision path , and a model correctly predicts a decision path if and only if it correctly predicts all the nodes in the path and how they are connected . • Lenvenshtein ratio of the decision tree ( Tree LR ) . Similar to the defini- tion of edit ratio defined for the node grouping task , we can arrange the contents of all nodes in the predicted or ground - truth tree into a single tu- ple in the in the order of depth - first search ( denoted as Tree Tuppred and Tree Tupgt , respectively ) , and treat each triple , node role label , node logical relation as indivisible elements . Thus Tree LR is defined by Tree LR(Tree Tuppred , Tree Tupgt ) = ED(Tree Tuppred , Tree Tupgt ) max(len(Tree Tuppred ) , len(Tree Tupgt ) ) . ( 11 ) 6.2 . Implementation Details Our code was implemented with Pytorch7 and Huggingface Transformers8 . For pretrained encoder based methods , we use the pre - trained Chinese medical BERT ( denoted as MedBERT ) by Guo et al . ( 2021b ) as the default backbone model . For ablation studies , we also consider the widely used BERT - wwm - ext9 , Google BERT - base Chinese Devlin et al . ( 2019 ) , and Erlangshen - ZEN1 - 224M- Chinese10 . For the decoding module such as the biaffine module Dozat and Man- ning ( 2016 ) and Wang et al . ( 2021 ) , we will use the original authors ’ default con- figurations . We will fine - tune all the model parameters . Batch size is set to 8 , warm - up steps is set to 50 , the number of training epochs is set to 50"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 17,
    "text": "( 2021 ) , we will use the original authors ’ default con- figurations . We will fine - tune all the model parameters . Batch size is set to 8 , warm - up steps is set to 50 , the number of training epochs is set to 50 , the learning rate is set to 2e-5 with a linear schedule , and the optimizer is AdamW Loshchilov and Hutter ( 2017 ) . The other hyper - parameters like gradient clipping , Adam ep- silon are kept the same with the Transformers repository . For generative LMs , we consider a collection of well - known language models of different sizes . ( a ) GPT-2 Chinese11 . ( b ) Randeng - T5 - 784M12 . ( c ) BLOOMZ- 7https://pytorch.org/. 8https://github.com/huggingface/transformers . 9https://huggingface.co/hfl/chinese-bert-wwm-ext . 10https://huggingface.co/IDEA-CCNL/Erlangshen-ZEN1-224M-Chinese . 11https://huggingface.co/uer/gpt2-chinese-cluecorpussmall 12https://huggingface.co/IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese \n 7.1B - mt13 . ( d ) ChatGLM-6B-2 . ( e ) ChatMed14 , which is adapted from the LlaMA- 7B backbone . ( f ) Chinese - LLaMA-2 7B/13B15 , which are the Chinese version of LlaMA-2 models Touvron et al . ( 2023 ) from Meta . ( g ) Ziya-13B - medical16 , which is also further pre - trained with the LlaMA-2 models . ( h ) Baichuan-2 7B/13B modelsYang et al . ( 2023 ) , which are one of the most recent open - sourced Chinese LLMs , and have achieved excellent performances in many evaluation benchmarks like Li et al . ( 2023a ) . Unless stated otherwise , We will use Baichuan-2 7B as the default generative LM . For generative LMs with parameters fewer than 500 mil- lions , we fine - tune all the model parameters . For larger models , we will use LoRA Hu et al . ( 2021 ) with rank 24 to modify the query , key , value , and output matrix in the self - attention module , and the two matrices in the feed - forward module Zhu et al . ( 2023 ) . The LoRA parameters are fine - tuned with learning rate 1e-4 . The rest of the hyper - parameters are set the same with the encoder - based methods . For each method , we validate the model performance on the dev set and choose the checkpoint with the best dev performance to predict on the test set . Each experiment is run for 5 times with different random seeds and the average scores are reported . 6.3 . Datasets The annotated Text2MDT dataset is open - sourced at https://github . com / michael - wzhu / text2dt . It is one of the evaluation dataset for The CHIP-2022 shared tasks Zhu et al . ( 2023a ) . And it is now a part of the CBLUE benchmark Zhang et al . ( 2022a ) and PromptCBLUE Zhu et al . ( 2023 ) benchmark . The original Text2MDT has a 800:100:100 train /"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 18,
    "text": "dataset for The CHIP-2022 shared tasks Zhu et al . ( 2023a ) . And it is now a part of the CBLUE benchmark Zhang et al . ( 2022a ) and PromptCBLUE Zhu et al . ( 2023 ) benchmark . The original Text2MDT has a 800:100:100 train / dev / test split . Since we are ex- perimenting with different methods from the pipeline and end2end frameworks , we now need to construct different variations of the Text2MDT datasets . 6.3.1 . Datasets for the pipeline framework Since the pipeline framework has three subtasks , thus , we need to construct a different dataset for each subtask so that we can train an encoder - based model : • Text2MDT - TE , the Text2MDT triplet extraction dataset , where the input is the medical text , and the target is the list of triplets in the structured format like JSON . This dataset has a 800:100:100 train / dev / test split . 13https://huggingface.co/bigscience/bloomz-7b1-mt 14https://github.com/michael-wzhu/ChatMed 15https://github.com/michael-wzhu/Chinese-LlaMA2 16https://huggingface.co/shibing624/ziya-llama-13b-medical-lora \n • Text2MDT - NG , the Text2MDT node grouping dataset , where the input is the medical text and the list of triplets in text sequence concatenated to- gether , and the output is the list of nodes in the structured format like JSON and each node contains a list of triplets and a logical relation label . For the Text2MDT - NG training set , we augment the original Text2MDT four times by shuffling the orders of triplets . Thus , this dataset has a 3200:100:100 train / dev / test split . • Text2MDT - TA , the Text2MDT tree assembling dataset , where the input is the medical text and the list nodes in text sequence concatenated together , and the output is the list of MDT nodes in the structured format like JSON and each node contains a list of triplets , a logical relation label and a role la- bel . For the Text2MDT - TA training set , we augment the original Text2MDT four times by shuffling the orders of nodes in the input . Thus , this dataset has a 3200:100:100 train / dev / test split . For each of the above datasets , we will construct a prompt - based dataset for the generative LM methods , with the prompt and response templates in the the Appendix . 6.3.2 . Datasets for the end2end framework For each end2end method , we will construct the end2end dataset with the prompt and response templates in the the Appendix . So that each end2end dataset has a 800:100:100 train / dev / test split . 6.4 . Competing Methods 6.4.1 . Methods for the pipelined framework For the triplet extraction tasks , we consider the following methods : ( a ) UNIRE Wang et al . ( 2021 ) ; ( b ) TPLinker Wang et al . ( 2020 ) ; ( c ) CasRel Wei et al . ( 2020 )"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 19,
    "text": "the pipelined framework For the triplet extraction tasks , we consider the following methods : ( a ) UNIRE Wang et al . ( 2021 ) ; ( b ) TPLinker Wang et al . ( 2020 ) ; ( c ) CasRel Wei et al . ( 2020 ) ; ( d ) Sep - Biaffine , which divide the triplet extraction sub - task into two tasks , entity extraction and relation classification , and use two BERT - biaffine models to handle each task ; ( e ) direct generation ( Generation ) ; ( f ) generation in the COT fashion ( COT - Generation ) , which is to ask the LMs to first detects the relations in the given medical text and then extract triplets . The prompts for COT - Generation is presented in the Appendix . The first four methods use encoder - based PLMs and the last two utilize the generation capabilities of generative LMs . For the node grouping subtask , as discussed in Section 5 , we could use the following methods : ( a ) the NG - biaffine method . ( b ) NG - TableFilling method , \n which substitute the biaffine module ( Equation 3 ) in the NG - biaffine method to the table - filling module in Wang et al . ( 2020 ) ( Equation ( 1 ) of Wang et al . ( 2020 ) ) . ( c ) generation based method ( Generation ) , which asks a generative LM to directly generate the node grouping results given the text input . ( d ) COT style generation ( COT - Generation ) , which asks the generative LM to first cluster the triplets into groups , and then identify the logical relation of each node . For the tree assembling subtask , we will use : ( a ) TreeAssemble - Biaffine method described in Section 5 . ( b ) substituting the biaffine module of TreeAssemb to the the table - filling module of Wang et al . ( 2020 ) ( Equation ( 1 ) of Wang et al . ( 2020 ) ) results in the TreeAssemble - TableFilling method . ( c ) generation based method ( Generation ) , which asks a generative LM to directly generate the medical deci- sion tree given the text input . ( d ) COT style generation ( COT - Generation ) , which asks the generative LM to first determine the role of each node , and then assemble the nodes to form a medical decision tree . 6.4.2 . Methods for the end2end framework Following Section 5 , we consider the following end2end methods : ( a ) Gener- ation ; ( b ) four variations of COT - style generation , ( b1 ) COT - Generation-1 ; ( b2 ) COT - Generation-2 ; ( b3 ) COT - Generation-3 ; ( b4"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 20,
    "text": "Section 5 , we consider the following end2end methods : ( a ) Gener- ation ; ( b ) four variations of COT - style generation , ( b1 ) COT - Generation-1 ; ( b2 ) COT - Generation-2 ; ( b3 ) COT - Generation-3 ; ( b4 ) COT - Generation-4 . 6.5 . Main experimental results In this subsection , we will report the experimental results for each subtask of the pipeline framework , and the overall performance when making predictions with the whole pipeline . In this part of the experiments , for encoder - based meth- ods , we utilize the MedBERT from Guo et al . ( 2021b ) as the pre - trained back- bone . For generation - based methods , we utilize the Baichuan-2 7B as the back- bone model . 6.5.1 . Performances on each subtask The results of each subtask are reported in Table 3 . From the results , we can see that : ( 1 ) Although two - magnitude smaller than the Baichuan-2 7B model in both parameters and complexity , the MedBERT ( 105 million parameters ) based methods achieve competitive performances in all the subtasks , and only falls be- hind the COT - Generation method by 0.5 % in F1 on the triplet extraction task , 0.6 % in NG LR on the node group task , and 1.0 % in Tree LR on the tree as- sembling task . The disadvantages of encoder - based methods are that they require different decoding architectures for obtaining the final predictions . ( b ) despite be- ing heavy in model sizes , Baichuan-2 7B model achieves better performances than \n Subtask Triplet extract Node Grouping Tree assembling Metric Prec Rec F1 NG LR TreeAcc DP - F1 Tree LR Encoder - based methods UNIRE 0.913 0.881 0.896 TPinker 0.909 0.878 0.893 CasRel 0.882 0.891 0.886 Sep - Biaffine 0.893 0.897 0.895 NG - Biaffine 0.962 NG - TableFilling 0.961 TreeAssemble - Biaffine 0.735 0.841 0.937 TreeAssemble - TableFilling 0.741 0.838 0.933 Generation - based methods Generation 0.901 0.894 0.897 0.965 0.745 0.848 0.943 COT - Generation 0.898 0.904 0.901 0.968 0.748 0.852 0.947 Text2MDT task when applying the framework . The average results in five different runs are re- ported . The best results are in bold . the encoder - based models on all the subtasks . The clear advantage of generative models is that they make predictions with the LM prediction head , so it does not require any additional architectural design for different subtasks . All we need to do is to formulate the task into prompt - response pairs . ( c ) Among the encoder- based methods , we can see that the biaffine based method , UNIRE , outperforms the Sep - Biaffine method , showing the adavantage of unifying entity detection and relation classification into a unified space . ( d ) COT style generation helps"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 21,
    "text": ". ( c ) Among the encoder- based methods , we can see that the biaffine based method , UNIRE , outperforms the Sep - Biaffine method , showing the adavantage of unifying entity detection and relation classification into a unified space . ( d ) COT style generation helps the LLMs to achieve better performances on all three sub - tasks , showing that COT is also helpful under manually designed steps for task solving and fine - tuning . This observation is consistent with PromptCBLUE Zhu et al . ( 2023 ) . 6.5.2 . Performances on whole task Now we consider the following combinations of methods for the complete pipelined predictions of test samples from a given medical text : ( a ) The encoder based pipeline method ( denoted as Enc - Pipe ) . In Enc - Pipe , UNIRE is responsible for triplet extraction , NG - Biaffine for node grouping , and TreeAssemble - Biaffine for tree assembling . ( b ) COT - Generation for all the three subtasks ( denoted as CGen - Pipe ) . These two approaches are compared with the five approaches of the end2end framework in Table 4 . From Table 4 , we can see that : ( a ) the CGen - Pipe achieves better perfor- mances than the Enc - Pipe method , which is natural since COT - Generation per- forms better than the encoder based models on all three subtasks . ( b ) InterestMethod TreeAcc DP - F1 Tree ER Pipeline methods Enc - Pipe 0.450 0.612 0.884 CGen - Pipe 0.470 0.631 0.897 End2end methods Generation 0.440 0.619 0.885 COT - Generation-1 0.470 0.628 0.894 COT - Generation-2 0.450 0.623 0.889 COT - Generation-3 0.490 0.632 0.898 COT - Generation-4 0.450 0.626 0.892 in five different runs are reported . The best results are in bold . ingly , the pipelined method CGen - Pipe performs better than the direct Genera- tion method , but does not perform better than COT - Generation-3 . Intuitively , the pipelined method CGen - Pipe suffer from error propagation from different steps in the pipeline , thus , although the pipelined methods utilize more model param- eters , it does not perform better than the end2end methods . ( c ) The COT style generation methods perform better than the direct Generation method , which is intuitively sound . Our Text2MDT task is a complex information extraction task containing multiple steps . The COT based generative methods injects priors on how the models should solve the task , thus it can be more informed to use the results of the previously generated contents for future tokens ’ generation . ( d ) Intuitively , the generative LMs should benefit more from more detailed and fine- grained COT instructions . However , Table 4 shows that COT - Generation-3 per- forms the best . COT - Generation-3 asks the LLMs to first extract triples"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 22,
    "text": "for future tokens ’ generation . ( d ) Intuitively , the generative LMs should benefit more from more detailed and fine- grained COT instructions . However , Table 4 shows that COT - Generation-3 per- forms the best . COT - Generation-3 asks the LLMs to first extract triples and then the MDTs , which has fewer thought steps than the other COT based generation methods . We believe that this is because triplet extraction and tree assembling are the two more challenging steps in the three subtasks , and COT - Generation-3 focus itself on these two subtasks during fine - tuning . And COT - Generation-3 ’s thought steps have relatively smaller response length , which is helpful for the LMs to keep track of the generation contents . 6.6 . Discussions and further analysis 6.6.1 . Impact of tree depth In Table 1 , we present the statistics of the Text2MDT datasets , showing that in our task , medical decision trees are of different depth . In table 5 , we present \n MDT depth CGen - Pipe COT - Generation-3 TreeAcc DP - F1 Tree ER TreeAcc DP - F1 Tree ER 0.750 0.833 0.943 0.750 0.833 0.943 0.428 0.607 0.884 0.442 0.603 0.883 0.454 0.648 0.946 0.545 0.676 0.950 the results of CGen - Pipe , and COT - Generation-3 on different MDT depths . We can see that the two methods obtain the same performance metrics on the MDTs with depth 2 . The performance difference between the end2end models and the pipeline methods mainly lie in MDTs with higher depth . And we can see that the performances on the MDTs with depth larger than 2 are significantly worse than those on the MDTs of depth 2 . Intuitively , tree depth is a direct reflection of the complexity of treatment procedures for different diseases , and it is expected that test samples with deeper MDTs will present higher difficulty for the models , especially for the generation based models . 6.6.2 . Impact of backbone models We first examine how different backbone models affect the Enc - Pipe method . Our main experiments ( Table 3 and 4 ) use the MedBERT Guo et al . ( 2021b ) as the backbone . And now we consider the three alternative encoder backbones men- tioned in Section 6.2 . In the main experiments , we use the Baichuan-2 7B as the generative LM backbone in Table 4 . Now we consider the nine generative LMs of different scales ( presented in Section 6.2 ) for the COT - Generation-3 method . For the methods with size smaller than 1B , we will fine - tune all the parameters of the backbone . For the larger models , we will fine - tune them with the LoRA method Hu et al . ( 2021 ) ( rank = 24 ) . The other experimental settings are kept the same with Section 6.2"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 23,
    "text": "will fine - tune all the parameters of the backbone . For the larger models , we will fine - tune them with the LoRA method Hu et al . ( 2021 ) ( rank = 24 ) . The other experimental settings are kept the same with Section 6.2 . following observations can be made : ( a ) for the Enc - Pipe method , the in - domain pre - trained model , MedBERT performs the best among the four pre - trained en- coders , showing that further pretraining on the large - scale medical corpus are benefical for the Text2MDT task . This observation is in line with Zhu ( 2021b ) ; Guo et al . ( 2021b ) ; Zhu et al . ( 2023b ) . ( b ) For the generative LMs , models with small parameter sizes performs unsatisfying in our task . The small - scale gener- ative LMs does not have enough language understanding and completion capaBackbone TreeAcc DP - F1 Tree ER The Enc - Pipe method MedBERT 0.450 0.612 0.884 BERT - www - ext 0.440 0.615 0.882 BERT - base Chinese 0.390 0.583 0.867 Erlangshen - ZEN1 - 224 M 0.410 0.596 0.873 The COT - Generation-3 method GPT-2 base Chinese 0.030 0.121 0.238 Randeng - T5 - 784 M 0.080 0.253 0.352 BLOOMZ-7.1B - mt 0.330 0.536 0.782 ChatGLM-6B-2 0.380 0.592 0.849 ChatMed 0.420 0.596 0.864 Chinese - LlaMA-2 7B 0.410 0.581 0.868 Chinese - LlaMA-2 13B 0.460 0.623 0.890 Ziya-13B - medical 0.450 0.614 0.886 Baichuan2 7B 0.490 0.632 0.898 Baichuan2 13B 0.490 0.628 0.896 bilities to face the challenges brought by this task . ( c ) Among the open - sourced generative LMs we experiment , the Baichuan2 models perform the best , which we believe results from their large - scale pretraining and complete instruction align- ment pipeline . 6.6.3 . Performance breakdown of the end2end framework In Table 7 , we present the performance breakdown of the test predictions by the best performing method in our main experiments , COT - Generation-3 . As a comparison , we also present the performance scores of the CGen - Pipe method ’s final test score on each sub - task . We present how these methods perform on each subtask . Note that with error propagation in the predicted samples , the scores on the node grouping and tree assembling tasks contain errors from the previous subtasks . From Table 7 , we can see that : ( a ) although the CGen - Pipe method has advantages in the triplet extraction step , the COT - Generation-3 method performs better at the node grouping task , with its generated triplet results . Thus , these two methods already has almost the same score on the node grouping step . And at the tree assembling task , COT - Generation-3 outperforms CGen - Pipe at all three metrics . \n Subtask"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 24,
    "text": "at the node grouping task , with its generated triplet results . Thus , these two methods already has almost the same score on the node grouping step . And at the tree assembling task , COT - Generation-3 outperforms CGen - Pipe at all three metrics . \n Subtask Metric COT - Generation-3 CGen - Pipe Triplet Extraction Prec 0.894 0.897 Rec 0.885 0.888 F1 0.889 0.892 Node grouping NG LR 0.887 0.887 Tree assembling TreeAcc 0.490 0.47 DP - F1 0.632 0.631 Tree LR 0.898 0.897 based pipeline method , CGen - Pipe . 6.6.4 . Case studies On the test set of the Text2MDT task , COT - Generation-3 achieves the best performance . Figure 3 and 4 report two examples where COT - Generation-3 can not predict the same MDTs with the ground truth . In Figure 3 , COT - Generation-3 misses the triplet ( 患者 , 治疗药物 , 缓解充血药 ) ( ( patient , treatment , deconges- tant ) ) in the second node , and the triplet ( 患者 , 治疗药物 , 退热药 ) ( ( patient , therapeutic drug , antipyretic drug ) ) in the fourth node , during prediciton . These errors are mainly from the triplet extraction subtask , which is the first step of tackling MDTs . In Figure 4 , COT - Generation-3 made an error in triplet extraction regarding the basic status of the patients , and as a result , made a mistake in node grouping . 6.7 . Limitations Our work is the first exploration of extracting MDTs from the medical texts , and our work is currently applicable to some simple scenarios , specifically : 1 ) The logic expression of nodes is limited . The triplets between nodes are only ” and ” and ” or , ” while in more complex scenarios , there should be a combination of multiple logical relationships ; 2 ) The expressiveness of the tree is limited . Our decision tree aborts after reaching a decision . The actual scenario should be a process of continuous judgment and decision - making . 3 ) The length of the text is limited . We only contend to extract one paragraph of medical text ; in fact , much medical knowledge needs to be based on multiple sections or even chapters . We will improve it in our future work . \n samples . 7 . Conclusion In this study , we propose a novel task , Text2MDT , which aims to automatically extract medical decision trees from medical texts that are significant for intelligent medicine . We constructed the first Text - to - MDT dataset in the NLP community with medical experts ’ participation . Since there are no existing neural network based methods that can directly deal with our novel tasks , we propose two cohorts of methods : ( a ) the pipeline based method , which decomposes the Text2MDT task into three subtasks"
  },
  {
    "source": "2401.02034v1.pdf",
    "chunk_index": 25,
    "text": "the NLP community with medical experts ’ participation . Since there are no existing neural network based methods that can directly deal with our novel tasks , we propose two cohorts of methods : ( a ) the pipeline based method , which decomposes the Text2MDT task into three subtasks and utilize the existing methods to complete the subtasks ; ( b ) the end2end method , which is challenging and can not be handled by the encoder- based models . We utilize the recent open - sourced LLMs and chain - of - thought prompting for the end2end methods . Experiments show that : ( a ) the LLMs can achieve promising results on the Text2MDT benchmark in an end2end fashion . ( b ) the encoder - based pipeline methods can achieve comparable results with the LLMs while requiring less computations . 8 . Declaration of generative AI in scientific writing The authors of this paper hereby declare that no generative artificial intelli- gence ( AI ) systems were employed in the production , composition , or generation of the content presented in this scientific work . The manuscript ’s creation did \n samples . not involve the utilization of any generative AI tools or systems to produce text , develop ideas , or enhance the overall quality of the content . The writing process and content refinement were conducted solely by human effort , with the primary aid of conventional writing and proofreading tools such as Grammarly for grammatical accuracy and language refinement purposes . The au- thors affirm that Grammarly was employed solely as an assistive tool for grammar and language checks and did not contribute to the conceptualization , generation , or development of the scientific ideas presented in this paper . 9 . Author contributions We now state the contributions of each author : • Conceptualization : Wei Zhu , Wenfeng Li . • Methodology : Wei Zhu , Wenfeng Li , Xing Tian , Pengfei Wang . • Resources : Yuan Ni and Guotong Xie . • Supervision : Xiaoling Wang , Yuan Ni and Guotong Xie . • Validation : Xiaoling Wang , Yuan Ni and Guotong Xie . \n • Writing - original draft : Wei Zhu , Wenfeng Li , Xing Tian , Pengfei Wang . • Writing - review & editing : Xiaoling Wang , Jin Chen , Yuanbin Wu , Yuan Ni and Guotong Xie ."
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 0,
    "text": "Complementary Information Mutual Learning Complementary Information Mutual Learning for Multimodality Medical Image Segmentation Chuyun Shen cyshen@stu.ecnu.edu.cn School of Computer Science and Technology East China Normal University Shanghai 200062 , China Wenhao Li liwenhao@cuhk.edu.cn School of Data Science The Chinese University of Hong Kong , Shenzhen Shenzhen Institute of Artificial Intelligence and Robotics for Society Shenzhen 518172 , China Haoqing Chen 51215901005@stu.ecnu.edu.cn School of Computer Science and Technology East China Normal University Shanghai 200062 , China Xiaoling Wang xlwang@cs.ecnu.edu.cn School of Computer Science and Technology East China Normal University Shanghai 200062 , China Fengping Zhu zhufengping@fudan.edu.cn Huashan Hospital Fudan University Shanghai 200040 , China Yuxin Li liyuxin@fudan.edu.cn Huashan Hospital Fudan University Shanghai 200040 , China Xiangfeng Wang xfwang@cs.ecnu.edu.cn School of Computer Science and Technology East China Normal University Shanghai AI Laboratory Shanghai 200062 , China Bo Jin bjin@tongji.edu.cn School of Software Engineering Shanghai Research Institute for Intelligent Autonomous Systems Tongji University Shanghai 200092 , China Abstract Radiologists must utilize medical images of multiple modalities for tumor segmentation and diagnosis due to the limitations of medical imaging technology and the diversity of tumor signals . This has led to the development of multimodal learning in medical image arXiv:2401.02717v2 [ cs . CV ] 10 Jul 2024 \n Chuyun Shen et al . segmentation . However , the redundancy among modalities creates challenges for existing subtraction - based joint learning methods , such as misjudging the importance of modalities , ignoring specific modal information , and increasing cognitive load . These thorny issues ultimately decrease segmentation accuracy and increase the risk of overfitting . This pa- per presents the complementary information mutual learning ( CIML ) framework , which can mathematically model and address the negative impact of inter - modal redun- dant information . CIML adopts the idea of addition and removes inter - modal redundant information through inductive bias - driven task decomposition and message passing - based redundancy filtering . CIML first decomposes the multimodal segmentation task into mul- tiple subtasks based on expert prior knowledge , minimizing the information dependence between modalities . Furthermore , CIML introduces a scheme in which each modality can extract information from other modalities additively through message passing . To achieve non - redundancy of extracted information , the redundant filtering is transformed into complementary information learning inspired by the variational information bottleneck . The complementary information learning procedure can be efficiently solved by variational inference and cross - modal spatial attention . Numerical results from the verification task and standard benchmarks indicate that CIML efficiently removes redundant information between modalities , outperforming SOTA methods regarding validation accuracy and segmentation effect . To emphasize , message - passing - based redundancy filtering allows neural network visualization techniques to visualize the knowledge relationship among different modalities , which reflects interpretability . 1 . Introduction The capacity of humans to develop a refined comprehension of their external environment can be attributed to the synergistic effects of multiple correlated sensory stimuli . This interaction results in emergent"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 1,
    "text": "network visualization techniques to visualize the knowledge relationship among different modalities , which reflects interpretability . 1 . Introduction The capacity of humans to develop a refined comprehension of their external environment can be attributed to the synergistic effects of multiple correlated sensory stimuli . This interaction results in emergent complexity , where the entirety of the system surpasses the simple sum of its individual components ( Cohen et al . , 1997 ; Gazzaniga et al . , 2006 ) . In the field of neuroscience , attention1 focused on modality - rich spatiotemporal events optimizes information acquisition ( Li et al . , 2008 ; Li , 2023 ) . Consequently , this enables individuals to utilize multimodal data for enhanced informational gain . The significance of multimodal information in influencing human cognition and its implications for the development of artificial intelligence ( AI ) systems that emulate human intelligence has been widely recognized ( McCarthy et al . , 2006 ) . This recognition has led to the advancement of multimodal learning ( MML ) as a key research area in AI ( Baltrušaitis et al . , 2018 ) . MML represents a general framework for constructing AI models capable of extracting and relating information from multimodal data ( Xu et al . , 2022 ) . In recent years , significant advances have been made in MML , particularly in computer vision and natural language processing ( Bayoudh et al . , 2021 ) . Large - scale MML models have achieved near - human performance on specific tasks ( Ramesh et al . , 2021 ; Reed et al . , 2022 ; Rombach et al . , 2022 ) . This paper focuses on medical image segmentation , an essential task in medical image analysis that involves assigning labels to each pixel or voxel to identify distinct organs , tissues , or lesions . The intricate pathological or physiological features of human tissues 1 . Upon encountering external stimuli , humans possess the ability to selectively concentrate on specific elements within these stimuli . This attention mechanism constitutes a fundamental aspect of human cognitive capacity , augmenting the efficacy of information processing ( Zhang et al . , 2012 ) . \n Complementary Information Mutual Learning ( a ) Subtract Operation ( b ) Addition Operation black slashes and the dark gray circle without black slashes represent the same information in embeddings . The Subtract operation first concatenates the information from different modalities and eliminates redundancy . The Addition operation first eliminates cross - modal redundant information and then concatenates the embeddings . and lesions , combined with the variable sensitivity of imaging technologies across different human body components , necessitate the use of multimodal medical images for patient diagnosis and treatment . For instance , clinical guidelines for spontaneous intracerebral hemorrhage indicate that determining appropriate strategies relies on the mismatch between \n Chuyun Shen et al . two magnetic resonance imaging modalities"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 2,
    "text": "technologies across different human body components , necessitate the use of multimodal medical images for patient diagnosis and treatment . For instance , clinical guidelines for spontaneous intracerebral hemorrhage indicate that determining appropriate strategies relies on the mismatch between \n Chuyun Shen et al . two magnetic resonance imaging modalities : perfusion and diffusion imaging ( Greenberg et al . , 2022 ) . Consequently , MML has become increasingly prevalent in medical image segmentation ( Zhou et al . , 2019 ) . Different from the modal - intensive events experienced by humans , machine learning problems with raw multimodal often present unaligned data ( Wei et al . , 2023 ) , highlighting modality alignment and modality synergy as two thorny issues in MML . Fortunately , given the advanced understanding of the human body in medicine , image registration techniques ( Hill et al . , 2001 ) have matured , enabling the alignment of different medical modalities . Multimodal medical image segmentation emphasizes the modality synergy problem , namely constructing knowledge relationships among modalities ( Han et al . , 2022 ) to enable better information complementation and fusion . ( a ) FLAIR →WT ( b ) T2 →TC ( c ) T1CE →TC+ET ( d ) FLAIR →WT+TC+ET structures annotated in various modalities ( bottom left ) and the final labels for the entire dataset ( right ) . Image patches show from left to right : ( a ) the FLAIR image and the whole tumor ( WT ) visible in FLAIR ; ( b ) the T2 image and the tumor core ( TC ) in T2 ; ( c ) the T1CE image , and the enhancing tumor ( ET ) visible in T1CE ( yellow ) , surrounding the necrotic and non - enhancing tumor core(red ) ; ( d ) Final labels of the tumor structures : edema ( green ) , ET ( yellow ) , necrotic and non - enhancing tumor core ( red ) . In the BraTS2020challenge , images are requested to segment into WT , TC , and ET regions . Multimodal medical image segmentation approaches are commonly designed with an end - to - end scheme to learn intermodal associations ( Isensee et al . , 2018 ; Zhang et al . , 2021a , b ; Zhou et al . , 2020a , 2022 ; Ding et al . , 2021 ; Dolz et al . , 2018 ) . Certain medical conditions ( Figure 2 ) require segmentor to simultaneously identify multiple regions , such as the tumors , edemas , and necrotic tumor cores . We then refer to the end - to - end learning scheme above as joint learning , as it jointly maps multimodal inputs to single or multiple regions . The joint learning method typically involves fusing multimodal image encoding into a deep encoder - decoder architecture , which outputs the segmentation(s ) . These methods"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 3,
    "text": "end - to - end learning scheme above as joint learning , as it jointly maps multimodal inputs to single or multiple regions . The joint learning method typically involves fusing multimodal image encoding into a deep encoder - decoder architecture , which outputs the segmentation(s ) . These methods can be broadly classified into early fusion and mid - term fusion . The former directly concatenates \n Complementary Information Mutual Learning multimodal images as input to the network ( Oktay et al . , 2018 ; Isensee et al . , 2018 ; Zhang et al . , 2021b ; Hatamizadeh et al . , 2022 ; Mai et al . , 2022 ) . While the latter uses modality- specific encoders to extract individual features that are later combined in the middle layers of the network and share the same decoder ( Xing et al . , 2022 ; Zhang et al . , 2021a ; Ding et al . , 2021 ; Zhou et al . , 2020a , 2022 ; Dolz et al . , 2018 ) . However , redundant information is present among medical images of different modalities , as evidenced by many works on the cross - modal generation that aim to increase the number of training samples or reduce medical costs by generating high - cost modalities based on low - cost modalities ( Van Tulder and de Bruijne , 2015 ; Ben - Cohen et al . , 2019 ; Zhou et al . , 2020b ; Bouman et al . , 2023 ) . In MML , taking into account that fixed representations can only encapsulate a limited amount of information , redundant information can cause joint learning algorithms to misjudge the importance of different modalities ( Li et al . , 2020 ) , disregard specific modal information ( Wang et al . , 2022 ) , generate additional cognitive load ( Mayer and Moreno , 2003 ; Cao et al . , 2009 ; Knoop - van Campen et al . , 2019 ) , and ultimately reduce prediction accuracy and result in overfitting ( Lin et al . , 2021 ; Mai et al . , 2022 ) . Redundant information in MML can be categorized into intra - modality and inter - modality redundancy . The former refers to redundancy within a single modality , and the latter refers to consistent information across different modalities . Existing MML techniques predominantly focus on addressing intra - modality redundancy ( Lin et al . , 2021 ; Wang et al . , 2022 ) . In the case of inter - modality redundancy , two separate operations , namely addition and subtraction , can be utilized to minimize redundant information in joint representations , as depicted in Figure 1 . A considerable number of mid - term fusion strategies ( Xing et al . , 2022 ; Zhang et al . , 2021a ; Ding et al ."
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 4,
    "text": "namely addition and subtraction , can be utilized to minimize redundant information in joint representations , as depicted in Figure 1 . A considerable number of mid - term fusion strategies ( Xing et al . , 2022 ; Zhang et al . , 2021a ; Ding et al . , 2021 ; Zhou et al . , 2020a , 2022 ; Dolz et al . , 2018 ) implicitly reduce inter - modality redundancy during joint learning by integrating modalities and applying end - to - end learning principles . An alternative method ( Mai et al . , 2022 ) employs the information bottleneck to decrease redundancy in joint representations associated with the subtraction operation . Conversely , the addition operation , which amalgamates complementary information from multiple modalities , is intuitively superior efficacy in eradicating inter - modality redundant information in comparison to the subtraction operation . Furthermore , experienced radiologists often analyze multimodal data in clinical practice by designating a primary modality and several auxiliary modalities for pathological diagnosis . This approach is exemplified in the BraTS challenge ( Menze et al . , 2014 ) ( Figure 2 ) . In this challenge , the segmentation target was the different areas of glioblastoma , which is the most common type of brain malignancy . Glioblastoma is characterized by its resistance to treatment and poor prognosis , making it a critical focus for advancements in medical imaging and treatment strategies(Li et al . , 2019 ) . Human annotators primarily employ the T2 modality2 for segmenting the edema region while using the FLAIR modality to verify the presence of edema and other fluid - filled structures . Subsequently , the tumor core ( TC ) is identified through the combined use of T1CE and T1 modalities . The expertise of these radiologists suggests that specific mapping relationships exist between modalities and target areas . Certain modalities facilitate the identification of particular area boundaries , while others serve as supplementary aids . Gleaning insights from this expert knowledge and incorporating it as an inductive bias can potentially reduce the complexity of learning 2 . T1 , T1CE , T2 , and FLAIR represent four modalities generated by MRI imaging technology . \n Chuyun Shen et al . relationships between modalities and corresponding regions . A similar example of adding priors to reduce learning difficulty is DetexNet ( Liu et al . , 2020 ) , which simplifies low - level representation patterns by embedding expert knowledge . Inspired by these observations , we propose the Complementary Information Mutual Assistance Learning ( CIML ) framework . The primary objective of the CIML framework is to effectively eliminate inter - modal redundant information in multi - modal segmentation tasks . To leverage doctors ’ prior knowledge regarding the correspondence between modalities and regions , the learning multimodal segmentation task is decomposed into multiple single- modal segmentation subtasks , as illustrated in Figure 3 . Within the CIML framework ,"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 5,
    "text": "- modal redundant information in multi - modal segmentation tasks . To leverage doctors ’ prior knowledge regarding the correspondence between modalities and regions , the learning multimodal segmentation task is decomposed into multiple single- modal segmentation subtasks , as illustrated in Figure 3 . Within the CIML framework , each modality serves a dual purpose : as the main modality , the corresponding segmentor encodes its information , integrates messages transmitted by other modalities , and extracts non - redundant information to complete the single - modal subtask ; as an auxiliary modality , the corresponding segmentor transmits auxiliary information to other modalities . This approach minimizes the mutual influence of modal information and ensures sufficient feature extraction for the target area in each subtask . When multiple regions require segmentation , such as in BraTS2020 , expert prior knowledge3 can be employed to match modalities to regions . In tasks involving only one region to segment , such as autoPET , each modal segment is matched to the corresponding region individually . The primary mode is the matching mode of the target ( sub)region , while the remaining modes serve as auxiliary modes . The final segmentation is obtained by combining or averaging all unimodal segmentations . After task decomposition , since the auxiliary mode typically contains less additionally useful information ( non - redundant information ) , we filter redundant information to extract non - redundant information representation from messages transmitted in the auxiliary mode . We first adopt an information theory perspective to transform the problem into an equivalent complementary information learning problem . Inspired by the variational information bottleneck ( Alemi et al . , 2017 ) , we model this problem as a mutual information bi - objective optimization problem . Variational inference is then employed to make optimization problems more tractable , including cross - entropy and Kullback - Leibler ( KL ) divergence minimization , which can be efficiently solved through automatic differentiation . Finally , we introduce cross- modal spatial attention as a parameterized backbone to achieve practical implementation . Overall , CIML adopts two mechanisms , namely task decomposition and redundancy filtering , to minimize the inter - modal redundant information that is relied upon by the algorithm in the segmentation process . Task decomposition physically minimizes the inter- dependence of information between modalities ( through inductive bias ) . At the same time , redundancy filtering extracts as little information as possible from other modalities using the addition operation at the algorithm level . To validate the effectiveness of CIML , we first perform a visual examination of a human - designed image segmentation task to confirm its redundancy - free nature . Subsequently , we evaluate our framework on standard multi- modal medical image segmentation benchmarks , including BarTS2020 , autoPET , and MICCAI HECKTOR 2022 . Experimental results demonstrate that CIML significantly outperforms state - of - the - art algorithms in terms of validation accuracy and"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 6,
    "text": "- free nature . Subsequently , we evaluate our framework on standard multi- modal medical image segmentation benchmarks , including BarTS2020 , autoPET , and MICCAI HECKTOR 2022 . Experimental results demonstrate that CIML significantly outperforms state - of - the - art algorithms in terms of validation accuracy and segmentation quality . 3 . To study the robustness of CIML to different task decompositions , we conducted experiments on a medical image segmentation task where expert prior knowledge is not available , using random matching of modalities and regions to be segmented . Although this scenario is rare , experimental results show that CIML is moderately sensitive to different task decompositions . \n Complementary Information Mutual Learning Moreover , the incorporation of task decomposition and redundancy filtering allows us to utilize neural network visualization techniques , such as Grad - CAM ( Selvaraju et al . , 2017 ) , to gain insights into the contribution of each modality to the segmentation of different regions . By visualizing the relationships and knowledge sharing among modalities , we enhance the credibility and interpretability of multimodal medical image segmentation algorithms , ultimately improving their effectiveness in clinical diagnosis and treatment . Main contributions can be summarized as follows : 1 ) We introduce the Complementary Information Mutual Learning ( CIML ) framework , which aims to enhance the information fusion efficiency in multimodal learning . CIML presents a pioneering approach by algorithmic modeling and mitigating the negative impact of inter - modal redundant information that arises in the joint learning used by state - of - the - art techniques ; 2 ) CIML adopts a unique perspective of addition to eliminate inter - modal redundant in- formation through inductive bias - driven task decomposition and message passing - based redundancy filtering , thus effectively decreasing the difficulty of constructing knowledge relationships among modalities in multimodel learning ; 3 ) We establish an equivalent transformation from the redundant filtering problem to the complementary information learning problem based on the variational information bottleneck and solve it efficiently with variational inference and cross - modal spatial attention ; 4 ) Message passing - based redundancy filtering allows for applying neural network visualiza- tion techniques , such as Grad - CAM ( Selvaraju et al . , 2017 ) , for visualizing the knowledge relationship among different modalities , which reflects the interpretability . The following is the roadmap of this paper . Section 2 provides the related works and preliminaries . Section 3 describes the proposed framework . Experiments and numerical results are presented in Section 4 , and we conclude this paper in Section 6 . 2 . Related Work and Preliminaries In this section , we present a comprehensive literature review on the state - of - the - art multimodal fusion strategies , mutual information , and information bottleneck techniques . In addition , we apply the class activation map methodology to visualize complementary information and thus provide an overview"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 7,
    "text": "In this section , we present a comprehensive literature review on the state - of - the - art multimodal fusion strategies , mutual information , and information bottleneck techniques . In addition , we apply the class activation map methodology to visualize complementary information and thus provide an overview of this technique . 2.1 Multimodal Fusion and Redundancy Reducing Multimodal machine learning has a broad range of applications , including but not limited to audio - visual speech recognition ( Yuhas et al . , 1989 ) , image captioning ( Xu et al . , 2015 ) , visual question answering ( Wu et al . , 2017 ) , besides medical image analysis . Multimodal learning involves the challenge of combining information from two or more modalities to perform accurate predictions ( Baltrušaitis et al . , 2018 ; Zhao et al . , 2024 ) . To effectively extract relevant information from multiple sources , various techniques must be employed to capture and integrate an appropriate set of informative features from multiple modalities . Early fusion and intermediate fusion schemes are the most commonly used methods for this purpose . Early fusion approaches ( Oktay et al . , 2018 ; Isensee et al . , 2018 ; Zhang et al . , 2021b ; Hatamizadeh et al . , 2022 ; Li et al . , 2023 ) adopt a single stream \n Chuyun Shen et al . fusion strategy , where multimodal images fuse before input into a neural network . However , these methods can hardly explore the inter - modality connections . Intermediate fusion approaches(Xing et al . , 2022 ; Zhang et al . , 2021a ; Ding et al . , 2021 ; Zhou et al . , 2020a , 2022 ; Dolz et al . , 2018 ; Yao et al . , 2024 ) follow a multi - stream fusion strategy , where features are fused in the middle layers of the network and share the same decoder . Among these multi - stream methods , attention mechanisms are often utilized to emphasize contributions from different modalities . Methods such as NestedFormer ( Xing et al . , 2022 ) , ModalityNet ( Zhang et al . , 2021a ) , Tri - attentionNet ( Zhou et al . , 2022 ) , and One - shotMIL ( Zhou et al . , 2020a ) leverage attention mechanisms to achieve this . The Tri - attentionNet algorithm ( Zhou et al . , 2022 ) additionally models the relationship between modalities ’ features , which helps to improve segmentation accuracy . However , they do not fully utilize the relationship between tumor regions and modalities . To address this limitation , the RFNet framework ( Ding et al . , 2021 ) was proposed , which employs a region - aware fusion scheme . This approach considers the different contributions of various modalities to each region , as"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 8,
    "text": "the relationship between tumor regions and modalities . To address this limitation , the RFNet framework ( Ding et al . , 2021 ) was proposed , which employs a region - aware fusion scheme . This approach considers the different contributions of various modalities to each region , as different modalities have distinct presentations and sensitivities to different tumor regions . Besides early fusion and intermediate fusion approaches , the PolicyFuser ( Huang et al . , 2023 ) retains one independent decision for each sensor and fusion decision . DrFuse ( Yao et al . , 2024 ) disentangles shared and unique features , then applies a disease - wise attention layer for each modality to make the final prediction . Addressing missing modalities in multimodal medical image segmentation has emerged as a critical research focus . A common strategy involves synthesizing the missing modalities using generative models . For example , Orbes - Arteaga et al . ( 2018 ) demonstrated the synthesis of FLAIR images from T1 modality for segmentation tasks . Similarly , the heteromodal variational encoder - decoder framework proposed by Dorent et al . ( 2019 ) performs joint modality completion and segmentation , generating the required modality . Another approach focuses on learning modality - invariant feature spaces . The method by Havaei et al . ( 2016 ) achieves segmentation by learning features that are consistent across different modalities . ShaSpec ( Wang et al . , 2023a ) further enhances this by leveraging all available modalities during training , learning both shared and specific features . Knowledge distillation and feature transfer techniques also address missing modalities effectively . The Modality - Aware Mutual Learning framework ( Zhang et al . , 2021a ) involves modality - specific models learning collaboratively to distill knowledge . ProtoKD ( Wang et al . , 2023b ) transfers pixel - wise knowledge from multi - modality to single - modality data , enabling robust feature representation and effective inference with a single modality . Latent space information imputation provides an alternative by estimating task - related information of missing modalities . M3Care ( Zhang et al . , 2022 ) utilizes auxiliary information from similar patient neighbors , guided by a modality - adaptive similarity metric , to estimate missing data in the latent space , thus facilitating clinical tasks . Finally , self - attention - based modality fusion offers another solution . SFusion ( Liu et al . , 2023 ) introduces a self - attention - based fusion block that automatically learns to fuse available modalities without synthesizing missing ones . By projecting features into a self - attention module and generating latent multimodal correlations , SFusion constructs a shared representation for downstream models . These varied approaches underscore the innovative strategies developed to handle missing modalities in multimodal medical image segmentation , enhancing the robustness and accuracy of segmentation models . In our work , we aim to eliminate redundancy"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 9,
    "text": "generating latent multimodal correlations , SFusion constructs a shared representation for downstream models . These varied approaches underscore the innovative strategies developed to handle missing modalities in multimodal medical image segmentation , enhancing the robustness and accuracy of segmentation models . In our work , we aim to eliminate redundancy to extract more \n Complementary Information Mutual Learning effective information , which contrasts with most methods addressing missing modalities . These methods typically utilize redundant information to complete other modalities , enabling the use of networks designed for full modalities to accomplish segmentation tasks . The main strategy of these works is contrary to ours . Moreover , the M3Care ( Zhang et al . , 2022 ) method is worth considering for integration with our approach . By leveraging similar patient neighbors , M3Care can extract the necessary complementary information to aid in segmentation . In future work , we will try to learn from M3Care to enable our method to cope with the situation of missing modalities . Further , the current methods for fusing multimodal features do not consider inter - modal redundancy , which may lead to misjudging the importance of modalities , ignoring specific modal information , and increasing cognitive load . To address this limitation , two methods have been proposed in the context of multi - view , which is similar to multimodal data . CoUFC ( Zhao et al . , 2020 ) couples the correlated feature matrix and the uncorrelated ones together to reconstruct data matrices . Although CoUFC utilizes an implicit way of eliminating redundancy by focusing on correlated features and uncorrelated features , its solution is not applicable to high - dimensional , high - resolution medical images . Another work ( Tosh et al . , 2021 ) introduces a contrastive learning method , which learns transformation functions from one view to the other in an unsupervised fashion and then learns a linear predictor for downstream tasks . However , this work focuses on theoretical analysis , lacks validation on complex high - dimensional data , and does not eliminate intra - modal redundancy due to its unsupervised fashion . There are two main differences between CIML and existing approaches . Firstly , our CIML algorithm decomposes the original task , thereby facilitating the establishment of an association between modal and target regions . Secondly , we employ redundancy filtering to extract complementary information , thereby eliminating redundancy and maximizing information gain from auxiliary modalities . 2.2 Mutual Information and Information Bottleneck The application of information - theoretic objectives to deep neural networks was first in- troduced in Tishby and Zaslavsky ( 2015 ) , although it was deemed infeasible at the time . However , variational inference provides a natural way to approximate the problem . To bridge the gap between traditional information - theoretic principles and deep learning , the variational information bottleneck ( VIB ) framework was proposed in Alemi et al . ( 2017"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 10,
    "text": "deemed infeasible at the time . However , variational inference provides a natural way to approximate the problem . To bridge the gap between traditional information - theoretic principles and deep learning , the variational information bottleneck ( VIB ) framework was proposed in Alemi et al . ( 2017 ) . This framework approximates the information bottleneck ( IB ) constraints , enabling the application of information - theoretic objectives to deep neural networks . Several works ( Federici et al . , 2020 ; Wu and Goodman , 2018 ; Zhu et al . , 2020 ; Lee and van der Schaar , 2021 ; Mai et al . , 2022 ; Wang et al . , 2019 ) have been proposed to adopt the IB for multi - view or MML , which is the most relevant to our work . IB variants such as those proposed in Federici et al . ( 2020 ) ; Wu and Goodman ( 2018 ) ; Zhu et al . ( 2020 ) ; Lee and van der Schaar ( 2021 ) extend the VIB framework for multi - view learning . These methods obtain a joint representation via Product - of - Expert ( PoE ) ( Hinton , 2002 ) . Another work ( Wang et al . , 2019 ) proposes a deep multi - view IB theory , which aims to maximize the mutual information between the labels and the learned joint representation while simultaneously minimizing the mutual information between the learned representation of each view and the \n Chuyun Shen et al . original representation . In addition , a recent study ( Mai et al . , 2022 ) introduced a multimodal IB approach , which aimed to learn a multimodal representation that is devoid of redundancy and can filter out extraneous information in unimodal representations . Instead of applying PoE , this work develops three different IB variants to study multimodal representation . CIML differs from the above multi - view IB methods in the following ways : 1 ) . We take into account the varying importance of different modalities . Drawing on expert knowledge that specific modalities contain a greater amount of relevant information than others , CIML decomposes the task and designates some modalities as primary and others as auxiliary ; 2 ) . We assume that the primary modality contains the majority of information about the target region . To maximize information gain and minimize redundancy for segmentation , our method constrains the representation from the auxiliary modalities to contain only complementary information . 2.3 Class Activation Map A widely - used method for determining the most influential pixels or voxels , specifically those with intensity changes that significantly affect the prediction score , involves the generation of a class activation map ( CAM ) ( Zhou et al . , 2016 ; Selvaraju et al . , 2017 ) . These maps highlight the regions in the input"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 11,
    "text": "or voxels , specifically those with intensity changes that significantly affect the prediction score , involves the generation of a class activation map ( CAM ) ( Zhou et al . , 2016 ; Selvaraju et al . , 2017 ) . These maps highlight the regions in the input data that contribute the most to the model ’s output , thereby providing insights into the decision - making process of the model . CAM assigns weights to feature maps in a specific convolutional layer and can be easily integrated into a pre - trained deep model without introducing additional parameters . Several variations have been proposed that build upon CAM to more accurately highlight important regions in the image , such as Gradient - weighted Class Activation Mapping ( Grad - CAM ) ( Selvaraju et al . , 2017 ) . Grad - CAM uses the gradient signal of the activations in a convolutional layer and has been successfully applied to image classification . An extension of Grad - CAM ( Vinogradova et al . , 2020 ) produces heatmaps showing the relevance of individual pixels or voxels for semantic segmentation . In this work , we apply Grad - CAM to visualize voxels that provide complementary information on auxiliary modalities . By doing so , we aim to identify and highlight the most informative and relevant regions in the auxiliary modality for accurate target prediction . 3 . Methodology In this section , we introduce the Complementary Information Mutual Learning ( CIML ) frame- work for medical image segmentation , which aims to efficiently segment through eliminating inter - modality redundancy . The framework incorporates two primary mechanisms : task decomposition ( Section 3.1 ) and redundancy filtering ( Section 3.2 ) . The task decomposition mechanism seeks to reduce the interdependence of information between modalities by drawing on expert prior knowledge as an inductive bias . On the other hand , the redundancy filtering mechanism reduces the amount of redundant information extracted from other modalities through the variational information bottleneck and variational inference . We also introduce the cross - modality information gate module that utilizes cross - modal spatial attention to implement redundancy filtering practically . We assume that our dataset comprises independent and identically distributed ( i.i.d . ) samples { Xi ∈RT×W } N i=1 drawn from a medical image data distribution . In this context , i \n Complementary Information Mutual Learning represents the index of the image , while T and W represent the number of modalities and the number of voxels , respectively . To distinguish between different modalities , we employ subscripts , such as { Xi m}m∈{1,2 , · · · , T } . Our objective is to segment the image into u distinct regions by classifying every voxel into one of u classes . We define Y i ∈{1 , 2 , · · · , u}W as the segmentation mask for the i - th"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 12,
    "text": "· · · , T } . Our objective is to segment the image into u distinct regions by classifying every voxel into one of u classes . We define Y i ∈{1 , 2 , · · · , u}W as the segmentation mask for the i - th sample . Since the multimodal images are spatially aligned , all modalities within a single sample share the same mask , which can be expressed as Y i m = Y i for all m ∈{1 , 2 , · · · , T } . 3.1 Inductive Bias - driven Task Decomposition CIML applies a unique perspective of addition to eliminate inter - modal redundant information . The first step of addition is task decomposition , which is driven by the inductive bias extracted from expert prior knowledge . As shown in Fig . 3 , task decomposition involves decomposing the task into several subtasks . For sub - task τω , the modality Xγω is assigned as the primary modality , which contains significant information for the target ( sub-)regions segmentation . In some cases , multiple modalities are combined as primary modalities for a sub - task , depending on the task . Furthermore , the remaining modalities , which serve as primary modalities for other sub - tasks , are treated as auxiliary modalities that provide complementary information to assist with the sub - task τω . This unique perspective of addition allows us to exploit the complementary information from multiple modalities effectively , reducing redundancy and improving the accuracy and efficiency of the segmentation algorithm . Additionally , we utilize a message - passing mechanism between sub - tasks to transport efficient information from auxiliary modalities . CIML utilizes a distinct sub - model for each sub - task , which is responsible for extracting uni - modal features , performing message passing , obtaining complementary information , and predicting the target ( sub-)regions . These sub- models are referred to as segmentors ( Figure 4 ) , with the segmentor for sub - task τω denoted as fω \u0000Xγω , M1∼T/ γω | θω \u0001 and parameterized by θω . In this notation , M1∼T/ γω denotes messages from other sub - tasks . A segmentor comprises three fundamental components : a message generator , a comple- mentary information filter , and a predictor . The message generator encodes input images to produce embeddings and messages . Subsequently , the complementary information filter utilizes the embeddings from the message generator and messages from other modalities to extract complementary information , thereby enhancing the segmentor ’s performance . Lastly , the predictor leverages the embeddings generated by the message generator and the complementary information to generate the final predictions . To describe task decomposition more clearly , we take BraTS2020 as an example . As illustrated in Fig.3 , the original multi - target task is divided into four distinct sub - tasks . The"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 13,
    "text": "generated by the message generator and the complementary information to generate the final predictions . To describe task decomposition more clearly , we take BraTS2020 as an example . As illustrated in Fig.3 , the original multi - target task is divided into four distinct sub - tasks . The sub - tasks involve utilizing FLAIR as the primary modality for segmenting the whole tumor ( WT ) region , employing T1 as the primary modality for segmenting both the tumor core ( TC ) and the enhanced tumor ( ET ) , leveraging T2 as the primary modality for segmenting the WT and TC regions and adopting T1CE as the primary modality for segmenting the TC and ET regions . This task decomposition is based on expert prior knowledge , which suggests that the selected primary modalities contain the most informative features for accurately segmenting their respective target regions . In addition , to test the performance of different \n Chuyun Shen et al . = = = = = FLAIR T2 T1CE T1 TC+ET TC WT WT+TC Input Input Input Input Seg Seg Message Passing FLAIR Segmentor T1CE Segmentor T2 Segmentor T1 Segmentor Seg Seg ( a ) CIML for BraTS2020 challenge . CT Input Message Passing CT Segmentor PET Segmentor Seg Input PET Seg Lesion Lesion ( b ) CIML for autoPET challenge . for BraTS2020 challenge and autoPET challenge . The input to each segmentor consists of multimodal images that are specific to each modality . After processing , the segmentors send a portion of the embeddings as messages to other segmentors to assist with other sub - tasks and accept messages from other segmentors to extract efficient information . The dark blue lines with bi - directional arrows in the figures represent the message passing . Finally , the segmentors complete their sub - tasks . The MICCAI HECKTOR 2022 challenge also applies a similar framework to the autoPET challenge . \n Complementary Information Mutual Learning Complementary    Information Filter Predictor Message Generator Seg Input Message Passing Segmentor filter , and predictor . CIG C CIG C CIG C CIG C C C C CIG C C C :   concatenation : convolution : up - sampling : { T1 , T2 , T1CE } : message input : message output employed to extract features from FLAIR images individually . Complementary Information Filter ( CIF ) Module is used to extract complementary information from messages , and predictor PFLAIR is utilized to generate the final segmentation . decompositions , we designed ablation experiments to compare several different decomposition methods , as detailed in Section 4 . The segmentor architecture is based on the nnUNet ( Isensee et al . , 2018 ) and utilizes an intermediate fusion scheme . It consists of an encoder and a decoder , which are connected \n Chuyun Shen et al . through skip connections . A schematic representation of the segmentor ’s overall structure is provided in Fig.4 ,"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 14,
    "text": "Isensee et al . , 2018 ) and utilizes an intermediate fusion scheme . It consists of an encoder and a decoder , which are connected \n Chuyun Shen et al . through skip connections . A schematic representation of the segmentor ’s overall structure is provided in Fig.4 , while the structure corresponding to sub - task τFLAIR is depicted in Fig.5 . For the sub - task τFLAIR , the message generator , GFLAIR , acts as the encoder and is composed of four stages . It takes the FLAIR images as input , which are first cropped into 3D patches and generate embedded images . At each stage , GFLAIR produces embeddings that are skip - connected to the decoder in the original U - Net architecture . These embeddings serve as messages denoted by { Mω FLAIR}ω∈{1,2,3,4 } for assisting other sub - tasks . The decoder of the segmentor for sub - task τFLAIR is composed of the complementary information filter ( CIFFLAIR ) and predictor ( PFLAIR ) . CIFFLAIR is designed to utilize the embeddings from the message generator ( GFLAIR ) and messages from other segmentors to filter out complementary information . It also contains four stages , and in each stage , CIFFLAIR incorporates a Cross - modality Information Gated ( CIG ) module based on cross- modal spatial attention , which will be explained in detail in the following subsection . The output of CIFFLAIR combines the embeddings from the encoder and the complementary representation . Finally , PFLAIR predicts the results of the segmentor based on the output of the complementary information filter . 3.2 Message Passing - based Redundancy Filtering In the second phase of the addition process , message passing - based redundancy filtering is employed to eradicate inter - modality redundancy , thereby extracting supplementary information . Drawing inspiration from the variational information bottleneck , we reformulate the problem as a bi - objective mutual information optimization problem . Subsequently , we leverage variational inference to make the optimization problem more tractable by minimizing cross - entropy and KL divergence , and we efficiently solve it using automatic differentiation . Finally , we employ cross - modal spatial attention as a parameterization backbone to obtain a practical implementation . To facilitate discussion , we concern a scenario where two sub - tasks , specifically τ1 : X1 → Y1 and τ2 : X2 →Y2 , are present . Our primary focus is on sub - task τ1 , and the same principles can be extended to more than two sub - tasks . In this context , X1 and X2 signify the first and second modalities , while Y1 and Y2 denote the corresponding ground truth for each sub - task . X1 represents the primary modality encompassing the majority of information pertaining to the target region Y1 . In accordance with standard supervised learning literature , we predict Y1 directly by minimizing the supervised learning"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 15,
    "text": ", while Y1 and Y2 denote the corresponding ground truth for each sub - task . X1 represents the primary modality encompassing the majority of information pertaining to the target region Y1 . In accordance with standard supervised learning literature , we predict Y1 directly by minimizing the supervised learning loss : LSL = −Exi 1,yi 1∼{X1,Y1 } log fX1(yi 1 | xi 1 ) , ( 1 ) where fX1 denotes the parameterized segmentation function . Besides , we aim to gener- ate a representation K2 derived from primary and auxiliary modalities that encapsulate complementary information to aid in predicting the target Y1 . This problem can be modeled within a Bayesian graph ( refer to Figure 6 ) following three Markov chains : X1 ←X →X2 , X1 →Y1 ←X2 , and X1 →K2 ←X2 . Here , X represents the patient as a hidden variable , and K2 is the representation derived from X1 and X2 . \n Complementary Information Mutual Learning representation K2 . Y1 , represented by the upper left , upper right , and lower circles , respectively . Y1 using a Venn diagram to demonstrate their overlapping relationships ( see B.1 for some basic knowledge about Mutual Information and Venn Diagrams ) . Entropy ( H ) and mutual information ( I ) are essential concepts in information theory , as they measure the uncertainty in a set of outcomes and the reduction in uncertainty about one variable due to the knowledge of another variable , respectively ( Shannon , 2001 ) . Further , representation K2 is generated following the Markov chain X1 →K2 ←X2 , so the Venn diagram of K2 is within the union of Venn diagram of X1 and X2 . The area shown in Figure 7 with yellow stripes represents the complementary information not contained in X1 and contributes to the identification of Y1 . Employing Markov chains as a foundation , the mutual information I1(X1 , X2 ; K2 ) can be partitioned into three distinct components through the application of the chain rule of mutual information ( more details see Appendix C.1 ): I1(X1 , X2 ; K2 ) = I2(K2 ; Y1 | X1 ) | { z } complementary predictive information + I3(K2 ; X1 ) | { z } duplicated information + I4(K2 ; X2 | X1 , Y1 ) | { z } unique but irrelevant information , ( 2 ) \n Chuyun Shen et al . where I2(K2 ; Y1 | X1 ) represents the information in K2 that is not involved by modality X1 and is predictive of Y1 . While I3(K2 ; X1 ) indicates the duplicated information already involved in modality X1 , and I4(K2 ; X2 | X1 , Y1 ) indicates unique information but irrelevant information . We regard I3 and I4 as inter - modality redundancy . Based on these observations , we formulate two objectives to generate K2 , i.e. , \u001a minK2 I1(X1"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 16,
    "text": "involved in modality X1 , and I4(K2 ; X2 | X1 , Y1 ) indicates unique information but irrelevant information . We regard I3 and I4 as inter - modality redundancy . Based on these observations , we formulate two objectives to generate K2 , i.e. , \u001a minK2 I1(X1 , X2 ; K2 ) , maxK2 I2(K2 ; Y1 | X1 ) . ( 3 ) The first objective seeks to maximize the mutual information between K2 and the target Y1 , given the modality information of modality X1 . This constraint ensures K2 contains the information depicted in the Venn diagram with yellow stripes . To guarantee that K2 encompasses solely essential information and minimizes redundancy , the mutual information between K2 and modalities X1 , X2 is minimized . This dual objective optimization constrains K2 to be a representation of complementary information containing only indispensable information . Considering the above Bayesian network , the joint probability can be expressed as ( see detailed derivation in Appendix B.2 ): p(X1 , X2 , Y1 , K2 ) = p(K2 | X1 , X2 ) · p(X1 , X2 , Y1 ) . ( 4 ) Furthermore , variational inference can be employed to render the optimization problem unconstrained . The first objective has an upper bound ( see detailed derivation in Appendix C.2 ): I1(X1 , X2 ; K2 ) = Z p ( x1 , x2 , κ2 ) · log \u0000p ( κ2 | x1 , x2 ) \u000e p(κ2 ) \u0001 dx1dκ2dx2 ≤ Z p(x1 , x2 , κ2 ) · log \u0000p ( κ2 | x1 , x2 ) \u000e r(κ2 ) \u0001 dx1dκ2dx2 ≈1 N N X i Z p \u0000κ2 | xi 1 , xi \u0001 · log \u0000p \u0000κ2 | xi 1 , xi \u0001 \u000e r(κ2 ) \u0001 dκ2 , ( 5 ) where r(κ2 ) is a standard normalization distribution . In practice , we can use neural networks µθ(x1 , x2 ) , σθ(x1 , x2 ) to approximatep ( κ2 | x1 , x2 ) by N ( µθ(x1 , x2 ) , σθ(x1 , x2 ) ) . The second mutual information maximization objective possesses a lower bound ( see detailed derivation in Appendix C.3 ): I2(K2 ; Y1 | X1 ) = Z dx1dκ2dy1dx2p(x1 , x2 , κ2 , y1 ) log p(y1 | κ2 , x1 ) p(y1 | x1 ) ≥ Z dx1dκ2dy1dx2p(x1 , x2 , κ2 , y1 ) log q(y1 | κ2 , x1 ) p(y1 | x1 ) ≈1 N N X i Z dκ2p(κ2 | xi 1 , xi 2 ) log q(yi 1 | κ2 , xi 1 ) + H , ( 6 ) \n Complementary Information Mutual Learning where q(y1 | κ2 , x1 ) serves as a variational approximation to p(y1 | κ2 , x1 ) . Notice that H is independent of the optimization procedure and can be ignored . Combining both of these"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 17,
    "text": ") + H , ( 6 ) \n Complementary Information Mutual Learning where q(y1 | κ2 , x1 ) serves as a variational approximation to p(y1 | κ2 , x1 ) . Notice that H is independent of the optimization procedure and can be ignored . Combining both of these bounds , we have that , Lcom ≈1 N N X i=1 Z \u0002 −p \u0000κ2 | xi 1 , xi \u0001 · log q \u0000yi 1 | κ2 , xi \u0001 + β p \u0000κ2 | xi 1 , xi \u0001 · log \u0000p(κ2 | xi 1 , xi 2 ) \u000e r(κ2 ) \u0001\u0003 dκ2 . ( 7 ) Furthermore , we can formulate the loss in this way , Lcom = LCE + βLKL , ( 8) where β ≥0 controls the tradeoff between two objectives . The total loss contains cross entropy and Kullback – Leibler divergence , and the former is LCE = 1 N N X i=1 Z \u0002 −p(κ2 | xi 1 , xi 2 ) log q(yi 1 | κ2 , xi 1 ) \u0003 dκ2 = Eϵ∼N(0,1 ) xi 1,xi 2,yi 1∼{X1,X2,Y1 } −log q \u0000yi 1 | xi 1 , fθ(xi 1 , xi 2 , ϵ ) \u0001 , ( 9 ) where κ2 is sampled with fθ(xi 1 , xi 2 , ϵ ) which is a deterministic function of xi 1 , xi 2 and the gaussian random variable ϵ which is sampled from a normal gaussian distribution . On the other hand , LKL is shown as follow LKL = 1 N N X i=1 Z \u0002 p \u0000κ2 | xi 1 , xi \u0001 · log \u0000p \u0000κ2 | xi 1 , xi \u0001 \u000e r(κ2 ) \u0001\u0003 dκ2 = KL [ p ( K2 | X1 , X2 ) || r(K2 ) ] . ( 10 ) Since both q(yi 1 | κ2 , xi 1 ) and fX1(xi 1 ) involve X1 as input and share the same prediction target Y1 , the cross - entropy loss LCE and the supervised loss LSL can be simplified by combining them and retaining only the cross - entropy loss LCE . The resulting loss function consists of two components : the cross - entropy term measures the discrepancy between our predictions and the targets , while the KL term constrains the representations from CIG modules to represent complementary information . This approach enables the efficient extraction of complementary information representations from messages through automatic differentiation . The function q \u0000yi 1 | xi 1 , f(xi 1 , xi 2 , ϵ ) \u0001 can be utilized to predict the target Y1 . Moreover , in order to improve the extraction of complementary information in auxiliary modalities , we propose a cross - modality information gate ( CIG ) that merges our formulated loss function . The CIG module utilizes cross - modal spatial attention as a parameterization backbone to achieve a practical implementation ."
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 18,
    "text": "in order to improve the extraction of complementary information in auxiliary modalities , we propose a cross - modality information gate ( CIG ) that merges our formulated loss function . The CIG module utilizes cross - modal spatial attention as a parameterization backbone to achieve a practical implementation . contains a CIG module . This CIG module is based on a cross - modal spatial attention mechanism and our proposed loss in Equation 8 to extract complementary information . \n Chuyun Shen et al . C C Pool Pool Attention Weights X Sample Leaky ReLU Sigmoid W Cross - modal Spatial Attention Module C :   concatenation : convolution : element - wise   multiplication : { T1 , T2 , T1CE } X W : weighted sum Unsqueeze Cross - Modality Information Gated Module CIG_Mu CIG_Sigma CIG_Attention modal spatial attention to identify key voxels , filtering complementary information via attention weights , and incorporating residual mechanism for combining local information . and the messages from other segmentors . The spatial attention mechanism has proven effective in achieving high performance with limited parameters and has been utilized in various computer vision applications ( Woo et al . , 2018 ; Zhou et al . , 2022 ; Hinton et al . , 2015 ) . We leverage a cross - modal spatial attention module to extract complementary information that is only contained in messages from other segmentors . The CIG module utilizes average pooling to reduce the number of network parameters while preserving location information . The pooled features are then concatenated and squeezed before passing through CIG_Attention module , which contains two convolutional layers with Leaky ReLU and sigmoid activation functions to obtain the cross - modality attention weights . These weights highlight the critical voxels and are unsqueezed and multiplied element - wise with the messages from other sub - tasks . To generate the complementary information features , two convolutional layers , CIG_Mu and CIG_sigma , are utilized to obtain the mean µ and standard deviation σ . These parameters are then used in the reparameterization trick , which is constrained by KL terms . We employ a weighted sum operation to integrate dominant and complementary information features . Specifically , we first align the channel dimensions of the auxiliary modality features with those of the primary modality by applying convolutional ( weighted ) adjustments . Subsequently , we combine these features through summation to complete the information . With this module , each segmentor in the proposed CIML framework can effectively extract complementary information from auxiliary modalities , allowing for the improvement of multimodal medical image segmentation . \n Complementary Information Mutual Learning 4 . Experiments and Results In this section , we investigate two questions to determine the feasibility and efficiency of our approach : Q1 ): Can the message passing - based redundancy filtering effectively extract non - redundant information from messages transmitted by auxiliary modalities ? Q2 ): Whether removing inter -"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 19,
    "text": "Experiments and Results In this section , we investigate two questions to determine the feasibility and efficiency of our approach : Q1 ): Can the message passing - based redundancy filtering effectively extract non - redundant information from messages transmitted by auxiliary modalities ? Q2 ): Whether removing inter - modality redundancy can improve the quality of medical image segmentation ? To solve these issues , we evaluate the proposed approach on four tasks , namely the ShapeComposition , BraTS2020 , autoPET and the MICCAI HECKTOR 2022 . We address Q1 ) using the hand - crafted demonstrated task ShapeComposition and address Q2 ) using three standardized benchmarks BraTS2020 , autoPET and MICCAI HECKTOR 2022 . To further eval- uate the effectiveness of our proposed CIML , we conduct ablation experiments by removing components from the proposed framework . Unlike existing SOTA methods , task decomposition and redundant filtering enable us to use neural network visualizers , such as Grad - CAM ( Selvaraju et al . , 2017 ) , to provide insight into the contribution of each modality to the segmentation of different regions . By visualizing the relationship of knowledge among modalities , the credibility of multimodal medical image segmentation algorithms is improved , enhancing their effectiveness in clinical diagnosis and treatment . 4.1 Public Dataset and Evaluation Metrics 4.1.1 Datasets We evaluate the performance of the proposed CIML on both a demonstration task , named as ShapeComposition , and three publicly available datasets , namely BraTS2020 ( Menze et al . , 2014 ) , autoPET ( Gatidis et al . , 2022 ) , and MICCAI HECKTOR 2022 ( Oreiller et al . , 2022 ) . BraTS2020 is a brain tumor segmentation dataset consisting of four different modalities : Flair , T1CE , T1 , and T2 , while the autoPET and MICCAI HECKTOR 2022 datasets contain positron emission tomography ( PET ) and computed tomography ( CT ) images , respectively . The BraTS2020 includes 369 subjects for training , with three distinct regions targeted for segmentation : the whole tumor ( WT ) , the tumor core ( TC ) , and the enhancing tumor ( ET ) , in addition to the background . The autoPET challenge is composed of 1 , 014 studies obtained from the University Hospital Tübingen and is publicly accessible on TCIA . The challenge aims to segment the lesion region . The MICCAI HECKTOR 2022 dataset consists of 524 training cases collected from seven different centers , with the goal of segmenting images into two regions : background and lymph nodes ( GTVn ) . For all datasets , we analyze the performance of various methods via five - fold cross - validation . In addition , we manually decompose the segmentation task beforehand to investigate the effects of different assignments on the segmentation performance , which will be presented in the ablation study . For BraTS2020 , we default set four sub - tasks ,"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 20,
    "text": "via five - fold cross - validation . In addition , we manually decompose the segmentation task beforehand to investigate the effects of different assignments on the segmentation performance , which will be presented in the ablation study . For BraTS2020 , we default set four sub - tasks , where FLAIR images are used to segment WT regions ; T1 images are used to segment TC regions ; T2 images are used to segment WT and TC regions ; T1CE images are used to segment TC and ET regions . Since autoPET and MICCAI HECKTOR 2022 contain only one target region , we set two segmentors that predict the same target region in these challenges . In the final results , \n Chuyun Shen et al . Size and Kernel Size . Stride can be easily inferred according to Input and Output size as we apply padding equals 1 . Additionally , we use Batch Normalization in the BraTS2020 dataset and Instance Normalization in other experiments . In detail , we use P to denote patch size , C to denote the base number of filters , O to denote the number of channels of output , and K to denote the number of messages . Architecture Modules Operators Input Size Output Size Kernel Size Encoder Down1 Conv3D + Norm + LeakyReLU P3×1 P3×C Dilated Conv3D + Norm + LeakyReLU P3×C ( P/2)3×C Down2 Conv3D + Norm + LeakyReLU ( P/2)3×C ( P/2)3×2C Dilated Conv3D + Norm + LeakyReLU ( P/2)3×2C ( P/4)3×2C Down3 Conv3D + Norm + LeakyReLU ( P/4)3×2C ( P/4)3×4C Dilated Conv3D + Norm + LeakyReLU ( P/4)3×4C ( P/8)3×4C Down4 Conv3D + Norm + LeakyReLU ( P/8)3×4C ( P/8)3×8C Dilated Conv3D + Norm + LeakyReLU ( P/8)3×8C ( P/16)3×8C Decoder CIG_Attention Conv3D + Norm + LeakyReLU ( P/16)3×(K+1 ) ( P/16)3×4(K+1 ) Conv3D + Norm + Sigmoid ( P/16)3×4(K+1 ) ( P/16)3×K [ CIG_Mu]*k Conv3D ( P/16)3×8C ( P/16)3×8C [ CIG_Sigma]*k Conv3D ( P/16)3×8C ( P/16)3×8C Up1 ConvTranspose3d + Norm + LeakyReLU ( P/16)3×16C ( P/8)3×8C Conv3D + Norm + LeakyReLU ( P/8)3×16C ( P/8)3×8C CIG_Attention Conv3D + Norm + LeakyReLU ( P/8)3×(K+1 ) ( P/8)3×4(K+1 ) Conv3D + Norm + Sigmoid ( P/8)3×4(K+1 ) ( P/8)3×K [ CIG_Mu]*k Conv3D ( P/8)3×4C ( P/8)3×4C [ CIG_Sigma]*k Conv3D ( P/8)3×4C ( P/8)3×4C Up2 ConvTranspose3d + Norm + LeakyReLU ( P/8)3×8C ( P/4)3×4C Conv3D + Norm + LeakyReLU ( P/4)3×8C ( P/4)3×4C CIG_Attention Conv3D + Norm + LeakyReLU ( P/4)3×(K+1 ) ( P/4)3×4(K+1 ) Conv3D + Norm + Sigmoid ( P/4)3×4(K+1 ) ( P/4)3×K [ CIG_Mu]*k Conv3D ( P/4)3×2C ( P/4)3×2C [ CIG_Sigma]*k Conv3D ( P/4)3×2C ( P/4)3×2C Up3 ConvTranspose3d + Norm + LeakyReLU ( P/4)3×4C ( P/2)3×2C Conv3D + Norm + LeakyReLU ( P/2)3×4C ( P/2)3×2C CIG_Attention Conv3D + Norm + LeakyReLU ( P/2)3×(K+1 ) ( P/2)3×4(K+1 ) Conv3D + Norm + Sigmoid ( P/2)3×4(K+1 ) ( P/2)3×K [ CIG_Mu]*k Conv3D ( P/2)3×C ( P/2)3×C [ CIG_Sigma]*k Conv3D ( P/2)3×C ( P/2)3×C Up4 ConvTranspose3d + Norm"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 21,
    "text": "P/2)3×2C Conv3D + Norm + LeakyReLU ( P/2)3×4C ( P/2)3×2C CIG_Attention Conv3D + Norm + LeakyReLU ( P/2)3×(K+1 ) ( P/2)3×4(K+1 ) Conv3D + Norm + Sigmoid ( P/2)3×4(K+1 ) ( P/2)3×K [ CIG_Mu]*k Conv3D ( P/2)3×C ( P/2)3×C [ CIG_Sigma]*k Conv3D ( P/2)3×C ( P/2)3×C Up4 ConvTranspose3d + Norm + LeakyReLU ( P/2)3×2C P3×C Conv3D + Norm + LeakyReLU P3×2C P3×C Output Conv3D P3×C P3×O \n Complementary Information Mutual Learning we ensemble the results of each segmentor by averaging the results if the same target region is present in multiple segmentors . As BraTS2020 is widely used in the literature , we mainly focus on this dataset in our experiments . 4.1.2 Evaluation Metrics The evaluation metrics in our experiments include the dice coefficient and 95 % Hausdorff distance ( HD ): • Dice coefficient ( Dice , 1945 ): the dice coefficient measures the segmentation per- formance of CIML . Concretely , the dice coefficient from set X to set Y is defined as : Dice(X , Y ) = 2 · ∥X ∩Y∥1 ∥X∥1 + ∥Y∥1 . ( 11 ) It is worth highlighting that higher dice coefficients imply that the predictions are closer to the ground truth , which indicates more accurate segmentation . • HD95 ( Henrikson , 1999 ): The maximum Hausdorff distance is the maximum distance of a set to the nearest point in the other set . More formally , The maximum Hausdorff distance from set X to set Y is defined as : dH(X , Y ) = max ( sup x∈X d(x , Y ) , sup y∈Y d(y , X ) ) , ( 12 ) where sup represents the supremum , d(a , B ) is the shortest Euclidean distance between point a and set B. 4.2 Implementation Details We run all experiments based on Python 3.8 , PyTorch 1.12.1 , and Ubuntu 20.04 . All training procedures are performed on a single NVIDIA A100 GPU with 40 GB memory . The initial learning rate is set to 1e −4 , and we employ a “ poly ” decay strategy as below : lr = initial_lr × \u0012 1 − epoch_id max_epoch \u00130.9 . ( 13 ) We apply Adam as the optimizer with weight decay set to 3e −5 and betas set to ( 0.9 , 0.999 ) . We trained our models using a maximum number of epochs set to 500 ( i.e. , the max epoch in Equation 13 ) for the three public datasets and 1000 for the demonstration experiment . Each epoch consists of 100 iterations . To enhance the generalization of our models , we applied the default augmentation strategy in nnUNet ( Isensee et al . , 2018 ) for the three public datasets . The configurations of the segmentor for the three public datasets are presented in Table 1 . The network architecture for the demonstration experiment is similar but has fewer filters , which are described in more detail"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 22,
    "text": "( Isensee et al . , 2018 ) for the three public datasets . The configurations of the segmentor for the three public datasets are presented in Table 1 . The network architecture for the demonstration experiment is similar but has fewer filters , which are described in more detail in Section 4.3 . LeakyReLU activation with a negative slope of 0.01 is used in all experiments . Batch Normalization is used with a batch size of 2 in the BraTS2020 dataset , while Instance Normalization is used in the other experiments . The \n Chuyun Shen et al . patch size is set to 64 × 64 × 64 for the BraTS2020 dataset and 128 × 128 × 128 for other datasets , unless otherwise specified . The notation P refers to patch size , C refers to the base number of filters and K refers to the number of messages . In our practical implementation , we utilize the sum of Dice loss ( Milletari et al . , 2016 ; Drozdzal et al . , 2016 ) and cross - entropy loss instead of exclusively employing cross - entropy loss . Our experimental results demonstrate that the former yields better outcomes . 4.3 Demonstrated Task : ShapeComposition To evaluate the effectiveness of our proposed redundancy filtering , we generated an artificial dataset containing 1000 sets of images . Each set consists of a triangle and an ellipse , deliberately overlapping in a specific region . The aim of the task is to take the input set of images and generate their union as the output , as illustrated in Figure 9 . We employed the task decomposition approach in the CIML framework and assigned one of the figures as the primary modality and the other as the auxiliary modality . Note that both triangles and ellipses can be considered the primary modality . In the implementation , we make several simplifications to the network architecture . Only one sub - network corresponds to the segmentation , and the other sub - network is utilized to acquire complementary information . Specifically , two encoders are employed independently to extract from the primary and auxiliary modalities , respectively . Two decoding pathways are then used . In the first path , one decoder inputs primary modality features ( without gradient backpropagation ) and auxiliary modality features , outputting µ and σ . 3D convolution layers without CIG modules are utilized to fuse features , thus eliminating the effect of CIG modules to verify the efficacy of our complementary information learning . Then , the reparameterization trick is used to sample complementary information features . In the second path , complementary information features are combined with the features directly extracted from the primary modality to predict the final results . Furthermore , as described in Section 3.2 , the Kullback – Leibler divergence between the complementary information features and the standard normal distribution is minimized to constrain the complementary information"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 23,
    "text": "complementary information features are combined with the features directly extracted from the primary modality to predict the final results . Furthermore , as described in Section 3.2 , the Kullback – Leibler divergence between the complementary information features and the standard normal distribution is minimized to constrain the complementary information features containing less information from the primary modality . For qualitative analysis , we maintain the dimension of the complementary information features in line with the original image . As depicted in Figure 9 , our proposed redundancy filtering - based complementary information extraction is effective , and the network efficiently extracts information from the auxiliary modality that is not present in the primary modality . Additionally , it contains little information that is already included in the primary modality . 4.4 Standardized Benchmarks In this section , we compared our proposed CIML algorithms to eight state - of - the - art seg- mentation methods , including nnUNet ( Isensee et al . , 2018 ) , AttentionUNet ( Oktay et al . , 2018 ) , UNETR ( Hatamizadeh et al . , 2022 ) , MAML ( Zhang et al . , 2021a ) , DIGEST ( Li et al . , 2022 ) , RFNet ( Ding et al . , 2021 ) , ACMINet ( Zhuang et al . , 2022 ) and NestedFormer ( Xing et al . , 2022 ) . The first three methods are general methods , while the last five methods are designed specifically for multimodal segmentation . The nnUNet is a widely used benchmark that simplifies the critical decisions in designing an effective segmentation pipeline for any dataset . It and its variant , AttentionUNet , both employ early fusion . UNETR employs \n Complementary Information Mutual Learning signify the Dice coefficients of the whole tumor , the tumor core and the enhancing tumor , respectively . We use the results of RFNet and DIGEST from their paper . Other results are the result of reproducing them . Methods Type Methods Patch Size Dice↑% HD95↓ WT TC ET MEAN WT TC ET MEAN General Methods nnUNet ( Isensee et al . , 2018 ) 91.59 87.50 83.47 87.52 4.75 4.0 8 5.60 4.81 AttentionUNet ( Oktay et al . , 2018 ) 90.62 86.33 81.32 86.09 5.42 7.88 8.98 7.43 UNETR ( Hatamizadeh et al . , 2022 ) 91.11 86.42 82.96 86.76 7.97 5.16 5.90 6.34 Multi - modal Methods MAML ( Zhang et al . , 2021a ) 91.40 88.05 82.40 87.28 4.84 5.95 7.90 7.82 DIGEST ( Li et al . , 2022 ) 90.20 87.00 81.20 86.17 / / / / RFNet ( Ding et al . , 2021 ) 91.11 85.21 78.00 84.77 / / / / ACMINet ( Zhuang et al . , 2022 ) 91.79 87.99 82.56 87.45 5.70 5.09 6.95 5.91 NestedFormer ( Xing et al . , 2022 ) 91.76 88.20 83.19 87.72 5.35 5.07 7.16 5.86 CIML(Ours ) 91.60"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 24,
    "text": "al . , 2021 ) 91.11 85.21 78.00 84.77 / / / / ACMINet ( Zhuang et al . , 2022 ) 91.79 87.99 82.56 87.45 5.70 5.09 6.95 5.91 NestedFormer ( Xing et al . , 2022 ) 91.76 88.20 83.19 87.72 5.35 5.07 7.16 5.86 CIML(Ours ) 91.60 89.14 83.91 88.21 5.83 3.70 4.95 4.83 CIML(Ours ) 91.88 88.69 84.34 88.30 4.58 4.12 5.23 4.64 outcomes of other algorithms result from reproducing them . Methods Type Methods Dice ↑% HD95 ↓ General Methods nnUNet ( Isensee et al . , 2018 ) 55.23 139.84 AttentionUNet ( Oktay et al . , 2018 ) 57.87 108.62 UNETR ( Hatamizadeh et al . , 2022 ) 41.62 177.04 Multi - modal Methods MAML ( Zhang et al . , 2021a ) 53.9 194.36 ACMINet ( Zhuang et al . , 2022 ) 45.67 121.81 NestedFormer ( Xing et al . , 2022 ) 52.51 183.14 CIML(Ours ) 61.37 107.58 outcomes of other algorithms resulting from reproducing them . Methods Type Methods Dice ↑% HD95 ↓ General Methods nnUNet ( Isensee et al . , 2018 ) 73.61 6.09 AttentionUNet ( Oktay et al . , 2018 ) 72.30 16.13 UNETR ( Hatamizadeh et al . , 2022 ) 74.50 5.85 Multi - modal Methods MAML ( Zhang et al . , 2021a ) 70.00 29.05 ACMINet ( Zhuang et al . , 2022 ) 74.23 8.13 NestedFormer ( Xing et al . , 2022 ) 75.16 6.09 CIML(Ours ) 76.28 5.17 \n Chuyun Shen et al . ( a ) Primary modality . ( b ) Auxiliary modality . ( c ) Predicted Segmentation . ( d ) Ground Truth . ( e ) Complementary Information . and second columns depict the primary and auxiliary modalities , respectively , with the first and second images in the set . The third column shows the predicted segmentation , while the fourth column presents the ground - truth segmentation . The fifth column displays the visualization of the complementary information . Note : the images are best viewed in color for optimal clarity . a transformer as the encoder to learn sequence representations of input images . MAML utilizes modality - specific encoders and incorporates a cross - modality attention mechanism for information fusion . DIGEST is a method applying a deeply supervised knowledge transfer network learning . RFNet also uses specific encoders like MAML and includes a region - aware module . ACMINet proposes a volumetric feature alignment module to align early features with late features . NestedFormer is a transformer - based method that designs a nested modality - aware feature aggregation module to model intra- and intermodality features for multimodal fusion . Since RFNet and DIGEST are not open - source algorithms , we only compare CIML with them in the BraTS2020 dataset and offer the results from the original studies . Precisely , \n Complementary Information Mutual Learning we reproduce other methods using open - sourced codes ."
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 25,
    "text": "multimodal fusion . Since RFNet and DIGEST are not open - source algorithms , we only compare CIML with them in the BraTS2020 dataset and offer the results from the original studies . Precisely , \n Complementary Information Mutual Learning we reproduce other methods using open - sourced codes . For a fair comparison , we employ the same training set , i.e. , the same batch size , training epochs , and learning rate decay mechanism for all approaches . As reported in Table 2 , 3 , and 4 , our proposed method demonstrates superior perfor- mance compared to other methods , achieving the highest dice score and HD95 score in all regions across the three challenges . For the BraTS2020 dataset , we investigate the optimal performance of all models by using a patch size of 128 × 128 × 128 . Even with a smaller patch size , which means a more limited field of view , our method still outperforms most existing methods . Our method achieves higher segmentation accuracy compared to the state - of - the - art ( SOTA ) method , indicating that our algorithm can effectively eliminate the negative impact of inter - modal redundant information . 4.5 Ablation Study scores are reported . \" Message \" : models transport embeddings as messages ; \" Attention \" : cross - modal attention mechanism ; \" VI \" : conditional mutual information constrain ; Methods Dice ↑% HD95 ↓ WT TC ET MEAN WT TC ET MEAN Baseline 88.08 86.54 83.78 86.13 13.13 4.70 5.31 7.71 + Message 90.30 88.37 81.38 86.68 8.71 6.22 6.63 7.18 + Message + Attention 90.43 87.23 82.93 86.86 6.47 5.26 7.49 6.08 + Message + VI 91.54 88.70 83.72 87.99 4.60 3.96 4.97 4.51 + Message + Attention + VI 91.60 89.14 83.91 88.21 5.83 3.70 4.95 4.83 scores are reported . The results are listed from smallest to largest , according to the MEAN Dice . Assignment Dice ↑% HD95 ↓ FLAIR T1 T2 T1CE WT TC ET MEAN WT TC ET MEAN TC TC , ET WT TC 91.01 88.95 81.01 86.99 6.01 4.55 6.85 5.80 WT WT TC TC , ET 91.13 88.96 82.38 87.49 5.16 5.34 5.13 5.21 WT WT , TC TC ET 91.21 89.12 82.41 87.58 5.08 6.18 5.24 5.50s WT WT , TC WT , TC TC , ET 91.62 88.20 83.79 87.87 5.98 3.97 5.17 5.04 WT TC WT , TC TC , ET 91.60 89.14 83.91 88.21 5.83 3.70 4.95 4.83 4.5.1 Importance of Different Components The efficacy of the proposed CIML segmentation method relies on several crucial components . An ablation study is conducted to evaluate the importance of each component and validate the efficiency of redundancy elimination , as shown in Table 5 . The “ Baseline \" approach utilizes the nnUNet model for segmentation for each segmentor without incorporating information transport , resulting in each sub - model only being able to extract"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 26,
    "text": "evaluate the importance of each component and validate the efficiency of redundancy elimination , as shown in Table 5 . The “ Baseline \" approach utilizes the nnUNet model for segmentation for each segmentor without incorporating information transport , resulting in each sub - model only being able to extract local information . The “ + Message \" notation represents the message passing between sub - models , where the local \n Chuyun Shen et al . embeddings are used as the message and fed into a one - layer 3D convolutional network equipped with a Batch Normalization layer and a sigmoid activation function , serving as a basic fusion module . The “ + Attention \" notation signifies the utilization of cross - model spatial attention mechanisms for integrating relevant information from the messages into the local embeddings . Finally , the “ + VI \" symbol represents the implementation of the variational inference for complementary information learning to extract complementary information from the messages . nism , and complementary information learning significantly improved the model ’s perfor- mance . Specifically , the “ baseline \" approach that only utilizes local observations performs the worst in terms of the MEAN dice score . Message passing enhances the dice score on the WT and TC regions , as the input to the model contains complete information . Conversely , for the ET region , the opposite effect is observed . This is because the T1CE modality primarily determines the boundary of the ET region , and the features of other modalities do not contribute to determining the boundary of the ET region . This supports the rationale for the task decomposition in our framework . Additionally , the utilization of the cross - model spatial attention mechanism and our proposed redundancy filtering can significantly enhance the ability to extract relevant information from messages , resulting in better performance in the dice score and HD95 . Notably , the implementation of our proposed redundancy filtering results in an even greater improvement . Combining the cross - modal spatial attention mechanism with the redundancy filtering yields optimal results . 4.5.2 Comparison of Different Task Decomposition Based on our proposed CIML framework , the task decomposition is flexible . Different modalities have different clinical implications . On brain tumour segmentation , ET regions can be clearly discriminated on T1CE , and FLAIR is easier to discriminate WT regions , which have a clearer correspondence with the corresponding regions ; TC regions have a relatively vague correspondence , so we have conducted some comparison experiments to test which assignment is better . We set 5 different assignment ways , and present results in Table 6 . The results are ranked by MEAN Dice score , and the last assignment has the best MEAN Dice score . The first assignment does not assign the ET region to the T1CE modality and the WT region to the FLAIR modality , and the segmentation result is the"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 27,
    "text": "6 . The results are ranked by MEAN Dice score , and the last assignment has the best MEAN Dice score . The first assignment does not assign the ET region to the T1CE modality and the WT region to the FLAIR modality , and the segmentation result is the worst , confirming the correlation between the modality and the target region and that it is reasonable for CIML to introduce human priori knowledge . Other assignments have similar segmentation results that exceed or are comparable to the results of nnUNet . 4.5.3 Hyperparametric Studies In our work , there are two essential hyperparameters , the base number of filters and the hyperparameter β , which controls the tradeoff between the CE loss and KL loss . As shown in Fig . 10 , we explore the results of dice scores with filters of 4 , 8 , 16 , 24 and 32 . As base number filters increase , the mean dice ( red dash line ) score increases , and 24 and 32 base filters approach the best dice score . \n Complementary Information Mutual Learning of filters , more filters mean wider network . The grey dash horizontal line indicates the average dice score of nnUNet . dataset . The grey dash horizontal line indicates the average dice score of nnUNet . Additionally , we experiment with β from 1 × 10−3 to 10 and find that the highest average dice were achieved with β equals 0.5 . All beta settings exceeded the results of nnUNet , indicating that our algorithm CIML is insensitive to β and has good generalization . 4.6 Visualization with Interpretability 4.6.1 Segmentation Results As illustrated in Figure 13 , we present the segmentation results of CIML alongside other state- of - the - art methods . Both nnUNet and our proposed method exhibit high accuracy across various cases . Notably , only UNETR and our method correctly classify the corresponding region as TC . \n Chuyun Shen et al . Each row corresponds to a primary modality and a corresponding target region pair . The first four columns display the FLAIR , T2 , T1CE , and T1 images and the information visualization representation masks ( Dark blue indicates the smallest value , light blue means the middle value , and yellow illustrates the largest value ) . Specifically , the primary modal displays only the original image without masks . The fifth column , ‘ Prediction , ’ displays prediction outcomes , while the final column , ‘ Target , ’ displays the ground truth . Note : the images are best viewed in color for optimal clarity . \n Complementary Information Mutual Learning FLAIR nnUNet AttentionUNet UNETR MAML ACMINet NestedFormer CIML Ground Thruth methods . The first column shows FLAIR images , and the other column shows the prediction results and the corresponding ground truth . Note : the images are best viewed in color for optimal clarity . 4.6.2"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 28,
    "text": "Information Mutual Learning FLAIR nnUNet AttentionUNet UNETR MAML ACMINet NestedFormer CIML Ground Thruth methods . The first column shows FLAIR images , and the other column shows the prediction results and the corresponding ground truth . Note : the images are best viewed in color for optimal clarity . 4.6.2 Complementary Information In subsection 4.3 , the demonstration experiment verifies that our proposed redundancy filtering is efficient . In CIML , the extracted information representations are high - dimensional features , and to visualize the complementary information , we use the Grad - CAM algorithm to get class - discriminative localization maps ( we will refer to it as a heatmap when there is no possibility of confusion ) that highlight the voxels focused on by our proposed CIML algorithm . In deep convolutional networks , the deeper layers typically extract semantic information but lose positional correspondence . Conversely , the shallower layers are able to extract detailed pattern information while preserving positional correspondence . Therefore , in order to maintain a clear understanding of positional information , we choose to visualize the information representations from the shallowest layers of the network , which have the same resolutions as the images with C channels . In the BraTS2020 dataset , we default decompose segmentation into four segmentors . Additionally , there are six modality target region pairs . For i - th pair , let { Ai}C c=1 be the information representations ( with C channels ) , and yk be the logit for a chosen pixel class k. Grad - CAM averages the partial gradients of yk with respect to N voxels of each information representation . The heatmap for class k in pair i ζk i = ReLU X c αc kAc i ! , ( 14 ) \n Chuyun Shen et al . with αc k = 1 N X n ∂yk ∂Ac i , n , ( 15 ) where αc k is the neuron importance weights of the c−th channel of information representations for pair i. In Figure 12 , we visualize information representations extracted from the last messages by applying heatmaps as masks added to the original images . The last message refers to the shallow embedding in the neural network , which is the message with the highest resolution . We normalize the heatmaps across all rows so that the amount of information in each auxiliary modality can be easily compared . Yellow represents the largest value , dark blue represents the smallest value , and light blue represents the middle value . As shown in the first row , FLAIR is the primary modality , and the T2 image contains the most complementary information compared to the other two modalities . In the second row , T2 is the primary modality , and the FLAIR image contains the most complementary information . Additionally , the left - down region of the FLAIR image contains more information , which is consistent with medical"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 29,
    "text": "complementary information compared to the other two modalities . In the second row , T2 is the primary modality , and the FLAIR image contains the most complementary information . Additionally , the left - down region of the FLAIR image contains more information , which is consistent with medical domain knowledge . This region , depicted in hyperintensity ( lighter in source images ) , indicates the presence of edema , typically locates at the periphery of the WT . The third and fourth rows illustrate TC is the target region and the hyperintensity regions in T1CE , which means ET regions , contain the most information . In the final two rows concerning T1CE as the primary modality , it is observed that auxiliary modalities contribute less information overall . However , the T2 image still provides some valuable complementary insights , particularly in the hyperintense areas , which appear as low - intensity zones in the T1CE image indicating the necrotic and non - enhancing tumor core . These results demonstrate that the results predicted by our proposed CIML methods are consistent with the physician ’s domain knowledge and permit further verification that the algorithm can extract complementary information from high - dimensional data . 4.6.3 Complementary Information Weights As shown in Figure 12 , auxiliary modalities contain various complementary information for each modality - region pair . To further explore the contribution of different auxiliary modalities to segmentation , we propose using the complementary information weight to quantify the contribution . We define the complementary information weight for pair i as ϖk i = P n ζk i , n P j P n ζk j , n . ( 16 ) weight for the T2 segmenting of the WT target region . Additionally , when segmenting the TC and ET target regions , T1CE requires minimal supplementary information . Furthermore , T1CE offers more complementary information than other modalities , especially for subtasks where the TC target region is segmented using T1 and T2 . These findings are consistent with the observations in Figure 2 , where T1CE is sensitive to NCR / NET and ET regions , while FLAIR is sensitive to the WT target region . Thus , CIML can effectively prioritize sensitive modalities and extract discriminative features from auxiliary modalities . \n Complementary Information Mutual Learning BraTS2020 dataset . 5 . Acknowledgements This work was supported by the STCSM ( 22511106000 , 22511106004 ) . 6 . Conclusion In this study , we propose the complementary information mutual learning ( CIML ) framework , which provides a unique solution to the problem of inter - modal redundant information in multimodal learning , an issue not addressed by previous state - of - the - art ( SOTA ) methods . Our framework , based on addition operation , provides a systematic approach by employing inductive bias for task decomposition and message passing for redundancy filtering , thereby enhancing the effectiveness of multimodal"
  },
  {
    "source": "2401.02717v2.pdf",
    "chunk_index": 30,
    "text": "an issue not addressed by previous state - of - the - art ( SOTA ) methods . Our framework , based on addition operation , provides a systematic approach by employing inductive bias for task decomposition and message passing for redundancy filtering , thereby enhancing the effectiveness of multimodal medical image segmentation . We extensively evaluate our approach and demonstrate its effectiveness , outperforming current SOTA methods . Furthermore , the message passed through redundancy filtering enables the application of visualization techniques such as Grad - CAM , thus improving the interpretability of the algorithm . In conclusion , our proposed CIML framework has the potential to significantly enhance the quality and reliability of multimodal medical image segmentation , ultimately leading to improved clinical diagnosis and treatment outcomes ."
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 0,
    "text": "K - QA : A Real - World Medical Q&A Benchmark Itay Manes1 , Naama Ronn1 , David Cohen1 , Ran Ilan Ber1 , Zehavi Horowitz - Kugler1 , Gabriel Stanovsky2 1 K Health Inc , New York , NY 2School of Computer Science , The Hebrew University of Jerusalem { itay.manes,david.cohen,ran.ilanber}@khealth.com Abstract Ensuring the accuracy of responses provided by large language models ( LLMs ) is crucial , particularly in clinical settings where incorrect information may directly impact patient health . To address this challenge , we construct K - QA , a dataset containing 1,212 patient questions originating from real - world conversations held on K Health ( an AI - driven clinical platform ) . We employ a panel of in - house physicians to answer and manually decompose a subset of K - QA into self - contained statements . Addi- tionally , we formulate two NLI - based evalu- ation metrics approximating recall and preci- sion : ( 1 ) comprehensiveness , measuring the percentage of essential clinical information in the generated answer and ( 2 ) hallucination rate , measuring the number of statements from the physician - curated response contradicted by the LLM answer . Finally , we use K - QA along with these metrics to evaluate several state - of - the- art models , as well as the effect of in - context learning and medically - oriented augmented re- trieval schemes developed by the authors . Our findings indicate that in - context learning im- proves the comprehensiveness of the models , and augmented retrieval is effective in reducing hallucinations . We make K - QA available to to the community to spur research into medically accurate NLP applications . 1 Introduction Recent advancements in large language models ( LLMs ) have led to a growing interest in their ap- plication in the medical domain in patient - facing applications , where LLMs hold the promise of pro- viding laypersons with high - quality advice at a rel- atively low cost ( Singhal et al . , 2022 ; Han et al . , 2023 ) . For instance , in response to the question 1The data and the evaluation script are available at https://github.com/Itaymanes/K-QA . Results and model comparisons can be viewed at https://huggingface.co/ spaces / Itaykhealth / K - QA “ What ’s good for muscular pain ? ” , a good patient- facing response may include , in addition to medical information ( e.g. , a muscle relaxant ) , also the ad- vice “ Seek medical attention if you have numbness or tingling in limbs ” . However , there is a lack of benchmarks re- flecting user needs and corresponding medically- accurate answers to test these models under real- world conditions . Most existing benchmarks as- sume textbook questions with multiple - choice or span - based answers ( Tsatsaronis et al . , 2015 ; Ben Abacha et al . , 2017 ;"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 1,
    "text": "re- flecting user needs and corresponding medically- accurate answers to test these models under real- world conditions . Most existing benchmarks as- sume textbook questions with multiple - choice or span - based answers ( Tsatsaronis et al . , 2015 ; Ben Abacha et al . , 2017 ; Jin et al . , 2019 ) . In contrast , real - world questions , like “ Is there any way I can get some medicine for cold sore and ulcer that is killing me ? ” , can often include various interacting medical conditions ( “ cold sore ” , “ ulcer ” ) , use am- biguous , non - medical jargon ( “ that is killing me ” ) , and require long - form , nuanced answers . In this work , we present K - QA , a medical QA benchmark containing 1,212 deidentified questions asked by real users on K Health,2 an AI - driven clinical platform with over 8 million unique users . The questions in K - QA were curated from K Health ’s vast database of patient - physician interac- tions , aiming to capture stand - alone medical ques- tions . These can be answered solely based on the information provided in the question , and do not require any prior knowledge about the patient ’s history or demographics . The resulting corpus is diverse and challenging , spanning over 100 differ- ent medical conditions ( see examples in Figure 1 ) . To evaluate state - of - the - art models against K- QA , a team of 12 in - house medical doctors rig- orously answered 201 questions from the dataset in a free - text format . Doctors consulted credible medical sources , such as UpToDate3 and PubMed4 to provide accurate and scientifically - backed an- swers . Their answers were further reviewed by an 2https://khealth.com 3https://www.wolterskluwer.com/en/solutions/uptodate 4https://pubmed.ncbi.nlm.nih.gov/ arXiv:2401.14493v1 [ cs . CL ] 25 Jan 2024 \n across a wide range of healthcare topics . The questions are open - ended and diverse . experienced overseeing physician and tested for inter - annotator agreement . To allow fine - grained evaluation , doctors decom- posed each answer into an average of roughly 8 minimal semantic content units ( Nenkova et al . , 2007 ) , resulting in over 1.5 K individual statements . In addition , the importance of each statement was manually marked as either ( 1 ) Must Have , indi- cating that a model must include this statement in order to be medically accurate ( e.g. , providing all contraindications for a drug ) , or ( 2 ) Nice to Have , indicating the statement is supplemental in nature ( e.g. , providing additional conditions where this drug may be helpful ) . Following recent work on evaluation of text gen- eration , we use the decomposed ground - truth an- swers in a natural language inference ( NLI)-based"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 2,
    "text": "to Have , indicating the statement is supplemental in nature ( e.g. , providing additional conditions where this drug may be helpful ) . Following recent work on evaluation of text gen- eration , we use the decomposed ground - truth an- swers in a natural language inference ( NLI)-based evaluation of predicted answers ( Honovich et al . , 2021 ; Laban et al . , 2022 ; Aharoni et al . , 2023 ) . Concretely , we define two complementing evalu- ation metrics . First , comprehensiveness , which is similar to recall , and measures the percentage of ground - truth statement included in the predicted answer . In order to excel in this metric , a model must cover as many of the Must Have statements annotated by doctors . Second , hallucination rate , which is similar to precision , and measures how many of all ground - truth statements contradict the predicted answer . To excel in this metric models must not produce any medically - inaccurate state- ments . We find that recent LLMs , like GPT-4 , are able to approximate such metrics with good perfor- mance , nearing human agreement on the task . Finally , we evaluate various state - of - the - art LLM - based architectures on K - QA , spanning a wide range of families , including open- and closed- source models , zero - shot and in - context learning prompting , and retrieval - augmented generations . We find that all models struggle on comprehen- siveness , with the best performing model covering only 67.7 % of medically - important statements , and while hallucinations seem to decrease with model size and augmented generation , all models may still provide advice which may be medically dangerous in subtle ways , which may not be noticeable by every - day users . We hope that future work adopts K - QA and accompanying metrics as a valuable benchmark to produce medically - accurate NLP applications which can be deployed in real - world scenarios . Background Our study is closely related to two topics : long- form question answering , particularly within the medical domain , and the evaluation of factuality in generated text . In this section , we provide a review of prior work in these fields . In addition , we also highlight key features of K - QA and contrast it with previous work in Table 1 . 2.1 Medical QA Benchmarks Several diverse health - related question - answering datasets have been compiled , including over biomedical scientific literature ( Tsatsaronis et al . , 2015 ; Jin et al . , 2019 ) and medical examina- tions ( Zhang et al . , 2018 ; Pal et al . , 2022 ) . The majority of these datasets rely on multiple - choice or span extraction ( Jin et al . , 2022 ) , which simplify"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 3,
    "text": "et al . , 2019 ) and medical examina- tions ( Zhang et al . , 2018 ; Pal et al . , 2022 ) . The majority of these datasets rely on multiple - choice or span extraction ( Jin et al . , 2022 ) , which simplify the evaluation process but do not reflect complexity of free - form responses which are often needed in real - world situations ( Gehrmann et al . , 2023 ) . \n Dataset Consumer Health Open Domain Patient - Physician Interaction Answer Decomposition Answer Format BioASQ ( Tsatsaronis et al . , 2015 ) ✗ ✗ ✗ ✗ span - based & binary MedQA ( Jin et al . , 2021 ) ✗ ✓ ✗ ✗ multiple choice LiveMedQA ( Ben Abacha et al . , 2017 ) ✓ ✗ ✗ ✓ retrieval MedicationQA ( Abacha et al . , 2017 ) ✓ ✗ ✗ ✗ retrieval MedQuAD ( Ben Abacha et al . , 2019 ) ✓ ✗ ✗ ✗ retrieval MEDIQA - AnS ( Savery et al . , 2020 ) ✓ ✗ ✗ ✗ ranking HealthQA ( Singhal et al . , 2023 ) ✓ ✓ ✗ ✗ - K - QA ( Ours ) ✓ ✓ ✓ ✓ long - form generation ✗ indicates the presence or absence of a particular feature in a dataset . For a more comprehensive discussion , please refer to the main text . In the context of consumer health questions , our dataset is different from existing benchmarks like MEDIQA - AnS ( Savery et al . , 2020 ) , LiveMedQA ( Ben Abacha et al . , 2017 ) and MedicationQA ( Ben Abacha et al . , 2019 ) in several additional key ways . While these datasets source their questions from users searching healthcare websites via the ChiQA system ( Demner - Fushman et al . , 2020 ) and retrieve answers through keyword matching , ours originates from authentic patient - physician interactions , en- suring genuine medical inquiries . Furthermore , our dataset includes free - form open - domain responses carefully curated by medical professionals . In ad- dition , the answers in K - QA are segmented into finer atomic statements , enabling fine - grained eval- uation . 2.2 Factuality Evaluation in Long - Form Generation In addition to the K - QA benchmark , we introduce an evaluation suite to compare the ability of differ- ent models to generate factual and coherent text , covering all of the required medical information . Our evaluation framework is inspired by recent work , which decomposes long text into atomic facts and verifies their factuality using NLI models ( Min et al . , 2023 ; Kamoi et al . , 2023 ; Chen et al . , 2022 ) . Alongside examining whether the generated text entails ground - truth statements , we also introduce novel metrics that model differences in the"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 4,
    "text": "their factuality using NLI models ( Min et al . , 2023 ; Kamoi et al . , 2023 ; Chen et al . , 2022 ) . Alongside examining whether the generated text entails ground - truth statements , we also introduce novel metrics that model differences in the impor- tance of these statements , classifying each as Must Have or Nice to Have . 2.3 Challenging LLM Benchmarks Our work joins a recent line work which produces test sets which are challenging for state - of - the - art LLMs . For example , the Bamboogle benchmark consists of 125 multi - hop questions which stump popular search engines ( Press et al . , 2022 ) , while the GPQA benchmark contains 445 graduate - level questions in various domains ( Rein et al . , 2023 ) . K - QA consists of 1,212 questions , as well as a sub- set of 201 answers , specially curated by in - house physicians . The K - QA Benchmark In this section , we describe the process of curat- ing and annotating the K - QA dataset , depicted in Figure 2 . K - QA consists of two portions - a medium - scale corpus of diverse real - world medical inquiries written by patients on an online platform ( Section 3.1 ) and a subset of rigorous and granular answers , annotated by a team of in - house medical experts ( Section 3.2 ) . Finally , in Section 3.3 , we present an analysis of the dataset , illustrating its diversity in medical characteristics . 3.1 Curating Questions from Real - World Patient - Physician Conversations All of the questions in K - QA originate from de- identified real - world text - based conversations held on K Health . These conversations contain a wide variety of user intents , such as billing inquiries or prescription renewals , alongside a wealth of queries on varied medical subjects ( see Figure 1 ) . \n offline response to an actual patient query obtained from patient - physician interactions . We then utilized LLMs to break - down the response into self - contained statements , subsequently categorized by a panel of medical experts as Must Have or Nice to Have . The example was simplified for presentation purposes . Our goal in creating K - QA is to extract from this large and noisy corpus a diverse dataset of medical questions which can be used to test automated mod- els ’ ability to provide factual and comprehensive medical answers . In particular , we aim for the ex- tracted questions to be as stand - alone as possible , without relying on the patient ’s medical record or the context of the medical discourse . For example ( adapted from Figure 1 ) , K - QA includes questions such as “ How do Genital herpes and HPV differ ? ” ,"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 5,
    "text": "as stand - alone as possible , without relying on the patient ’s medical record or the context of the medical discourse . For example ( adapted from Figure 1 ) , K - QA includes questions such as “ How do Genital herpes and HPV differ ? ” , while we omit questions such as “ Can this allergic reaction be related to my age ? ” which assumes prior knowledge about the patient and their previ- ous symptoms . To achieve this , we performed a rigorous man- ual annotation , aided by a preliminary automatic preprocessing step . First , we used an open - source BERT - based classifier ( Devlin et al . , 2018 ) , fine- tuned for distinguishing questions from statements , such as “ sounds like hives to me”.5 Next , we ap- plied regular expressions to filter questions about logistics ( e.g. , billing or delivery instructions ) . This preprocessing yielded roughly 26 K questions , each individually assessed by a medical professional to identify those suitable as stand - alone questions . Overall , this resulted in a dataset of 1,212 diverse questions , each accompanied by a physician ’s clas- sification of the medical condition discussed in the 5https://huggingface.co/mrsinghania/ asr - question - detection question , according to ICD-10 conventions ( e.g. , the question in Figure 1 is classified as “ Dermatitis , unspecified ” ) ( WHO , 1993 ) . 3.2 Annotating Granular Physician Answers We provide comprehensive and granular answers for a diverse subset of K - QA questions , annotated in three stages by a team of 12 in - house medical doctors . This subset enables us to automatically compare different LLMs against high - quality ex- pert answers . Step 1 : Long - form answer annotation . In the first annotation step , exemplified in Figure 2(A ) , six medical physicians were tasked with providing free- form responses to different sets of questions from K - QA , while an additional physician reviewed their answers and advised where needed . Overall , the first step required roughly 400 skilled person hours ( at a cost of roughly 26 K USD , based on aver- age physician hourly pay in the U.S. ) , during which 201 questions from K - QA were answered . Each physician was granted unlimited time and access to reputable medical resources such as UpToDate and PubMed for referencing purposes ( refer to Figure 3 for the most used resources ) . Notably , they were explicitly instructed to avoid using any generative language models or services . To best emulate the requirements from a user - facing model in the med- ical domain , annotators were further instructed to write answers tailored for a lay audience seeking \n consumer - health information . For example , note how the answer in Figure 2 regarding ringworm strays from medical jargon . annotators during the"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 6,
    "text": "a user - facing model in the med- ical domain , annotators were further instructed to write answers tailored for a lay audience seeking \n consumer - health information . For example , note how the answer in Figure 2 regarding ringworm strays from medical jargon . annotators during the curation of the long - form answers . Step 2 : Answer decomposition into self- contained statements . Following literature on the evaluation of text generation via minimal se- mantic content units ( Nenkova et al . , 2007 ; Liu , 2022 ) , we guided annotators to decompose answers into self - contained statements . Each statement is expected to capture a distinct fact and include suffi- cient context for independent evaluation . Answer decomposition is presented in Fig- ure 2(B ) , illustrating the decomposition of a natural answer into atomic statements . A more complex example of medically oriented statement decompo- sition is presented below . In response to a question about treating hypertension in diabetic patients , the physician recommends ACE inhibitors or ARBs , either alone or in combination with other drugs like calcium channel blockers and thiazides . This scenario illustrates an exclusive OR relation often observed in medical contexts , where multiple treat- ments are optional but not advised together . To address the dependency between treatment options , we use Nice to Have to enable us to preserve the physician ’s intention , emphasizing the importance of taking either ACE or ARBs and suggesting an ad- ditional optional treatment for each . This results in one Must Have statement : “ A recommended treat- ment includes either ACE or ARBs , but not both . ” , and two Nice to Have statements : ( 1 ) “ ARBs can be taken alone or with other medications , such as calcium channel blockers and thiazides . ” ; and ( 2 ) “ ACE can be taken alone or with other medications , such as calcium channel blockers and thiazides . ” The full annotation guidelines and more examples are provided in Appendix B.1 . This step was carried out by a panel of 6 medi- cal doctors ( distinct from the annotators in the first step ) who deconstructed each answer into individ- ual statements . To assist in this process , the panel utilized GPT-4 with a few - shot prompt suggesting potential answer decompositions . The full prompt is provided in Appendix D.1 . The annotators could amend or remove noisy statements , as well as add any missing statements , which they did for 6.86 % of the automatically generated statements . In total , this process yielded 1,589 annotated statements , averaging roughly 7.9 statements per answer . The completion of this phase required a total of approx- imately 30 hours at the cost of approximately 2 K USD . Step 3 : Categorizing statements as Must Have or Nice to Have . At the last"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 7,
    "text": "yielded 1,589 annotated statements , averaging roughly 7.9 statements per answer . The completion of this phase required a total of approx- imately 30 hours at the cost of approximately 2 K USD . Step 3 : Categorizing statements as Must Have or Nice to Have . At the last step , we asked the same group of medical professionals from step 2 to classify each statement into one of two categories : ( 1 ) Must Have – facets of the answer which are cru- cial to convey to a patient when providing medical advice ; or ( 2 ) Nice to Have – statements which are supplemental or informative , but not clinically cru- cial . For example , as can be seen in Figure 2(C ) , the long - form answer regarding the itchiness of ring- worm was decomposed into four statements , three of which were deemed as Must Have , while a state- ment which provided information about ringworm which is not related to its itchiness was deemed as Nice to Have . This process was carried out collabo- ratively , facilitating discussions within the medical group to collectively assess and reach consensus re- garding the perceived levels of importance , amount- ing to approximately 20 person hours , at the cost of roughly 1.5 K USD . The categorization into Must Have and Nice to Have represents a discrete approach to assigning importance to statements . However , in a broader context , this method can be extended to assign var- ious weighted scores to each statement and each metric . 3.3 Dataset Statistics Our K - QA dataset is derived from a diverse group of 1,055 unique users featuring 1,212 questions , \n and the frequencies of clinical entities within those questions . On the far right , the text most frequently matched with the clinical entities is displayed . Count # Words ( avg . ) Questions 1,212 10.06 Answers 88.52 Statements Must Have 14.9 Nice to Have 13.74 Age Group Sex ( % of users ) Female Male 18 - 25 9.09 6.67 26 - 45 36.97 30.30 46 - 60 9.70 3.64 60 + 2.42 1.21 and biological sex . including 201 answers meticulously curated by physicians . Table 2 shows detailed statistics on statements and word counts , while information on the distribution of age and biological sex among users can be found in Table 3 . The questions in our dataset were written to ad- dress a wide array of health concerns , as evidenced in Figure 1 . These patients discuss 172 different medical conditions , coded according to the ICD-10 system . The diversity in questions is highlighted in agnoses and their distributions across various ques- tion types , such as Be , WH - questions and other forms . Additionally , on the right side of Figure 4 , frequencies of clinical categories ( problem , treat- ment , and test ) are displayed"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 8,
    "text": "questions is highlighted in agnoses and their distributions across various ques- tion types , such as Be , WH - questions and other forms . Additionally , on the right side of Figure 4 , frequencies of clinical categories ( problem , treat- ment , and test ) are displayed , classified by using an open - source , fine - tuned named entity recogni- tion BERT - based model.6 For example , a question like “ Is it usually normal for someone to have side effects when first starting vitamins ? ” is labeled with both problem ( “ side effects ” ) and treatment ( “ vitamins ” ) . Evaluation Metrics for K - QA To evaluate models against our K - QA benchmark , we propose a natural language inference ( NLI ; Da- gan et al . , 2005 ; Bowman et al . , 2015 ) evaluation framework , following recent text generation eval- uation ( Honovich et al . , 2021 ; Laban et al . , 2022 ; Aharoni et al . , 2023 ) . Our evaluation framework is inspired by FActScore ( Min et al . , 2023 ) , a metric that mea- sures factual precision by computing the percent- age of atomic facts in a generated answer supported by a reliable external source . Unlike FActScore , which automatically generates statements and as- signs them equal importance , our approach in- volves predefined medical statements with vary- ing clinical significance . This modification enables us to extend the framework and establish a proxy metric for factual recall as well . We consider a predicted answer as a premise and each ground - truth statement derived from an anno- tated answer as an hypothesis . Intuitively , a cor- rectly predicted answer should entail every ground- truth statement . This formulation aims to quantify the extent to which the model ’s answer captures the semantic meaning of the gold answer , abstract- ing over the wording chosen by a particular expert 6https://huggingface.co/samrawal/ bert - base - uncased_clinical - ner \n language model . Each statement is then tested separately and automatically with a model in an NLI framework to determine its relationship to the generated answer . Finally , the metrics are computed , where Hall ( ˆP ) is equal to 1 since there is only one contradiction , and Comp ( ˆP ) is equal to 0 because none of the Must Have statements were entailed . Different robot symbols signify different models , and the example was simplified for presentation . annotator . As formulated below , we devise two NLI - based metrics : comprehensiveness and hallucination rate . These adapt the evaluation of text generation to the medical domain by taking into account K - QA ’s an- notation of Must Have , i.e. , clinically crucial facets of information , and Nice to Have statements , which are supplemental in nature . Both"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 9,
    "text": "and hallucination rate . These adapt the evaluation of text generation to the medical domain by taking into account K - QA ’s an- notation of Must Have , i.e. , clinically crucial facets of information , and Nice to Have statements , which are supplemental in nature . Both metrics were aggregated across all assessed questions , where higher values of the comprehensiveness metric and lower values of hallucination rates indicate bet- ter performance . Figure 5 provides an example illustrating the complete process of evaluating a generated answer and deriving these metrics . Formally , let ˆP denote the model ’s predicted an- swer , Must Have represents the set of ground - truth statements marked as crucial , Nice to Have repre- sents the set of ground - truth statements marked as supplemental , and by S = Must Have ∪ Nice to Have is the set of all statements in the gold reference answer . Comprehensiveness metric . This metric mea- sures how many of the clinically crucial claims are included in the predicted answer . Comp ( ˆP ) = |{x ∈Must_Have| ˆP entails x}| |Must_Have| ( 1 ) I.e. , similarly to recall , Comp ( ˆP ) ∈[0 , 1 ] mea- sures how many ground - truth statements were con- veyed in the predicted answer . We particularly focus on those statements marked as crucial by medical experts and which do not penalize models for not covering supplemental statements , as these may be somewhat arbitrary . Hallucination rate . This metric measures how many of the ground - truth statements contradict the model ’s answer . Hall ( ˆP ) = |{x ∈S| ˆP contradicts x}| ( 2 ) I.e. , Hall ( ˆP ) ∈{0 , 1 , ... , |S| } penalizes an- swers that contradict any of the statements and hence discourages models from making any sort of false medical statements . Similar to precision , a model can achieve a perfect score trivially by gener- ating an empty answer ˆP = ∅since , by definition , no hypothesis contradicts an empty premise . Automatic evaluation . Following work on NLI- based evaluation , we approximate the metrics above via an automated NLI model . We employed GPT-4 as the language model in conjunction with few - shot Chain - of - Thought ( CoT ; Wei et al . , 2022 ) , a prompt that generates sequential intermediary text representations . To assess the quality of the evaluation framework , we randomly selected 50 pairs of questions and their corresponding gener- ated answers from the models described in Section 5.1 . This process yielded 402 unique statements pertaining to the specified set of 50 questions . Three physicians received instructions on how to classify the logical relationship for each triplet ( question , answer , statement ) into one of three NLI categories . The inter - agreement among annotators was assessed using Fleiss ’ kappa"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 10,
    "text": "statements pertaining to the specified set of 50 questions . Three physicians received instructions on how to classify the logical relationship for each triplet ( question , answer , statement ) into one of three NLI categories . The inter - agreement among annotators was assessed using Fleiss ’ kappa ( κ ; Fleiss , 1971 ) and pairwise agreement . For the three human an- notators , the pairwise agreement was 83.2 % , and \n the κ was calculated to be 0.70 , signifying mod- erate to substantial agreement among raters . The agreement with the majority vote of the annotators and the automated model was 83.0 % , indicating that the model can perform at a level comparable to human annotators for this complex task . Evaluating State - of - the Art Models Following the creation of K - QA and the formula- tion of evaluation metrics , we turn to evaluate the current state of the art in this challenging task . 5.1 Experimental Setup Models . We use K - QA to evaluate the medi- cal capabilities of 7 recent LLM - based models from diverse families and model sizes . Specifi- cally , we evaluate two 7B instruction - tuned open access models : Mistral ( Jiang et al . , 2023 ) , and MedAlpaca ( Han et al . , 2023 ) which was built upon LLaMA ( Touvron et al . , 2023 ) and trained specif- ically for biomedical tasks , three recent closed instruction - tuned LLMs : Open AI ’s GPT-3.5 and GPT-4 ( Brown et al . , 2020 ; OpenAI , 2023 ) , and Google ’s PALM-2 ( Anil et al . , 2023 ) , and finally two recent commercial closed generation search engines : BARD,7 and Bing Chat.8 We use zero temperature sampling for all models , except for BARD and Bing Chat , which do not allow setting temperature . Retrieval augmented generation ( RAG ) . We note that BARD and Bing Chat differ from the other 5 models in our evaluation in that they can reportedly augment their prompt with content re- trieved from the external sources , albeit in an undis- closed manner . To examine the effect that retrieved content may have on the performance of the other models , we implement Retrieval Augmented Gen- eration approach ( RAG ; Lewis et al . , 2020 ) , which produces responses by conditioning the language model on both the input query and retrieved con- tent . To achieve this , we index publicly available medical documents aimed at the lay audience ( such as MayoClinic9 and NHS10 ) aiming for medical- specific RAG . All the documents in this RAG are publicly available , which is distinct from the pri- mary sources that the physician annotators used to create their answers ( Figure 3 ) . 7https://bard.google.com/ 8https://www.bing.com 9https://www.mayoclinic.org/ 10https://www.nhs.uk/ Model Comp ↑ Hall ↓ % respond"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 11,
    "text": ") aiming for medical- specific RAG . All the documents in this RAG are publicly available , which is distinct from the pri- mary sources that the physician annotators used to create their answers ( Figure 3 ) . 7https://bard.google.com/ 8https://www.bing.com 9https://www.mayoclinic.org/ 10https://www.nhs.uk/ Model Comp ↑ Hall ↓ % respond MedAlpaca 7B 31.4 56.7 Mistral 7B 47.6 28.4 PALM-2 50.8 31.3 BARD 62.5 28.4 95.0 Bing Chat 57.3 25.9 99.5 GPT-3.5 56.2 27.9 GPT-3.5+ICL 59.5 23.4 99.5 GPT-3.5+RAG 50.5 17.9 89.0 GPT-3.5+ICL+RAG 62.9 15.4 96.0 GPT-4 57.5 23.9 GPT-4+ICL 67.7 25.4 GPT-4+RAG 52.2 22.9 91.5 GPT-4+ICL+RAG 65.2 24.4 addition of three in - context examples , and RAG is the medical retrieval augmented setup , as detailed in Sec- tion 5.1 . The performance of the highest scoring model is bolded for each metric . % respond indicates the percentage of generations that do not abstain from an- swering the questions . Prompts . Most of our evaluations use the same vanilla zero - shot prompt without prompt engineer- ing which only presents the question , without any additional instructions . In addition , for some mod- els we also report results on another empirically engineered prompt which includes three in - context examples , to explore some of the effect that in con- text learning ( ICL ) may have on performance . We note however that finding optimal prompts for a given task and model is an open question ( Liu et al . , 2023 ) , which we leave in our case as interest- ing avenue for future work . 5.2 Results The results for all models are shown in Table 4 , in terms of the comprehensiveness and hallucina- tion rate metrics defined in Section 4 . We anlayze additional facets of the models ’ performance in Ta- ble 5 . Below , we highlight key observations based on these results . Attaining high comprehensiveness is challeng- ing even for state - of - the - art models . Across all models and prompts , the comprehensiveness met- ric ( Comp ) consistently remains below 68 % . This is evident even in cases with longer texts , as seen in the BARD model , which contains nearly three times as many words per answer ( 242.1 , Table 5 ) compared to the physician ’s response ( 88.36 , Ta- ble 2 ) . This underscores the models ’ difficulty in capturing what physicians consider critically important . Additionally , ICL improves comprehensive- ness by instructing the model to include elements beyond a direct response to the question , such as assuming underlying medical concerns in patient inquiries . Some of the low scores may be explained by model ’s abstaining from answering certain ques- tions , as shown in Table 4 . While hallucinations seem rare , they could po- tentially lead to unintended and unsafe medical recommendations . The best hallucination rate , achieved by GPT-3.5+ICL+RAG , represents a con- tradiction of"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 12,
    "text": "may be explained by model ’s abstaining from answering certain ques- tions , as shown in Table 4 . While hallucinations seem rare , they could po- tentially lead to unintended and unsafe medical recommendations . The best hallucination rate , achieved by GPT-3.5+ICL+RAG , represents a con- tradiction of roughly 30 statements out of 1500 annotated examples . However , some of the hallu- cinations may lead to subtle yet dangerous advice . For example , in Figure 5 , the physician ’s statement asserts that “ Combining Buspar and Zoloft may increase the risk of serotonin syndrome ” , while , on the other hand , the model claims that “ there are no known major interactions between Buspar and Zoloft ” . This contradiction can impact the patient ’s medication intake . We note that finding cause of error in such cases is hard , and physicians are also prone to making dangerous errors . This particular error can be attributed to a combination of miss- ing information within publicly available medical sources and by the LLM assuming that their omis- sion implies the drug combination is safe . For the GPT models in our evaluation , it seems that larger models lead to improved comprehen- siveness ... In Table 4 , we observe that GPT-4 outperforms GPT-3.5 under every comparable set- ting . These findings align with those of Roberts et al . ( 2020 ) , who demonstrated , in the task of open- domain question answering , that the capability of a pretrained model to answer questions without access to any external knowledge scales with the model ’s size . ... yet the larger the GPT model , the more it seems to introduce new hallucinations . Revis- iting Table 4 , we see that the improved compre- hensiveness comes at the cost of an increase in the hallucination rate . Domain - specific RAG reduces hallucinations . Among all configurations , GPT-3.5+ICL+RAG demonstrates the fewest hallucinations while main- taining a comparatively good comprehensiveness score . Upon close examination , we found that the tradeoff with comprehensiveness is partly due to its tendency to abstain from answering in certain questions ( see % respond column in Table 5 ) . We Model Use search # words MedAlpaca 7B × 48.6 Mistral 7B × 68.4 PALM-2 × 80.0 BARD ✓ 242.1 Bing Chat ✓ 95.7 GPT-3.5 × 60.8 GPT-3.5+ICL × 80.3 GPT-3.5+RAG ✓ 96.3 GPT-3.5+ICL+RAG ✓ 173.2 GPT-4 × 63.1 GPT-4+ICL × 132.8 GPT-4+RAG ✓ 89.8 GPT-4+ICL+RAG ✓ 130.1 models . # words represents the average number of words in a response note that this may actually be desired over mis- information in a patient - facing application . For this model , when computing the metrics only over the answered questions , it receives a Comp score of 65.5 % and a Hall score of 16.1 , which is still the lowest in hallucinations but with the second- highest comprehensiveness . However , Bing"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 13,
    "text": "- facing application . For this model , when computing the metrics only over the answered questions , it receives a Comp score of 65.5 % and a Hall score of 16.1 , which is still the lowest in hallucinations but with the second- highest comprehensiveness . However , Bing Chat and BARD , which also abstain occasionally ( e.g. , “ I ’m unable to help , as I am only a language model and do n’t have the ability to process and under- stand that . ” ) , appear to underperform compared to their base models , even though they use some vari- ants of GPT and PALM . This discrepancy might stem from our prompts lacking task optimization and their generic web retrieval , especially failing to focus on consumer health inquiries in the medical domain from reliable sources . MedAlpaca performs poorly on K - QA . Even though MedAlpaca was fine - tuned specifically for the biomedical domain and intended for use as medical conversational AI , it exhibites the poorest results on both metrics , with an especially high hal- lucination rate . These findings indicate a mismatch between closed - QA ( e.g. , medical exams and short answers ) and real - world patient questions which require the generation of long medical answers . Conclusion In this work , we introduce K - QA , a question- answering benchmark that includes real - world pa- tients ’ questions and carefully curated fine - grained \n answers by physicians . In addition , we formu- late metrics that quantify how well a predicted answer covers important information and to what extent it contradicts gold answers . Our evaluation shows that while models improve with size and augmented generation , there is still a lot of room for improvement , especially in the comprehensive- ness of the answers and ensuring that they do not include medically inaccurate statements . Ethics Statement The data in K - QA originates from deidentified real- world patient conversations that have been man- ually reviewed to ensure there are no leaks . In particular , we verified that the questions do not dis- close any private medical information , and revolve around general medical questions . The answers in K - QA were manually written by medical doctors , who did not use any automated writing assistance and wrote their answers with a general audience in mind . Our legal team has reviewed and approved the methodology used . Limitations One of the major limitations of our evaluation ap- proach is its reliance on LLMs for approximating the entailment relation between ground - truth and predicted answers . While this was done in various recent works , it may propagate noise into the evalu- ation process , and yield a costly evaluation protocol . To mitigate this concern , we measure the agreement between human annotators and predicted labels , finding overall good agreement , while reducing evaluation"
  },
  {
    "source": "2401.14493v1.pdf",
    "chunk_index": 14,
    "text": "While this was done in various recent works , it may propagate noise into the evalu- ation process , and yield a costly evaluation protocol . To mitigate this concern , we measure the agreement between human annotators and predicted labels , finding overall good agreement , while reducing evaluation costs an important avenue for future work ( Perlitz et al . , 2023 ) . Acknowledgments We would like to express our gratitude to Carmel Vider , Lee Herzog , Tamar Brufman , Nadia Mor- denfeld , Lior Hayat , Ohad Simons , Jessica Thien , Shira Ben Porat , Itay Omer , Assaf Frajman , and Noa Bleistein for curating and reviewing the med- ical answers . In addition , we would like to thank Nadav Rabani and Ya’ara Arkin for insightful dis- cussion that influenced the direction of this work ."
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 0,
    "text": "Prompt - driven Latent Domain Generalization for Medical Image Classification Siyuan Yan , Chi Liu , Zhen Yu , Lie Ju , Dwarikanath Mahapatra , Brigid Betz - Stablein , Victoria Mar , Monika Janda , Peter Soyer , and Zongyuan Ge , Senior Member , IEEE Abstract — Deep learning models for medical image anal- ysis easily suffer from distribution shifts caused by dataset artifacts bias , camera variations , differences in the imag- ing station , etc . , leading to unreliable diagnoses in real- world clinical settings . Domain generalization ( DG ) meth- ods , which aim to train models on multiple domains to per- form well on unseen domains , offer a promising direction to solve the problem . However , existing DG methods assume domain labels of each image are available and accurate , which is typically feasible for only a limited number of medical datasets . To address these challenges , we propose a novel DG framework for medical image classification without relying on domain labels , called Prompt - driven Latent Domain Generalization ( PLDG ) . PLDG consists of unsupervised domain discovery and prompt learning . This framework first discovers pseudo domain labels by cluster- ing the bias - associated style features , then leverages col- laborative domain prompts to guide a Vision Transformer to learn knowledge from discovered diverse domains . To facilitate cross - domain knowledge learning between differ- ent prompts , we introduce a domain prompt generator that enables knowledge sharing between domain prompts and a shared prompt . A domain mixup strategy is additionally employed for more flexible decision margins and mitigates the risk of incorrect domain assignments . Extensive exper- iments on three medical image classification tasks and one debiasing task demonstrate that our method can achieve comparable or even superior performance than conven- tional DG algorithms without relying on domain labels . The code is available at https://github.com/SiyuanYan1/PLDG . Index Terms — Domain generalization , Prompt Learning , Dermatology , Skin Cancer , Diabetic Retinopathy I. INTRODUCTION Deep learning has made remarkable progress in various applications . However , recent studies [ 1 ] , [ 2 ] have highlighted the vulnerability of deep learning models against distribution shifts caused by dataset bias , camera variations , differences in the imaging station , etc . , which poses a significant risk S. Yan , C. Liu , Z. Yu , L. Ju and Z. Ge is with the Monash Uni- versity , Clayton , VIC . 3800 Australia , and also with Airdoc - Monash Research , Monash University , Clayton , VIC . 3800 Australia ( E - mail : siyuan.yan@monash.edu , zongyuan.ge@monash.edu ) . D. Mahapatra is with the Inception Institute of AI , Abu Dhabi , UAE ( E - mail : dwarikanath.mahapatra@inceptioniai.org ) B. Betz - Stablein , M. Janda and P. Soyer is with the Univer- sity of Queensland Diamantina Institute , Dermatology Research Centre , The University of Queensland"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 1,
    "text": ". D. Mahapatra is with the Inception Institute of AI , Abu Dhabi , UAE ( E - mail : dwarikanath.mahapatra@inceptioniai.org ) B. Betz - Stablein , M. Janda and P. Soyer is with the Univer- sity of Queensland Diamantina Institute , Dermatology Research Centre , The University of Queensland , Brisbane , Australia ( E - mail : , p.soyer@uq.edu.au ) . V. Mar is with Victorian Melanoma Service , Alfred Health , Melbourne , VIC . 3004 , Australia ( Email : victoria.mar@monash.edu ) The comparison between conventional domain generalization ( DG ) and our latent domain generalization . Conventional DG aims to train the model to learn from multiple domains to generalize well in unseen domains . Latent domain generalization aims to automatically discover essential domain information from a training set , enabling the training of a DG algorithm capable of generalizing to unseen domains . in life - critical scenarios such as medical image analysis . For example , in skin cancer diagnosis using dermoscopic images , models may excessively rely on “ dermoscopic artifacts ” such as rulers , gel bubbles , dark corners , and hairs [ 3]–[5 ] , rather than learning the correct lesion patterns , leading to unreliable diagnoses . Similarly , models for Diabetic Retinopathy ( DR ) classification in ophthalmology can overfit specific camera styles , rendering them ineffective on novel images with dif- ferent style features [ 6 ] , [ 7 ] . This overreliance on specific cues rather than learning real patterns hinders the models ’ performance in real - world clinical environments where such cues are absent or inconsistent . A number of methods have been proposed to alleviate this issue from the perspective of domain generalization ( DG ) . DG aims to train models on multiple different but related domains and expects them to perform well on unseen test domains . For instance , illustrated in the upper part of Fig . 1 , the domains within medical datasets can be various factors , including dermoscopic artifacts such as hairs , rulers , and dark corners for skin cancer diagnosis , distinct camera devices for dia- arXiv:2401.03002v1 [ eess . IV ] 5 Jan 2024 \n betic retinopathy ( DR ) classification , and diverse hospitals for histopathology images . However , DG sitll remains underex- plored and relatively ineffective in medical image analysis due to several reasons from different perspectives . From a dataset- centric perspective : 1 ) domain labels in medical datasets are often unavailable as they are expensive and laborious to acquire ; 2 ) the definition of visual domains in medical datasets is more ambiguous than natural images that can be clearly defined ( e.g. , photo , art painting , sketch , and cartoon in the PACS [ 8 ] dataset ) . Specialists differing in clinical experience , diagnostic interests , etc . , may have varying opinions on the optimal domain definition , making it"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 2,
    "text": "that can be clearly defined ( e.g. , photo , art painting , sketch , and cartoon in the PACS [ 8 ] dataset ) . Specialists differing in clinical experience , diagnostic interests , etc . , may have varying opinions on the optimal domain definition , making it challenging to define domains accurately ; 3 ) the domain splitting in medical datasets can be heavily associated with the downstream tasks , making it difficult to transfer DG algorithms from one medical task to another due to inconsistent domain labels . From an algorithm- centric perspective , previous DG algorithms that learn domain- invariant features can also cause to ignore signals that are useful for unseen novel domains [ 9 ] . Although ensemble learning methods based on domain experts [ 10 ] , [ 11 ] can mitigate this limitation by learning domain - specific knowledge from different source domains independently , they overlook the rich cross - domain information that all domain experts can collectively contribute to the target domain prediction . To address the domain challenges specific to medical image analysis , in this paper , we reconceptualize the DG problem in medical datasets as latent domain generalization ( LDG ) , where generalized models are desired to learn from multiple underlying medical domains without relying on any pre- defined domain labels , as shown in the lower part of Fig.1 . Correspondingly , we proposed a novel , universal , prompt- driven LDG framework , called PLDG ( Prompt - driven Latent Domain Generalization ) , to alleviate the above - mentioned challenges from both perspectives . The proposed PLDG framework consists of unsupervised domain discovery and domain prompt learning . The unsuper- vised domain discovery module aims to address the dataset- centric LDG challenges . We propose to discover and cluster the implicit dataset biases utilizing the Simplicity Bias prop- erty of learning - based algorithms [ 12]–[14 ] . The clustering is performed based on the style features extracted from the shallow layer of the Vision Transformer ( ViT ) . As the style features contain the implicit cues of common medical biases such as artifacts , skin tone , and image style , their clusters can serve as pseudo - domain labels . To address the algorithm- centric problem , we propose an ensemble - like domain prompt learning strategy , which leverages multiple lightweight domain prompts to enhance the learning of domain - specific knowledge from diverse source domains . Unlike existing ensemble - like methods that learn domain knowledge independently , we in- troduce a domain prompt generator to enable different domain prompts to collaborate and benefit mutually via low - rank weight updating so as to facilitate cross - domain knowledge learning . Furthermore , we employ a domain mixup strategy to mitigate the problem of noisy domain label assignments caused by unsupervised domain clustering . In this paper , we make the following contributions : 1 ) We"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 3,
    "text": "- rank weight updating so as to facilitate cross - domain knowledge learning . Furthermore , we employ a domain mixup strategy to mitigate the problem of noisy domain label assignments caused by unsupervised domain clustering . In this paper , we make the following contributions : 1 ) We present a novel framework called Prompt - driven Latent Domain Generalization ( PLDG ) to address domain generalization in medical image classification without the need for explicit reliance on domain labels . 2 ) We propose a novel Simplicity Bias - guided pseudo domain label discovery method for arbitrary medical datasets . 3 ) We propose a prompt - based DG algorithm that takes advantage of a ViT - based domain prompt learning strategy and a novel domain prompt generator to promote cross - domain knowledge learning . 4 ) We benchmark our LDG framework and compare it with extensive existing DG algorithms using ViT - based backbones on three medical tasks and one debiasing task . The results demonstrate that our method achieves comparable or even superior performance without relying on any domain labels . The preliminary version of our work , presented at MICCAI 2023 [ 15 ] , introduced the first prompt - based domain general- ization method for skin lesion recognition . However , similar to most DG algorithms , the previous work limited itself to a nar- row domain generalization task specific to skin lesion datasets and required annotated dermoscopic artifacts as domain labels . In this paper , we have extended the original method from a conventional DG framework to a novel LDG framework , which is universal for different medical classification datasets without relying on domain labels . The main advancements include : 1 ) we incorporate a novel Simplicity Bias - guided clustering step , which can discover pseudo domain labels directly from the datasets , eliminating the requirement for pre - defined domain labels ; 2 ) we make the framework ap- plicable to diverse medical classification datasets , offering flexibility and adaptability across a wide range of scenarios ; 3 ) we validate the effectiveness of our method in far more datasets and tasks , including the original skin lesion datasets ( Dermatology ) , four fundus datasets ( Ophthalmology ) and the Camelyon17 - wild ( Histopathology ) benchmark dataset ; 4 ) we provide a thorough analysis of important components and hyper - parameters to ensure the stability of our approach ; 5 ) we benchmark both DG and LDG algorithms across three distinct medical classification tasks to facilitate future research . II . RELATED WORK A. Domain Generalization Domain generalization focuses on learning models that can generalize well to unseen target domains despite distribution shifts . Previous approaches have focused on learning domain- invariant features . For instance , DANN [ 16 ] aligns feature dis- tributions from different source domains using an adversarial loss , while CORAL [ 17 ] matches the second - order statistics"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 4,
    "text": "well to unseen target domains despite distribution shifts . Previous approaches have focused on learning domain- invariant features . For instance , DANN [ 16 ] aligns feature dis- tributions from different source domains using an adversarial loss , while CORAL [ 17 ] matches the second - order statistics of different source domains . Other methods utilize model ensembles to explicitly learn domain knowledge through dif- ferent model parameters . DAS and DNM [ 10 ] , [ 11 ] learn an ensemble of multiple classifiers or batch normalization statistics for different source domains . DoPrompt [ 18 ] embeds extra prompts into the network to capture domain - specific knowledge independently . Additional techniques include meta- learning [ 19 ] , [ 20 ] , feature disentanglement [ 21 ] , data aug- mentation [ 22 ] , and distributional robust learning [ 23 ] for achieving domain generalization . \n Another direction for domain generalization is to leverage the power of deep learning architectures to learn stronger representations . Sarath et al . [ 24 ] demonstrate different archi- tectures exhibit varying performance on domain generaliza- tion datasets , with the vanilla Empirical Risk Minimization ( ERM ) outperforming many state - of - the - art algorithms when benchmarked using ResNet-50 . Additionally , Dosovitskiy et al . [ 25 ] show transformer - based architectures generally out- perform ResNet-50 on domain generalization datasets as they are less biased towards texture . In this paper , we extensively benchmark domain generalization algorithms using the Vision Transformer ( ViT ) backbone on medical domain generaliza- tion datasets and take advantage of ViT ’s design to develop our own prompt - driven domain generalization algorithm . B. Domain Generalization in Medical Images Compared to domain generalization in natural images , do- main generalization in medical images has received relatively less attention , mainly due to the challenges in obtaining domain labels for medical datasets . Bissoto et al . [ 3 ] annotate artifact - based domain labels in skin datasets using multiple artifact classifiers , while Mohammad et al . [ 6 ] combine four common Diabetic Retinopathy ( DR ) datasets to construct the largest benchmark for domain generalization in DR classifi- cation where domain labels reflect as different datasets . How- ever , both approaches have limitations compared to domain generalization datasets in natural images . For instance , the domain labels in skin datasets are noisy as they are only annotated by binary classifiers rather than human annotators . The definition of domains in the DR dataset is also suboptimal , as images from EyePACS [ 26 ] and APTOS [ 27 ] datasets are captured by multiple different cameras . The domain label , in this case , is dataset difference . Therefore , there is a need for latent domain generalization methods that can infer reasonable pseudo domain labels for domain generalization in any medi- cal classification dataset . To acheive it"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 5,
    "text": "are captured by multiple different cameras . The domain label , in this case , is dataset difference . Therefore , there is a need for latent domain generalization methods that can infer reasonable pseudo domain labels for domain generalization in any medi- cal classification dataset . To acheive it , our work clusters style- based features as pseudo - labels . Although Toshihiko et al . [ 28 ] explored inferring style features by clustering convolutional feature statistics from CNN during each training iteration , there is no existing work that explores how to obtain style features from ViT architectures in an efficient way . C. Prompt Learning Prompt learning was originally designed for natural lan- guage processing and involved prepending heuristics ( manu- ally designed ) or learnable prompts ( continuous vectors ) into the input text , enabling large language models to handle vari- ous downstream tasks . Recently , prompt learning has also been applied to computer vision tasks . VPT [ 29 ] inserts a series of learnable randomly initialized prompts into the pre - trained ViT and optimizes these prompts for diverse downstream tasks using corresponding task labels . Wang et al . [ 30 ] incorporates prompt tuning methods into continual learning tasks , which leverages multiple learnable prompts to handle corresponding tasks . Doprompt [ 18 ] designs a series of learnable prompts for different domains to capture domain - specific knowledge independently for domain generalization . In contrast to ex- isting methods , our prompt learning strategy introduces a novel domain prompt generator that enables different domain prompts to collaborate and learn from each other , explicitly enforcing prompts to learn cross - domain knowledge for target domain generalization . III . METHOD In conventional domain generalization ( DG ) , the training dataset Dtrain consists of M source domains , denoted as Dtrain = { Dk|k = 1 , ... , M } . Here , each source domain Dk is represented by nk labeled instances { ( xk j , yk j ) } nk j=1 , which can also be represented by Dtrain = { ( xi , yi , di)}n i=1 where di denotes the domain labels and n denotes the total training instances . The goal of DG is to learn a model G : X →Y from the M source domains so that it can generalize well in unseen target domains Dtest . In latent domain generalization ( LDG ) , unlike DG , the domain labels are unknown . The training dataset thus becomes Dtrain = { ( xi , yi)}n i=1 . The overall pipeline of our prompt - driven latent domain generalization method , PLDG , is shown in Fig.2 . Our method aims to learn a domain generalization model using pseudo- domain labels . To obtain style - based pseudo domain labels that capture spurious correlations in the data , we cluster the class token in the shallow layer of a"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 6,
    "text": "method , PLDG , is shown in Fig.2 . Our method aims to learn a domain generalization model using pseudo- domain labels . To obtain style - based pseudo domain labels that capture spurious correlations in the data , we cluster the class token in the shallow layer of a ViT model during the early epochs , as described in Section III - A. By utilizing the pseudo - domain labels , we propose using domain - based prompts to learn domain - specific knowledge in Section III - B. Furthermore , we encourage cross - domain knowledge learning and alleviate the noisy domain label assignments problems using a domain prompt generator and domain - based Mixup in Sections III - C and III - D. A. Simplicity Bias - guided Pseudo Domain Label Clustering In learning - based algorithms , there is often a tendency to overfit to biases towards simplicity , such as focusing on the background rather than the target lesion in medical images or capturing dermoscopic artifacts instead of skin lesions [ 12 ] – [ 14 ] , [ 31 ] , as known as Simplicity Bias . We leverage the Sim- plicity Bias property to identify bias attributes that are highly correlated with the target class . We then use these attributes as pseudo - domain labels . To identify the bias attributes , we fol- low the approach of previous works [ 32 ] , [ 33 ] and cluster the model ’s easy - to - learn biased features within each class . To en- sure that the clustering captures domain - related features rather than category - related features , we leverage style features , as they provide a more discriminative cue for common medical biases , such as distinct color styles caused by different cameras or hospitals , dermoscopic artifacts and skin tone . Although existing works [ 32 ] utilize convolutional statistic features as style features to perform clustering , there is limited research on applying this approach to ViT architectures . Inspired by ViT- based style transfer algorithms , Tumanyan et al . [ 34 ] propose an appearance loss Lapp to align the style features of ViT between the generated It and appearance image Ia via the \n Illustration of our prompt - driven latent domain generalization ( PLDG ) algorithm , ( a ) We perform one - time clustering on the CLS token from the shallow layer of the ViT model to discover the bias - related pseudo domain labels ( see III - A ) . ( b ) Train a domain prompt - based ViT to learn domain - specific knowledge for unseen domain prediction ( see III - B ) . A domain prompt generator is further employed to facilitate cross - domain knowledge learning ( see III - C ) . CLS token : Lapp = ∥T L cls(It ) −T L cls(Ia)∥2 ( 1 ) where T L cls is"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 7,
    "text": "domain prediction ( see III - B ) . A domain prompt generator is further employed to facilitate cross - domain knowledge learning ( see III - C ) . CLS token : Lapp = ∥T L cls(It ) −T L cls(Ia)∥2 ( 1 ) where T L cls is the CLS token extracted from layer L of the ViT. In our task , we utilize the CLS token T 1 cls from the shallow layers ( e.g. , block 1 ) of the ViT for one - time k - means clustering , as shown in Fig . 2 . This choice is motivated by the fact that the shallow layers provide global appearance and style information [ 25 ] , [ 34 ] , as also demonstrated in section IV- E. Additionally , we find that performing one - time clustering in the early epochs is sufficient and yields stable results for discovering the pseudo - domain labels , as demonstrated in Section IV - E. Once we obtain these pseudo - domain labels , we can apply domain generalization algorithms based on them to improve the model ’s performance on unseen data . B. Domain - specific Prompt Learning with Vision Transformer Domain - specific learning : To enable the pre - trained vi- sion transformer ( ViT ) to capture knowledge from different domains , we define a set of M learnable domain prompts produced by a domain prompt generator ( introduced in III- C ) , denoted as PD = { P m ∈Rd}M m=1 , where d is the same size as the feature embedding of the ViT , and each prompt P m corresponds to one specific domain . To incorporate these prompts into the model , we follow the conventional practice of visual prompt tuning [ 29 ] , which prepends the prompts PD into the first layer of the transformer . Particularly , for each prompt P m in PD , we extract the domain - specific features as : Fm(x ) = F ( [ X1 , P m , E1 ] ) ( 2 ) where F is the feature encoder of the ViT , X1 denotes the class token , E1 is the image patch embedding , Fm is the feature extracted by ViT with the m - th prompt , and 1 is the index of the first layer . Domain prompts PD are a set of learnable tokens , with each prompt P m being fed into the vision transformer along with the image and corresponding class tokens from a specific domain ; the domain - specific prompt optimization is defined as : Ldomain = LCE(H(Fm(x ) ) , y ) ( 3 ) where H is the classification head , LCE is the cross entropy loss . Through optimizing , each prompt P m becomes a domain expert only responsible for the images from its own domain . By the self - attention mechanism of"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 8,
    "text": ") ) , y ) ( 3 ) where H is the classification head , LCE is the cross entropy loss . Through optimizing , each prompt P m becomes a domain expert only responsible for the images from its own domain . By the self - attention mechanism of ViT , the model can effectively capture domain - specific knowledge from the domain prompt tokens . Domain Mixup : While optimizing Ldomain based on the pseudo - domain labels inferred by clustering , a challenge arises due to the potential incorrect assignments of domains . To alleviate this issue , we propose employing the domain mixup strategy [ 22 ] on the domain loss term ( Ldomain ) to leverage inter - domain information . Instead of assigning a binary label ( ” 0 ” or ” 1 ” ) to each image , a mixing operation is applied to every image in each batch . This mixing operation involves randomly selecting two images from different domains and combining them . As shown in Fig . 3.b , the loss function Lmixup is then computed based on the predictions of the mixed images and their corresponding labels : Lmixup = λLCE(H(Fm(xmix ) ) , yi ) + ( 1 −λ)LCE(H(Fm(xmix ) ) , yj ) ( 4 ) where xmix = λxk i + ( 1 −λ)xq j ; xk i and xq j are samples from randomly two different domains k and q , and yk i and yq j are the corresponding labels . C. Cross - domain Knowledge Learning using Domain Prompt Generator To facilitate effective knowledge sharing across different domains while preserving the domain - specific parameters of each domain prompt , we introduce a domain prompt generator , as illustrated in Fig . 3.a . Our approach draws inspiration from model adaptation and multi - task learning techniques used in natural language processing [ 35 ] , [ 36 ] . Aghajanyan et al . [ 37 ] \n strategy . have demonstrated that when adapting a model to a specific task , the weight updates exhibit a low intrinsic rank . Similarly , each domain prompt P m should possess a unique low intrinsic rank when learning knowledge from its respective domain . To achieve this , we decompose each P m into a Hadamard product between a randomly initialized shared prompt P ∗and a rank - one matrix Pk obtained from two randomly initialized learnable vectors uk and vk , given by : P m = P ∗⊙Pk where Pk = uk · vT k ( 5 ) Here , P m represents the domain - specific prompt , computed as the Hadamard product of P ∗and Pk . The shared prompt P ∗∈Rs×d is used to learn general knowledge , where s and d denote the dimensions of the prompt vector and feature embedding , respectively . On the other hand , Pk is computed using domain - specific trainable vectors"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 9,
    "text": "Hadamard product of P ∗and Pk . The shared prompt P ∗∈Rs×d is used to learn general knowledge , where s and d denote the dimensions of the prompt vector and feature embedding , respectively . On the other hand , Pk is computed using domain - specific trainable vectors : uk ∈Rs and vk ∈Rd . These vectors capture domain - specific information in a low- rank space . By decomposing the domain prompts into rank - one subspaces , the model can effectively learn domain information . The Hadamard product enables the model to leverage cross- domain knowledge for target domain prediction . D. Optimization and Inference So far , we have introduced Lmixup in Eq . 4 for optimizing our model . However , since our goal is to generalize the model to unseen environments , we need to take advantage of each domain prompt . Instead of assigning equal weights to each domain prompt , we employ an adapter [ 18 ] that learns the linear correlation between the domain prompts and the target image prediction . To obtain the weighted prompt for inference in the target domain , we define it as a linear combination of the source domain prompts : Pweighted = A(F(x ) ) = M X m=1 wm · P m , s.t . M X m=1 wm = 1 ( 6 ) where A represents an adapter containing a two - layer MLP with a softmax layer , and wm denotes the learned weights . To train the adapter A , we simulate the inference process for each image in the source domain by treating it as an image from the pseudo - target domain . Specifically , we first extract features from the ViT : ˆ Fm(x ) = F([X0 , E0 ] ) . Then we predict the weighted prompt Pweighted for the pseudo- target environment image x using the adapter A : Pweighted = A ( ˆ Fm(x ) ) . Next , we extract features from ViT conditioning on the weighted prompt : ˆ Fm(x ) = F ( [ ˆ Fm(x ) , Pweighted , E0 ] ) . Finally , the classification head H is applied to predict the label y : y = H ( ˆFm(x ) ) . Additionally , the inference process is the same as the simulated inference process during adapter training , and our final prediction will be conditioned on the weighted prompt Pweighted . To ensure that the adapter learns the correct linear corre- lation between the domain prompts and the target image , we use the pseudo domain label from source domains to directly supervise the weights wm . We also use the cross - entropy loss to maintain the model performance with the weighted prompt : Lweighted = LCE(H ( ˆFm(x ) ) , y ) + λ ( 1 M M X m=1 M ( LCE(wm m , 1 ) + X t̸=m LCE(wm t ,"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 10,
    "text": ". We also use the cross - entropy loss to maintain the model performance with the weighted prompt : Lweighted = LCE(H ( ˆFm(x ) ) , y ) + λ ( 1 M M X m=1 M ( LCE(wm m , 1 ) + X t̸=m LCE(wm t , 0 ) ) ( 7 ) where ˆFm(x ) is the obtained feature map conditioned on the weighted prompt Pweighted , and H is the classification head . The total loss is then defined as Ltotal = Lmixup+Lweighted . IV . EXPERIMENTAL RESULTS A. Experimental Setup 1 ) Dataset Description : We evaluate the generalization abil- ity of our method in four challenging classification settings that closely mimic real - world scenarios . Notably , our method stands out by not relying on domain labels . However , to facilitate a meaningful comparison with conventional domain generalization algorithms that require domain labels , we se- lected three domain generalization datasets for evaluation . It is important to note that during our evaluation , our method was assessed without utilizing any domain labels , while all other baseline algorithms were evaluated using domain labels . 1 ) DG in Melanoma Classification : We use the ISIC2019 dataset [ 38 ] for training and validation , which consists of melanoma and benign categories . The training set contains 12,360 images , and the validation set contains 2,060 images . In this setting , the domain labels are defined based on artifact an- notations from [ 3 ] . The training set of ISIC2019 is divided into five groups : dark corner , hair , gel bubble , ruler , and clean , with 2,351 , 4,884 , 1,640 , 672 , and 2,796 images , respectively . For testing , we use four out - of - distribution ( OOD ) datasets from [ 3 ] : Derm7pt - Dermoscopic [ 39 ] , Derm7pt - Clinical [ 39 ] , PH2 [ 40 ] , and PAD - UFES-20 [ 41 ] . These datasets consist of 872 , 839 , 200 , and 531 images , respectively . It is impor- tant to note that ISIC2019 , Derm7pt - Dermoscopic , and PH2 are dermoscopic images , while Derm7pt - Clinical and PAD are clinical images . Model selection is performed using the training - domain validation set method [ 42 ] . 2 ) DG in Diabetic Retinopathy ( DR ) Classification : To evaluate OOD generalization in DR classification , followed by [ 6 ] , we combine four commonly used DR datasets : EyePACs [ 26 ] , Aptos [ 27 ] , Messidor , and Messidor 2 [ 43 ] . The com- bined datasets contain 35,126 , 3,657 , 1,200 , and 1,744 images , respectively . The datasets consist of five categories , with grade 0 being the lowest form of DR and grade 4 being the most proliferative . Following [ 6 ]"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 11,
    "text": "2 [ 43 ] . The com- bined datasets contain 35,126 , 3,657 , 1,200 , and 1,744 images , respectively . The datasets consist of five categories , with grade 0 being the lowest form of DR and grade 4 being the most proliferative . Following [ 6 ] , we train and validate our model on three datasets and test it on the remaining one . We report the testing results on all four datasets using this method . \n TABLE I THE COMPARISON RESULTS ON FOUR OUT - OF - DISTRIBUTION MELANOMA CLASSIFICATION DATASETS ( ROC - AUC ) Method DM7 D DM7 C PAD PH2 Average ERM 80.23 72.00 75.74 84.64 78.15 DRO [ 23 ] 82.55 72.86 80.02 84.97 80.10 CORAL [ 17 ] 80.12 71.24 88.17 86.98 81.62 MMD [ 45 ] 81.40 71.34 84.95 87.12 81.20 DANN [ 16 ] 81.46 72.07 83.94 85.94 80.85 IRM [ 46 ] 77.00 70.21 74.847 78.84 75.13 MLDG [ 19 ] 82.94 68.57 78.59 88.14 79.56 CAD [ 47 ] 82.72 69.57 81.36 88.4 81.51 DoPrompt [ 18 ] 82.38 71.61 83.81 91.33 82.06 SelfReg [ 48 ] 81.43 73.18 85.78 89.28 82.42 MMLD† [ 28 ] 79.6 88.8 81.3 79.68 EPVT [ 15 ] 83.25 74.52 87.41 92.53 84.43 PLDG†(Ours ) 83.69 72.03 89.92 89.09 83.68 * †indicates the DG algorithm without domain labels . * Bold indicates the best result . * Underline indicates the second - best result . TABLE II THE COMPARISON RESULTS ON OUT - OF - DISTRIBUTION DIABETIC RETINOPATHY CLASSIFICATION DATASETS ( ACC ) Method EyePACS APTOS Messidor Messidor2 Average ERM 74.53 71.44 57.75 60.03 65.94 CORAL [ 17 ] 75.21 71.65 56.83 62.27 66.49 Fishr [ 49 ] 74.53 71.68 57.83 59.62 65.92 DANN [ 16 ] 75.38 68.32 54.92 64.6 65.81 SelfReg [ 48 ] 75.98 69.41 58.5 62.06 66.49 DoPrompt [ 18 ] 73.04 71.33 56.25 62.5 65.78 MMLD † [ 28 ] 71.32 66.5 55.12 61.39 63.58 EPVT [ 15 ] 74.59 71.57 55.58 64.45 66.52 PLDG †(Ours ) 73.8 73.32 57.97 65.22 67.58 3 ) DG in Cancerous Tissue Detection : The dataset CAMELYON17 - WILDS [ 44 ] comprises histopathology im- ages captured across different hospitals . Each image represents a 96x96 patch from a whole - slide image ( WSI ) of a lymph node section from a patient with potentially metastatic breast cancer . The category label indicates whether the patch contains a tumor , and the domain labels correspond to the five hospitals . The training set consists of 302,436 patches from 30 WSIs belonging to the first three hospitals . The validation set con- tains 34,904 patches from the fourth hospital , and the testing set contains 33,560 patches from the fifth hospital . Model selection is performed using the OOD validation method [ 44 ] . 4 ) Debiasing in Skin Datasets : We use the trap skin dataset [ 3 ] that contains seven artifacts . The dataset"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 12,
    "text": "hospital , and the testing set contains 33,560 patches from the fifth hospital . Model selection is performed using the OOD validation method [ 44 ] . 4 ) Debiasing in Skin Datasets : We use the trap skin dataset [ 3 ] that contains seven artifacts . The dataset consists of six trap sets with increasing bias levels , ranging from 0 ( randomly split training and testing sets from the ISIC2019 dataset ) to 1 ( the highest bias level where the correlation between artifacts and class label is in the opposite direction in the dataset splits ) . As the bias factor increases , the distribution difference caused between the training and testing sets also increases . 2 ) Implementation Details : For a fair comparison , we use the ViT - Base/16 [ 25 ] backbone , pre - trained on the ImageNet , as the base model for all experiments . The evaluation metrics include Accuracy for DR classification and the Camelyon17- WILDS dataset , and ROC - AUC for all other datasets . Hy- perparameters play a crucial role in domain generalization algorithms , we conduct a grid search over the following hyperparameters for all models : learning rate ( ranging from TABLE III THE COMPARISON RESULTS ON HOSPITAL FIVE ON CANCEROUS TISSUE DETECTION DATASETS Method Accuracy ERM 73.1 CORAL [ 17 ] 71.8 DANN [ 16 ] 83.5 IRM [ 46 ] SelfReg [ 48 ] 70.4 MMLD † [ 28 ] 70.2 EPVT [ 15 ] 86.4 PLDG †(Ours ) 84.3 3e−4 to 5e−8 ) , weight decay ( ranging from 1e−2 to 1e−5 ) , and prompt length ( ranging from 4 to 16 , when available ) . We report the best performance achieved among all models . After the grid search , we employ the AdamW optimizer with specific hyperparameter settings for each task . For melanoma classification , we set the learning rate to 5e−6 , weight decay to 1e−2 , and the prompt length to 4 . For cancerous tissue detection , we use a learning rate of 5e−6 , weight decay of 1e−4 , and a prompt length of 10 . For DR classification , we use a learning rate of 5e−7 , weight decay of 1e−5 , and a prompt length of 4 . All input images are resized to 224×224 . Standard data augmentation techniques , such as random flip , crop , rotation , and color jitter , are applied . To prevent overfitting , we employ early stopping with patience of 22 . All models are trained for a total of 60 epochs for out - of - distribution ( OOD ) evaluation and 100 epochs for trap set debiasing . The experiments are conducted on two NVIDIA RTX 3090 GPUs . 3 ) Baseline Methods : We compare our method with rep- resentative strong domain generalization baselines from the domainbed codebase [ 42 ] . These baselines cover various ap- proaches"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 13,
    "text": "evaluation and 100 epochs for trap set debiasing . The experiments are conducted on two NVIDIA RTX 3090 GPUs . 3 ) Baseline Methods : We compare our method with rep- resentative strong domain generalization baselines from the domainbed codebase [ 42 ] . These baselines cover various ap- proaches in the domain generalization literature , including domain invariant representation learning [ 16 ] , [ 17 ] , [ 46 ] , dis- tributionally robust optimization [ 23 ] , feature disentanglement [ 21 ] , ensemble learning [ 18 ] , meta - learning [ 19 ] , gradient operation [ 49 ] , prompt learning [ 18 ] , self - supervised learning [ 48 ] , latent domain adversarial learning [ 28 ] and others [ 47 ] . To ensure a fair comparison , we benchmark all algorithms using the same ViT - Base/16 backbone , except for latent do- main adversarial learning [ 28 ] , which uses ResNet50 as it is specifically designed for convolutional neural networks . Additionally , we denote our method that utilizes domain labels ( without the clustering step in section III - A ) as EPVT , which corresponds to our conference version [ 15 ] . Furthermore , we refer to our method without using domain labels as PLDG , which represents the latent domain generalization method proposed in this work . B. Comparisons with existing domain generalization methods 1 ) Melanoma classification and Cancerous tissue detection : Table I and Table III present a comparison of our PLDG algorithm with existing domain generalization methods , also including our algorithm with domain labels ( EPVT ) on Melanoma classification and Cancerous Tissue detection . The results clearly demonstrate the superiority of our approach . In melanoma classification , our PLDG algorithm achieves the \n ( a ) prompt length ( b ) cluster number ( a ) w/ domain labels ( b ) wo/ domain labels domain labels ( b ) . TABLE IV ABLATION STUDY OF PLDG ON FOUR OOD SKIN DATASETS Method DM D DM C PAD PH2 Average baseline 80.23 72.00 75.74 84.64 78.15 + P 81.93 73.56 82.82 87.89 81.55 + P+A 83.05 72.45 84.95 86.17 81.67 + P+A+M 82.55 73.73 86.80 86.61 82.42 + P+A+M+G 83.69 72.03 89.92 89.09 83.68 best performance on two out of four OOD datasets and shows remarkable improvements over the ERM algorithm . Specifi- cally , we achieve a 3.46 % improvement on the Derm7pt derm dataset and a significant 14.18 % improvement on the PAD dataset . Our algorithm also outperforms most state - of - the - art domain generalization algorithms on average performance and achieves competitive average performance with our method that utilizes domain labels . Similarly , our PLDG achieves the second - best performance on Cancerous Tissue Detection datasets , only performing worse than our method using do- main labels . These results highlight the practicality of our latent domain generalization algorithm"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 14,
    "text": "achieves competitive average performance with our method that utilizes domain labels . Similarly , our PLDG achieves the second - best performance on Cancerous Tissue Detection datasets , only performing worse than our method using do- main labels . These results highlight the practicality of our latent domain generalization algorithm in the medical setting , as it shows comparable performance with the best domain generalization algorithm while eliminating the requirement for domain labels . This performance of our method also emphasizes the effectiveness of our prompt learning strategy in learning robust features for detecting melanoma and cancerous tissues , showcasing its potential in medical applications . 2 ) Diabetic Retinopathy classification : Table II shows the comparison of our method with existing DG methods on DR classification . It can be seen that our method achieves the best average performance , surpassing even our algorithm with domain labels ( EPVT ) . This result is surprising but reasonable considering the characteristics of the DR dataset benchmark [ 6 ] . Unlike the previous two tasks , the DR dataset benchmark is created by simply combining four different datasets , where the domain labels are assumed to represent style differences TABLE V ABLATION STUDY OF PLDG ON DR CLASSIFICATION DATASETS Method APTOS Messidor2 Average baseline 71.44 60.03 65.74 + P 72.01 61.07 66.54 + P+A 71.64 61.75 66.61 + P+A+M 72.43 63.15 67.79 + P+A+M+G 73.32 64.62 68.97 primarily caused by variations in cameras . However , the im- ages from the EyePACS and APTOS datasets are captured using multiple types of cameras , which makes the dataset- based domain separation sub - optimal . Further , it can be seen that all conventional domain generalization algorithms that rely on domain labels do not significantly improve the ERM baseline performance . In contrast , our PLDG algorithm shows a significant improvement in performance . This indicates that latent domain generalization is more effective when domain labels are noisy or unavailable , as it can effectively capture and utilize the underlying latent domain of the data . C. Ablation Study 1 ) Contribution of different components : We conduct abla- tion studies to analyze each component of our model , as shown in Table IV and Table V. We set our baseline as the Empirical Risk Minimization ( ERM ) algorithm followed by conventional DG papers [ 42 ] , and we gradually add P ( prompt [ 29 ] ) , A ( Adapter ) , M ( Mixup ) , and G ( domain prompt generator ) into the model . For methods without an adapter , we use an equal- weighting mechanism . The performance of each component is evaluated on melanoma datasets and DR classification datasets . Firstly , we added multiple randomly initialized learn- able prompts into the baseline and optimized them using the Ldomain loss function in Equation 3 , denoted as “ + P ” . Compared to the baseline , using the domain prompt"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 15,
    "text": "evaluated on melanoma datasets and DR classification datasets . Firstly , we added multiple randomly initialized learn- able prompts into the baseline and optimized them using the Ldomain loss function in Equation 3 , denoted as “ + P ” . Compared to the baseline , using the domain prompt learning strategy improved the average ROC - AUC by 3.39 % and the \n ( a ) L1 ( b ) L5 ( c ) L8 ( d ) L12 epoch assignments . average accuracy by 0.8 % for the melanoma and DR datasets , respectively . This result demonstrates the effectiveness of the domain prompt learning component . Next , we incorporated the adapter and domain - based Mixup into the model , denoted as “ + P+A+M ” . Compared to ” + P ” , the model achieved an average improvement of 0.87 % and 1.16 % on the melanoma and DR datasets , respectively . This finding highlights the importance of addressing wrong label assignments and uti- lizing domain weighting to improve generalization . Finally , we incorporated the domain prompt generator into the model , resulting in our PLDG approach , denoted as “ + P+A+M+G ” . It can be observed that combining the domain prompt generator improved the average ROC - AUC by 1.26 % and the average accuracy by 1.18 % on the two tasks . This emphasizes the importance of facilitating cross - domain learning in the context of medical domain generalization . 2 ) Analysis on hyper - parameters : In our method , the prompt length and cluster number are two important hyperparameters . We investigate the impact of different prompt lengths and cluster numbers on the performance of our method , and the results are shown in Fig . 4 . For the prompt length , we find that setting it to 4 leads to the best average performance on both the skin and DR datasets . Moreover , when the prompt length is set to 10 , our method achieves the best performance on specific datasets such as Derm7pt clinic and PH2 . For the cluster number , we observe that setting it to 4 results in the best average performance for both datasets . Interestingly , we find that our method is not highly sensitive to the cluster number , as it consistently outperforms most domain generalization baselines when using different values of the cluster number ranging from 2 to 5 . D. Domain prompt weights analysis To evaluate whether our method has successfully learned the correct domain prompts for target domain prediction , we conduct an analysis and plot the results in Fig . 5 . This analysis is performed for both our method with domain labels and our method without domain labels . Firstly , we extract the features of each domain ( or pseudo domain using clustering ) from our training set ISIC2019 , and we also extract the features from a target"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 16,
    "text": ". This analysis is performed for both our method with domain labels and our method without domain labels . Firstly , we extract the features of each domain ( or pseudo domain using clustering ) from our training set ISIC2019 , and we also extract the features from a target dataset , Derm7pt - Clinc . Using these extracted features , we calculate the Frechet distance [ 50 ] between each domain and the target dataset , which represents the domain distance between them . Next , we record the learned weights of each domain prompt . When domain labels are available , we observe that our model assigns the highest weight to the ” dark corner ” group , as the domain distance between the ” dark corner ” group and the Derm7pt - Clin dataset is the closest , as shown in the right panel of Fig . 5(a ) . This indicates that the ” dark corner ” group shares the most similar domain information with the target dataset , and thus , it is given the highest weight . On the other hand , the ” clean ” group is assigned the smallest weight , as the domain distance between the ” clean ” group and the target dataset is the largest . This suggests that the domains of the ” clean ” group are significantly different from the target domain and contain less useful information for target domain prediction . A similar relationship can be observed when our model uses pseudo domain labels , as shown in Fig.5.b . In both cases , there is a negative correlation between the domain distance and the corresponding prompt ’s weights . This implies that our model can precisely learn the relevant knowledge from different domains and assign higher weights to the domains that are more similar to the target domain . E. Clustering analysis Our method acquires pseudo domain labels via one - time clustering . One concern is whether the clustering is performed based on category labels rather than visual domains . To inves- tigate this , we evaluated the correlation between the category labels and the pseudo domain labels . The results , shown in the blue lines in Fig . 6 , indicate the normalized mutual information ( NMI ) between them at different layers ( 1 , 5 , 8 , and 12 ) of the ViT. A high NMI value suggests a strong correlation . We observed that the correlation between pseudo domain labels \n Debiasing evaluation on trap set consisting of six skin datasets with different bias levels . and category labels is high in the last layer ( L12 ) of the ViT but low in the shallow layer ( L1 ) . This indicates that the clustering was not performed based on category labels when selecting the CLS token from the shallow layer ( L1 ) . Furthermore , we noticed that the NMI between pseudo domain labels and"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 17,
    "text": "of the ViT but low in the shallow layer ( L1 ) . This indicates that the clustering was not performed based on category labels when selecting the CLS token from the shallow layer ( L1 ) . Furthermore , we noticed that the NMI between pseudo domain labels and their previous assignments became stable after a few epochs in layer 1 . Based on these observations , we chose to cluster the CLS token from the later 1 in epoch 5 , where the clustering is stable and not based on category labels . However , Fig . 6 also shows that the clustering is not performed totally based on the pre - defined domain labels in the original dataset , as evidenced by the relatively low NMI between the pseudo domain labels and the original domain labels . Although previous work by Deecke et al . [ 51 ] showed that the use of pre - defined domain labels in original datasets is not necessary , given that the domain labels in many well- known domain generalization datasets are sub - optimal , we are still curious about how our method clusters the samples . To gain further insights , we visualize the CLS token features in layer 1 for the skin dataset using T - SNE in Fig . 7 . It reveals that the pseudo domain labels are still based on style features , with clusters representing ” ink marking , ” ” dark corner , ” ” dark skin , ” and ” light skin . ” This aligns with observations from the dermatology literature [ 3 ] , [ 52 ] . F. Trap set debiasing In Fig . 8 , we compare the performance of the ERM baseline , our method using domain labels ( EPVT ) , and our method without domain labels ( PLDG ) on six biased trap datasets . Each point on the graph represents an algorithm trained and tested on a specific bias degree split of the trap set . The graph illustrates that the ERM baseline outperforms our PLDG when the bias degree is low ( 0 and 0.3 ) . However , this can be attributed to the fact that ERM heavily relies on spurious correlations between artifacts and class labels , leading to overfitting the training set . As the bias degree increases , the correlation between artifacts and class labels decreases , and relying solely on artifacts for prediction becomes unreliable . This causes the performance of ERM to drop dramatically on the test set with a significant distribution difference . In contrast , our PLDG shows greater robustness to different bias levels . Notably , our PLDG outperforms the ERM baseline by 6.13 % on the bias 1 dataset . Although EPVT exhibits greater robustness than our PLDG , it is important to note that PLDG is more general for debiasing as it does not require domain labels for the dataset , making"
  },
  {
    "source": "2401.03002v1.pdf",
    "chunk_index": 18,
    "text": "Notably , our PLDG outperforms the ERM baseline by 6.13 % on the bias 1 dataset . Although EPVT exhibits greater robustness than our PLDG , it is important to note that PLDG is more general for debiasing as it does not require domain labels for the dataset , making it applicable in real - world scenarios where domain labels are unavailable or noisy . V. CONCLUSION In this paper , we introduce a latent domain generalization method for medical image classification , and we try to answer some important questions : ( 1 ) whether domain labels are always necessary for medical domain generalization , and ( 2 ) whether the latent domain generalization method can outper- form conventional domain generalization method that relies on domain labels . To answer these questions , we propose a prompt - driven latent domain generalization framework that leverages pseudo domain labels obtained through clustering . Our extensive ex- perimental results on different datasets have provided valuable insights . Firstly , we have shown that domain labels are not always necessary for achieving competitive performance in medical domain generalization tasks . Our method , without the use of domain labels , has achieved comparable performance to our method that employs domain labels and even outperformed most conventional SOTA domain generalization algorithms . This indicates that our method can effectively capture and leverage the underlying domain knowledge without explicitly relying on domain labels . Furthermore , our experiments have demonstrated that latent domain generalization methods can exhibit superior generalization abilities compared to conven- tional domain generalization methods , especially in scenarios where domain labels are either not available or not reliable . This highlights the practicality and versatility of our method in various medical settings , where obtaining precise domain labels can be challenging ."
  },
  {
    "source": "2401.05827v2.pdf",
    "chunk_index": 0,
    "text": "arXiv:2401.05827v2 [ cs . CL ] 3 Apr 2024 Published as a Tiny Paper at ICLR 2024 HALLUCINATION BENCHMARK IN MEDICAL VISUAL QUESTION ANSWERING Jinge Wu∗ , Yunsoo Kim∗ , Honghan Wu University College London { jinge.wu.20,yunsoo.kim.23,honghan.wu}@ucl.ac.uk ABSTRACT The recent success of large language and vision models ( LLVMs ) on vision ques- tion answering ( VQA ) , particularly their applications in medicine ( Med - VQA ) , has shown a great potential of realizing effective visual assistants for health- care . However , these models are not extensively tested on the hallucination phe- nomenon in clinical settings . Here , we created a hallucination benchmark of medi- cal images paired with question - answer sets and conducted a comprehensive eval- uation of the state - of - the - art models . The study provides an in - depth analysis of current models ’ limitations and reveals the effectiveness of various prompting strategies . INTRODUCTION The emergence of large language and vision models ( LLVMs ) like LLaVA ( Liu et al . , 2023b ) and its biomedical version , LLaVA - Med ( Li et al . , 2023 ) , marks signiﬁcant progress in AI for health- care , particularly in Medical Visual Question Answering ( Med - VQA ) . These models can be used to enhance clinical decision - making as a visual assistant . However , their performance remains ques- tionable , especially regarding the risk of ’ hallucination ’ - producing coherent but factually incorrect responses . Evaluating hallucination is important in healthcare as a visual assistant that hallucinates can cause misdiagnoses or inappropriate treatments . In healthcare , there are few VQA datasets avail- able ( Zhang et al . , 2023 ; He et al . , 2020 ; Lau et al . , 2018 ) , however , as far as we know there are no benchmark datasets that test the hallucination with multi - modality . In this paper , we created a bench- mark dataset for assessing model performance regarding hallucinatory responses in Med - VQA . We analyzed the state - of - the - art models , exploring their response accuracy to various types of medical images and textual queries . This comprehensiveanalysis provides a baseline score as well as insights into the current large vision and language models ’ capabilities and limitations in medical settings . The dataset and evaluation code are now available at https://github.com/knowlab/halt-medvqa . HALLUCINATION BENCHMARK CREATION We modify the three publicly available VQA datasets : PMC - VQA , PathVQA , and VQA - RAD , with the format of multiple - choice questions as hallucination benchmark ( Zhang et al . , 2023 ; He et al . , 2020 ; Lau et al . , 2018 ) . The following three scenarios are considered : • FAKE Question . Fake or nonsensical questions are used to examine the model ’s ability to detect incoherent questions ."
  },
  {
    "source": "2401.05827v2.pdf",
    "chunk_index": 1,
    "text": "Zhang et al . , 2023 ; He et al . , 2020 ; Lau et al . , 2018 ) . The following three scenarios are considered : • FAKE Question . Fake or nonsensical questions are used to examine the model ’s ability to detect incoherent questions . The fake questions are mostly generated by GPT3.5 - turbo , while a subset is extracted from Med - Halt ( Umapathi et al . , 2023 ) . We consider the follow- ing scenarios : 1 ) a fake and generic scenario , 2 ) a fake patient description , which can not be observed by the given image 3 ) fake medical questions which are not factually correct . • None of the Above ( NOTA ) . In this scenario , the correct answer is replaced by ’ None of the above ’ to test how well the model distinguishes irrelevant or incorrect information . • Image SWAP . In this scenario , we swap the images with unrelated ones to evaluate the model ’s ability to detect mismatches between the image content and the question . * Equal Distributions . \n Published as a Tiny Paper at ICLR 2024 means the number of irrelevant predictions in the results . FAKE NONE SWAP AVERAGE n = 542 n = 1000 n = 817 models accuracy # irr accuracy # irr accuracy # irr accuracy # irr LLaVA - Med 0.18 0.20 0.61 0.33 770.7 LLaVA - v0 - 7B 0.74 0.70 0.86 0.77 726.7 LLaVA - Med - pvqa 9.39 2.30 3.67 5.12 770.7 LLaVA - Med - slake 10.50 5.30 6.60 7.46 317.3 LLaVA - Med - rad 13.44 1.80 8.19 7.81 428.3 LLaVA - v1.5 - 7B 59.12 30.40 52.32 47.28 0.3 LLaVA - v1.5 - 13B 77.90 8.70 79.71 55.44 0.0 GPT-4 - turbo - vision 72.93 44.40 72.37 63.23 42.3 MODELS For evaluation , we mainly use LLaVA - based models including LLaVA - v0 - 7B , LLaVA - v1.5 - 7B , and LLaVA - v1.5 - 13B ( Liu et al . , 2023b;a ) . The medical domain ﬁnetuned version of LLaVA - v0- 7B , LLaVA - Med ( Li et al . , 2023 ) . Speciﬁcally for LLaVA - Med , we also compare three distinct versions , each ﬁne - tuned on separate VQA datasets : PathVQA ( ‘ LLaVA - Med - pvqa ‘ ) , VQA - RAD ( ‘ LLaVA - Med - rad ‘ ) , and SLAKE ( ‘ LLaVA - Med - slake ‘ ) . We also include OpenAI ’s GPT-4 - turbo- vision ( ‘ gpt-4 - vision - preview ‘ ) model * . EXPERIMENT AND RESULTS The model ’s performance is measured by the classiﬁcation accuracy of the prediction ’s token . If the model provides a token other than the given options , the prediction is regarded as wrong and irrelevant ( i.e. #"
  },
  {
    "source": "2401.05827v2.pdf",
    "chunk_index": 2,
    "text": "vision - preview ‘ ) model * . EXPERIMENT AND RESULTS The model ’s performance is measured by the classiﬁcation accuracy of the prediction ’s token . If the model provides a token other than the given options , the prediction is regarded as wrong and irrelevant ( i.e. # irr in Table 1 ) . If the model provides a token in the given options but a wrong answer , then the prediction is regarded as wrong only . We conduct an ablation study for various prompt styles , aiming to rigorously assess the models ’ performance ( Table 2 ) . The ablation study with the largest open source model that we use , LLaVA - v1.5 - 13B model , conﬁrms the effect of different prompting and shows that L + D0 prompt is the best strategy for hallucination evaluation , which we use for further evaluation ( Table 7 ) . The evaluation of hallucination for various models shows that the best LLaVA variant model is LLaVA - v1.5 - 13B model ( Table 1 ) . GPT-4 - turbo - vision model outperforms LLaVA - v1.5 - 13B model on average , but LLaVA - v1.5 - 13B model performs better in FAKE and SWAP scenarios . Also , regarding the number of irrelevant answers , LLaVA - v1.5 - 13B performs better than other models including GPT-4 - turbo - vision . This is also conﬁrmed by qualitative analysis of samples of response ( Table 6 ) . CONCLUSION Among the three scenarios , NOTA has the lowest accuracy for all the models , indicating its chal- lenge to the current LLVMs . In general , the models with improved backbone models , LLaVA- v1.5 - 7B and LLaVA - v1.5 - 13B , performs much better than all the the models based on LLaVA - v0 ( LLaVA - Med , LLaVA - Med - pvqa , LLaVA - Med - rad and LLaVA - Med - slake ) . We also ﬁnd that ﬁne- tuning in domain - speciﬁc data does not guarantee a performance boost in hallucination evaluation as LLaVA - Med performs worse than LLaVA - v0 - 7B. To conclude , LLaVA - v1.5 - 13B is more ro- bust than GPT-4 - turbo - vision in two scenarios ( FAKE and SWAP ) and less irrelevant predictions , making it less prone to hallucinations . * https://platform.openai.com/docs/guides/vision \n Published as a Tiny Paper at ICLR 2024 URM STATEMENT The authors acknowledge that at least one key author of this work meets the URM criteria of the ICLR 2024 Tiny Papers Track ."
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 0,
    "text": "Medical Dialogue Generation via Intuitive - then - Analytical Differential Diagnosis Kaishuai Xu1 , Wenjun Hou1,2 , Yi Cheng1 , Jian Wang1 , Wenjie Li1 1Department of Computing , The Hong Kong Polytechnic University , Hong Kong 2Research Institute of Trustworthy Autonomous Systems and Department of Computer Science and Engineering , Southern University of Science and Technology , Shenzhen , China { kaishuaii.xu , alyssa.cheng , jian-dylan.wang}@connect.polyu.hk , houwenjun060@gmail.com , cswjli@comp.polyu.edu.hk Abstract Medical dialogue systems have attracted growing research attention as they have the potential to provide rapid diag- noses , treatment plans , and health consultations . In medical dialogues , a proper diagnosis is crucial as it establishes the foundation for future consultations . Clinicians typically em- ploy both intuitive and analytic reasoning to formulate a dif- ferential diagnosis . This reasoning process hypothesizes and verifies a variety of possible diseases and strives to gener- ate a comprehensive and rigorous diagnosis . However , recent studies on medical dialogue generation have overlooked the significance of modeling a differential diagnosis , which hin- ders the practical application of these systems . To address the above issue , we propose a medical dialogue generation framework with the Intuitive - then - Analytic Differential Di- agnosis ( IADDx ) . Our method starts with a differential di- agnosis via retrieval - based intuitive association and subse- quently refines it through a graph - enhanced analytic proce- dure . The resulting differential diagnosis is then used to re- trieve medical knowledge and guide response generation . Ex- perimental results on two datasets validate the efficacy of our method . Besides , we demonstrate how our framework assists both clinicians and patients in understanding the diagnostic process , for instance , by producing intermediate results and graph - based diagnosis paths . Introduction Medical dialogue systems ( MDS ) endeavor to provide di- verse medical services such as diagnosis , treatment plans , and health consultations ( Yan et al . 2022 ; Xu et al . 2023 ; Chen et al . 2022b ) . These systems have garnered increasing research attention in recent years due to their potential to as- sist clinicians in diagnosing and prescribing ( Shi et al . 2023 ; Liu et al . 2022b ; Zeng et al . 2020 ; Zhou et al . 2021 ; He et al . 2022 ; Xu et al . 2019 ) . In medical dialogues , diagnosis is a crucial process as the results of diagnosis establish an essential foundation for sub- sequent consultations ( Maynard and Heritage 2005 ; Silver- man , Kurtz , and Draper 2016 ) . However , previous studies on medical dialogue generation using pre - trained language models neglected to explicitly model the diagnostic process ( Xu et al . 2023 ; Zhao et al . 2022 ; Li et al . 2021 ; Liu et al . Copyright © 2024 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) ."
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 1,
    "text": "pre - trained language models neglected to explicitly model the diagnostic process ( Xu et al . 2023 ; Zhao et al . 2022 ; Li et al . 2021 ; Liu et al . Copyright © 2024 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) . All rights reserved . How long have you been experiencing these symptoms ? Do you usually have a regular diet ? 👨 ⚕ Hi , I have been suﬀering from a dull pain in the upper right abdomen all these days . 🙍 Pain in this area primarily suggests cholecys))s and gastri)s . Have you had any tests done previously ? 👨 ⚕ So , gastri)s needs to be considered . I suggest … 👨 ⚕ For several years . My usual diet is relaDvely regular and healthy . 🙍 I had a B - ultrasound this year , and the results showed polyps in the gallbladder . 🙍 Differential Diagnosis digesDve system System Organ Disease Symptom gallbladder cholecys -- s pain in … feel sick acid reﬂux Similar paDent cases & disease docs Intuitive Analytic stomach gastri - s dialogue , which contains intuitive and analytic reasoning . 2021 ; Lin et al . 2021 ; Liu et al . 2022b ) . One significant is- sue with these methods is that although responses benefiting from pre - trained models may appear coherent , they usually lack an interpretation grounded in meticulous medical diag- nosis . It is challenging for clinicians or patients to accept responses from MDSs without a clear and interpretable di- agnostic basis ( Kurtz , Draper , and Silverman 2017 ) . In practice , clinicians typically employ both intuitive and analytic reasoning during the dialogue to formulate a differ- ential diagnosis , i.e. , a set of potential diseases guiding how the subsequent dialogue unfolds ( Croskerry 2009 ; Tchango et al . 2022 ; Silverman , Kurtz , and Draper 2016 ) . Intuitive reasoning forms a rough disease list through a quick re- view of extensive clinical experience , while analytic reason- ing cautiously verifies some diseases via a systematic analy- sis of body systems , organs , and symptoms . As an example shown in Figure 1 , if a patient is diagnosed with a high pos- sibility of gastritis but may still have chronic cholecystitis , arXiv:2401.06541v1 [ cs . CL ] 12 Jan 2024 \n the physician will first ask if any tests have been done to rule out chronic cholecystitis and then inquire about more gastritis - related symptoms to prescribe medications . Prior studies often overlook the importance of differential diag- nosis ( Li et al . 2021 ; Zhao et al . 2022 ; Xu et al . 2019 ; Wei et al . 2018 ; Kao , Tang , and Chang 2018 ) . In our work , we ar- gue that modeling a differential diagnosis with intuitive and analytic reasoning is crucial , and"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 2,
    "text": "2021 ; Zhao et al . 2022 ; Xu et al . 2019 ; Wei et al . 2018 ; Kao , Tang , and Chang 2018 ) . In our work , we ar- gue that modeling a differential diagnosis with intuitive and analytic reasoning is crucial , and generating responses con- ditioned on potential diseases can improves the reliability and accuracy of medical dialogue generation . To address the above issues , we propose a medical di- alogue generation framework with Intuitive - then - Analytic Differential Diagnosis ( IADDx ) , which first produces a dif- ferential diagnosis and then utilizes potential diseases to guide response generation . For differential diagnosis , we draw inspiration from the diagnostic reasoning research ( Croskerry 2009 ) and design a two - stage ( i.e. , intuitive - then- analytical ) differential diagnosis method . In the intuitive stage , we extract patients ’ conditions from the dialogue and use them to retrieve previous cases and disease documents that present similar situations . A preliminary list of potential diseases can be concluded from the cases and documents . In the analytical stage , we first create a diagnosis - oriented en- tity graph that contains body systems , organs , diseases , and symptoms . Then , we employ ConceptTransformer ( Rigotti et al . 2022 ) to incorporate the constructed graph and build a multi - disease classifier to discriminate multiple diseases , thereby assisting in refining the preliminary list . Our analyti- cal stage achieves a multi - disease classification and provides a faithful and plausible interpretation represented by entities on the graph . For response generation , we utilize refined po- tential diseases to retrieve medical knowledge and generate responses conditioned on the knowledge . Our main contributions are summarized as follows : • We propose a medical dialogue generation framework , IADDx , which explicitly models a differential diagnosis with intuitive - then - analytic reasoning and incorporates diagnosis to guide response generation . • We build a diagnosis - oriented entity graph composed of systems , organs , diseases , and symptoms and apply the graph to enhance and interpret the diagnostic process in conversations . • Experimental results on two medical datasets show the effectiveness and interpretability of our IADDx . Related Work Medical dialogue systems ( MDS ) strive to offer healthcare services to patients . Initial research concentrated on au- tomated diagnosis through task - oriented dialogue systems , emphasizing the swift identification of latent symptoms and providing a final diagnosis ( Liao et al . 2020 ; Lin et al . 2019 ; Chen et al . 2022a ; Liu et al . 2022a ) . The work of Wei et al . ( 2018 ) introduced a dataset marked with symptom annota- tions and developed a medical dialogue system using rein- forcement learning . Xu et al . ( 2019 ) integrated a medical knowledge graph into MDS to manage"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 3,
    "text": "al . 2022a ) . The work of Wei et al . ( 2018 ) introduced a dataset marked with symptom annota- tions and developed a medical dialogue system using rein- forcement learning . Xu et al . ( 2019 ) integrated a medical knowledge graph into MDS to manage the order of inquired symptoms . Tchango et al . ( 2022 ) further improves system reliability by outputting a differential diagnosis , using the exploration - confirmation method , and prioritizing serious diseases . However , these systems conclude with diagnostic results without providing treatment plans or consultations . The advent of large - scale medical dialogue datasets like MedDialog ( Zeng et al . 2020 ) , MedDG ( Liu et al . 2022b ) , and KaMed ( Li et al . 2021 ) and pre - trained language mod- els ( Lewis et al . 2020 ; Radford et al . 2019 ) have amplified interest in medical dialogue generation with multiple ser- vices . The study by Liu et al . ( 2022b ) approached medical dialogue generation by focusing on entity prediction cou- pled with entity - centric response generation . Moreover , Liu et al . ( 2021 ) enhanced dialogue understanding and entity reasoning using a unified heterogeneous graph . Similarly , Zhao et al . ( 2022 ) construct a dialogue graph to leverage medical relationships implied in the context . Li et al . ( 2021 ) treated medical entities within both patient and doctor utter- ances as states and actions , introducing a semi - supervised variation reasoning system complemented by a patient state tracker and a physician action network . Xu et al . ( 2023 ) pro- posed a dual flow ( i.e. , dialogue act and entity flows ) model- ing method to improve dialogue understanding and use acts and entities to guide response generation . Lin et al . ( 2021 ) explored to transfer the diagnostic experience from rich- resource diseases to low - resource ones . Although previous studies on medical dialogue genera- tion have attempted to enhance dialogue understanding and guide response generation by incorporating predicted dia- logue acts and medical entities , they ignore modeling the diagnostic process , which provides interpretation for gen- erated responses . Besides , few works focus on differential diagnosis and apply it to instruct response generation . Our framework aims to model the process of differential diagno- sis and generate responses with diagnosis hints . Preliminary Problem Formulation A medical dialogue is denoted as U={(U P k , U D k ) } T k=1 , where utterances from patients and doctors are repre- sented by U P and U D respectively . Each dialogue is an- notated with several possible diseases D={di}nd i=1 , and each doctor utterance is annotated with multiple dialogue acts A={ai}na i=1 . Given the historical dialogue sequence Ut={U P 1 , U D 1 , ... , U P"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 4,
    "text": "by U P and U D respectively . Each dialogue is an- notated with several possible diseases D={di}nd i=1 , and each doctor utterance is annotated with multiple dialogue acts A={ai}na i=1 . Given the historical dialogue sequence Ut={U P 1 , U D 1 , ... , U P t } , the objective is to produce the t - th doctor utterance U D t . SOAP - based Content Structuring In each round of a medical dialogue ( U D , U P ) , we first ex- tract medical - related segments in accordance with the SOAP Notes ( Krishna et al . 2021 ) , i.e. , Subjective personal reports { SS i } , Objective quantifiable data { SO i } , an Assessment ( or diagnosis results ) { SA i } , and subsequent Plans { SP i } for pa- tient care . SOAP notes serve as a documentation method used by medical specialists to structure patient information . For example , a subjective segment can be “ vomited three times ” . The extraction filters out irrelevant details , and seg- ments from the subjective and objective sections are instru- mental for diagnosis . We employ pre - trained large language models ( LLMs ) , such as GPT-4 ( OpenAI 2023 ) , to extract \n Dialogue History LLM Stage 1 : Intuitive Association Stage 2 : Analytic Refinement System Organ Disease Symptom Cross Attention SOAP Encoder Refined Diagnosis … Q K 𝑑 ! , 𝑑 \" , 𝑑 # V Differential Diagnosis Response Generation … Retrieve disease documents Retrieve patient cases Retrieved Cases Retrieved Documents Patient X suffer … Cholecystitis is a … Preliminary Diagnosis 𝑑$ , 𝑑 ! , … , 𝑑% Dialogue History Dialogue Act Predictor Response Disease - guided Retriever Generator 𝑎$ , 𝑎 ! … 𝑎 & Diagnosis - Oriented Graph Dialogue Acts top-𝐾 Passages 𝑓 ' ( ) SOAP Segments SOAP Segments 𝑑!𝑑 \" 𝑑 # ⋯ 𝜎(⋅ ) finement stage . The multi - disease classifier in Stage 2 generates a refined diagnosis to guide response generation . Right : The structure of Response Generation . The diagnosis combined with the dialogue acts are used to retrieve relevant knowledge . segments in a few - shot manner . The input prompts for LLMs contain SOAP instructions and some examples of extracted segments . Details are described in Appendix . Diagnosis - Oriented Graph Construction We construct a Diagnosis - Oriented Graph ( DOG ) G= { ei } inspired by the problem - specific framework used in differen- tial diagnosis ( Stern , Cifu , and Altkorn 2020 ) . This frame- work aids in pinpointing medical issues and inferring a dif- ferential diagnosis . The graph we develop encompasses en- tities such as body systems ( eSys ) , organs ( eOrg ) , diseases ( eDis ) , and symptoms ( eSym ) . Notably , “ System −→Organ −→Disease"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 5,
    "text": "work aids in pinpointing medical issues and inferring a dif- ferential diagnosis . The graph we develop encompasses en- tities such as body systems ( eSys ) , organs ( eOrg ) , diseases ( eDis ) , and symptoms ( eSym ) . Notably , “ System −→Organ −→Disease −→Symptom ” can be a diagnostic path . In de- tail , systems and organs serve to categorize disease types and help understand the potential impact , while symptoms provide the basis for confirming or ruling out a particular disease . Based on findings from anatomic studies ( Tortora and Derrickson 2018 ) , we incorporate major body systems , such as the digestive and endocrine systems , and associated organs like the stomach and thyroid . Besides , we select dis- eases and symptoms from an online medical encyclopedia website1 that is edited and reviewed by medical specialists . The relations between body systems and their respective or- gans , as well as between diseases and symptoms , are inher- ent . To establish connections between diseases and the or- gans they affect , we utilize pre - trained LLMs to associate diseases with specific organs based on their pathological manifestations . Method Our proposed IADDx framework comprises two main com- ponents , as illustrated in Figure 2 . The Intuitive - then- 1https://www.baikemy.com/ Analytic Differential Diagnosis component makes a differ- ential diagnosis through intuitive and analytic reasoning . Subsequently , the Diagnosis - guided Response Generation component utilizes diagnosis results to guide knowledge re- trieval and generate an appropriate response with the re- trieved knowledge . Ituitive - then - Analytic Differential Diagnosis We model the differential diagnosis process in two stages : intuitive association and analytic refinement . The first stage draws upon clinical experience to make a rough di- agnosis . We use patient case and disease document retrieval to generate a preliminary list of diseases . The disease doc- ument retriever assists in identifying diseases that have not been previously encountered . Then , the second stage further refines the diagnosis through a more detailed problem analy- sis , enhancing both diagnostic accuracy and interpretability . We construct a multi - disease classifier with the aid of our diagnosis - oriented graph for this stage . Stage 1 : Intuitive Association . The objective of this stage is to make a rough diagnosis . Given the dialogue history Ut , we extract SOAP segments { SS i } , { SO i } , { SA i } , and { SP i } using a pre - trained LLM and use them to retrieve knowl- edge for disease list generation . These segments contain in- formation ( i.e. , patient symptoms , signs , and medical his- tory ) needed for differential diagnosis . We concatenate the { SS i } , { SO i } , and { SA i } segments as the query for subse- quent retrieval . Here , the"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 6,
    "text": "in- formation ( i.e. , patient symptoms , signs , and medical his- tory ) needed for differential diagnosis . We concatenate the { SS i } , { SO i } , and { SA i } segments as the query for subse- quent retrieval . Here , the plan segments are not considered since their content is presented after diagnosis . We use the aforementioned query to retrieve two types of knowledge : ( 1 ) patient cases ( in the format of SOAP ) from the training corpus exhibiting conditions similar to those of the current patient and ( 2 ) disease documents ( i.e. , etiology and symp- toms ) from a disease corpus with descriptions that align with \n the current patient ’s conditions . The query and patient case ( or disease document ) sequences with a “ [ CLS ] ” token in- serted at the front are separately input to a BERT encoder ( Devlin et al . 2019 ) . We select hidden states of “ [ CLS ] ” to- kens for each sequence as their representations S ∈Rd and SCase∈Rd ( or SDoc∈Rd ) and calculate relevance scores as : sCase = ⟨S , SCase⟩ , sDoc = ⟨S , SDoc⟩ , ( 1 ) where ⟨ , ⟩represents a similarity function . After retrieving patient cases , each case is assigned a relevance score , which is then applied to all diseases associated with that case . Thus , each disease can be assigned a group of scores { sCase j } j∈Ci from different patient cases , where Ci denotes cases diag- nosed with disease i. We select the maximum as the rele- vance score for each disease sCase∗ i . After retrieving disease documents , each disease can also be directly assigned a rel- evance score sDoc i . We average these two scores as the final disease relevance score , si=(sCase∗ i + sDoc i ) /2 , and select top- K diseases to generate a preliminary disease list { di}K i=1 . We adopt Contrastive Learning ( Chen et al . 2020 ) to op- timize these two retrievers . The loss function for the patient case retriever is defined as : LCase = −log exp(⟨S , SCase+ t ⟩ ) P SCase− t ∈B exp(⟨S , SCase− t ⟩ ) , ( 2 ) where SCase+ t denotes representations from positive cases that share at least one diagnosed disease with the current di- alogue , and SCase− t denotes those from negative cases that do not coincide with diseases discussed in the current dialogue . We use negative cases from input batches B for training . The loss function LDoc for the disease document retriever is de- fined in the same way . Stage 2 : Analytic Refinement . This stage further refines the diagnosis via specific problem analysis , improving diag- nostic accuracy and interpretability . We construct a multi- disease classifier"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 7,
    "text": "for training . The loss function LDoc for the disease document retriever is de- fined in the same way . Stage 2 : Analytic Refinement . This stage further refines the diagnosis via specific problem analysis , improving diag- nostic accuracy and interpretability . We construct a multi- disease classifier inspired by the ConceptTransformer ( Rig- otti et al . 2022 ) model for this stage , which leverages domain knowledge to improve classification accuracy and provide concept - based interpretations . The diagnosis - oriented graph is employed to augment and interpret the classification . As depicted in the left of Figure 2 , the inputs of the clas- sifier are SOAP segments extracted in the first stage and the diagnosis - oriented graph . Specifically , SOAP segments are encoded into several representations { Si}ns i=1 through a BERT encoder , and each one is obtained by averaging hid- den states of tokens corresponding to one segment . ns is the number of no duplicate segments until the current turn . We incorporate entities that are involved in the diagnostic path of diseases identified in the preliminary diagnosis and obtain the sub - graph Gt . For all entities in the sub - graph , we use the same encoder as SOAP segments to get entity embeddings . The average token embedding of each entity is utilized as the raw embedding , denoted as e0 ∈Rd . We employ Graph Attention Network ( GAT ) ( Velickovic et al . 2018 ) to merge neighboring information for each entity : αk ij = exp \u0000σ1 \u0000aT[Wke0 i ∥Wke0 j ] \u0001\u0001 P µ∈Ni exp \u0000σ1 \u0000aT[Wke0 i ∥Wke0µ ] \u0001\u0001 , ( 3 ) ei =  σ2  X j∈Ni αk ijWke0 j     h k=1 , ( 4 ) where ei ∈Rd represents the updated embedding , a ∈R2d and Wk ∈Rdh×d are learnable parameters , σ1 and σ2 de- note activation function , Ni is a set of neighboring entities that connect to entity i , and h is the number of heads . The multi - disease classifier incorporates a cross - attention layer to facilitate interaction between segment representa- tions and entity embeddings . Each segment representation integrates relevant entity information from the graph , and this enriched representation is then transformed via linear mapping to estimate probabilities for multiple diseases . The attention matrix A and the probability pd j for each disease are calculated as follows : A = softmax(QKT √ d ) , ( 5 ) pd j = sigmoid ( ns X i=1 [ AVO]ij ) , j = 1 , . . . , n , ( 6 ) where K ∈RnGt×d and V ∈RnGt×d are the linear pro- jected matrix based on the concatenation of entity embed- dings { ei}i∈Gt , and Q ∈Rns×d is based on the segment representations { Si}ns i=1 . O ∈Rd×n denotes an output pro- jection matrix , and"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 8,
    "text": ", ( 6 ) where K ∈RnGt×d and V ∈RnGt×d are the linear pro- jected matrix based on the concatenation of entity embed- dings { ei}i∈Gt , and Q ∈Rns×d is based on the segment representations { Si}ns i=1 . O ∈Rd×n denotes an output pro- jection matrix , and n is the number of total diseases in our corpus . We select the probabilities of diseases within the pre- liminary list and employ a proper threshold to predict multi- ple diseases ( See in Experiments ) as the refined differential diagnosis { d′i}nd i=1 . We optimize the multi - disease prediction by minimizing a binary cross - entropy loss . The loss function Ld is calculated as follows : Ld = −1 K X [ yd j · log pd j + ( 1 −yd j ) · log(1 −pd j ) ] , ( 7 ) where yd j is the label of j - th disease , and K denotes the number of diseases in the preliminary list . We optimize the disease probabilities within each list , which vary across di- alogues . Besides , we add an explanation loss ( Rigotti et al . 2022 ) to supervise the attention weights . This loss can guide the attention heads to attend to entities that are beneficial for disease classification . The loss Lexpl is defined as : Lexpl = ∥A −A′∥2 F , ( 8) where ∥ · ∥F denotes the Frobenius norm , A′ is the de- sired distribution of attention . The final loss used for training multi - disease classification is defined as follows : L = αLd + βLexpl , ( 9 ) where α and β are weights for balancing these two losses . \n Diagnosis - guided Response Generation In this section , we utilize the refined diagnosis to guide med- ical knowledge retrieval , thereby enhancing response gener- ation with the aid of retrieved disease - related knowledge . The diagnostic results help to select knowledge more ac- curately and guide the dialogue around related diseases . A doctor dialogue act predictor is introduced since acts help to select a specific aspect of disease knowledge ( e.g. , clin- ical manifestations or examinations ) and guide the flow of the dialogue ( Xu et al . 2023 ) . The detailed architecture is shown on the right of Figure 2 . Dialogue Act Prediction . We develop a multi - act predic- tor to assist knowledge retrieval and manage the dialogue . The input of the predictor is SOAP segments and dialogue history . We concatenate “ [ CLS ] ” token at the front of the segment sequence and encode this sequence using a BERT encoder , which is also applied to the dialogue history . The final hidden states of “ [ CLS ] ” token in these two encodings are selected as the representation of the segments Hs t ∈Rd and dialogue history Ht"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 9,
    "text": "segment sequence and encode this sequence using a BERT encoder , which is also applied to the dialogue history . The final hidden states of “ [ CLS ] ” token in these two encodings are selected as the representation of the segments Hs t ∈Rd and dialogue history Ht ∈Rd . We merge the structured pa- tient information and dialogue content to predict dialogue acts . The act probability is calculated as follows : pa i = sigmoid(Wa[Hs t ; Ht ] ) , i = 1 , . . . , m , ( 10 ) where Wa ∈Rm×2d is a trainable parameter matrix , [ ; ] de- notes a concatenation operation , and m is the number of can- didate acts . The predicted dialogue acts { ai}na i=1 are selected through an appropriate threshold ( See in Experiments ) . We apply a binary cross - entropy loss to optimize the multi - act prediction . The loss function is denoted as follows : La = −1 m X [ ya i · log pa i + ( 1 −ya i ) · log(1 −pa i ) ] , ( 11 ) where ya i is the label of i - th dialogue act . Disease - guided Retrieval . To augment response gener- ation , we retrieve disease - related passages based on the refined differential diagnosis and predicted dialogue acts . These passages are from the medical encyclopedia web- site , providing external knowledge for response generation . For dialogue acts directly related to a specific aspect of medicine , we choose corresponding passages without re- quiring retrieval . For example , responses with the act “ In- quire about present illness ” are closely related to the pas- sage describing clinical manifestations of one disease . For non - medical dialogue acts , we use the current dialogue his- tory to retrieve relevant passages from a disease corpus . We remove stop words and punctuation from the dialogue his- tory sequence and retrieve top - k passages through the BM25 algorithm ( Sch¨utze , Manning , and Raghavan 2008 ) . Response Generation . After retrieving medical knowl- edge and predicting dialogue acts , we incorporate these two pieces of information to guide response generation . We con- struct a generation model following the Fusion - in - Decoder ( FiD ) method ( Izacard and Grave 2021 ; Shuster et al . 2021 ) , which allows the decoder to attend to all encoding represen- tations at the same time when generating a response . The input of the model contains a group of sequences , where the first sequence is the dialogue history sequence Ut con- catenated with unique tokens denoting dialogue acts { ai } , and other sequences are retrieved knowledge sequences Zt . Each sequence is concatenated with a special token ( “ [ U ] ” or “ [ K ] ” ) at the front"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 10,
    "text": "the dialogue history sequence Ut con- catenated with unique tokens denoting dialogue acts { ai } , and other sequences are retrieved knowledge sequences Zt . Each sequence is concatenated with a special token ( “ [ U ] ” or “ [ K ] ” ) at the front to represent the dialogue history or knowledge . Compared with concatenating the dialogue his- tory with knowledge as an input sequence , the above input can leverage longer dialogue history . We train the generation model by a negative log- likelihood loss . The loss function is defined as : Lg = − N X i=1 log p(U D t , i ) , ( 12 ) where U D t , i is the i - th token ’s probability . During training , we use ground truth acts as part of the input and apply ground truth diseases to guide knowledge retrieval . Then in infer- ence , we apply predicted acts and diseases as guidance . Experiments Datasets We perform our experiments using two medical dialogue datasets : MedDG ( Liu et al . 2022b ) and KaMed ( Li et al . 2021 ) . MedDG encompasses 17 K dialogues , primarily cen- tered on 12 diseases within the gastroenterology department . We partition the dataset into 14862 , 1999 , and 999 dialogues for training , validation , and testing , respectively . KaMed of- fers a comprehensive collection of over 63 K dialogues span- ning nearly 100 hospital departments . To address privacy concerns ( See in Appendix ) , we exclude certain dialogues from KaMed , resulting in 29159 , 1532 , and 1539 dialogues for training , validation , and testing , respectively . Baseline Models We evaluate IADDx with six baseline models . Non- Pretrained models : ( 1 ) Seq2Seq ( Sutskever , Vinyals , and Le 2014 ) employs an RNN for sequence - to - sequence gen- eration enhanced by an attention layer . ( 2 ) HRED ( Serban et al . 2016 ) leverages a multi - level RNN design to encode dialogues both at the token and utterance levels . ( 3 ) VRBot ( Li et al . 2021 ) is designed for medical dialogue generation , emphasizing the tracking and predicting of patient and doc- tor entities . Pretrained models : ( 1 ) GPT-2 ( Radford et al . 2019 ) is a transformer decoder - based language model . ( 2 ) BART ( Lewis et al . 2020 ) is a transformer - based encoder- decoder model . ( 3 ) DFMed ( Xu et al . 2023 ) is a medical di- alogue generation model that learns entity and dialogue act flows . For our experiments on the MedDG dataset , we sup- plement models with entity hints as described by Liu et al . ( 2022b ) . This involves appending extracted medical entities to the end of"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 11,
    "text": "a medical di- alogue generation model that learns entity and dialogue act flows . For our experiments on the MedDG dataset , we sup- plement models with entity hints as described by Liu et al . ( 2022b ) . This involves appending extracted medical entities to the end of the input sequence . Evaluation Metrics Automatic Evaluation . BLEU ( Papineni et al . 2002 ) and ROUGE ( Lin 2004 ) scores across varied n - grams ( specifi- cally , B-1 , B-2 , B-4 , R-1 , and R-2 ) are utilized as metrics to evaluate the quality of generated responses . Additionally , \n Methods B-1 B-2 B-4 R-1 R-2 E - P E - R E - F1 w/o Pre - training Seq2Seq 28.55 22.85 15.45 25.61 11.24 16.79 10.44 12.88 Seq2Seq - Entity 29.13 23.22 15.66 25.79 11.42 23.79 15.89 19.06 HRED 31.61 25.22 17.05 24.17 9.79 15.56 10.12 12.26 HRED - Entity 32.84 26.12 17.63 24.26 9.76 21.75 15.33 17.98 VRBot 29.69 23.90 16.34 24.69 11.23 18.67 9.72 12.78 w/ Pre - trained LM GPT-2 35.27 28.19 19.16 28.74 13.61 18.29 14.45 16.14 GPT-2 - Entity 34.56 27.56 18.71 28.78 13.62 21.27 17.10 18.96 BART 34.94 27.99 19.06 29.03 14.40 19.97 14.29 16.66 BART - Entity 34.14 27.19 18.42 28.52 13.67 23.49 16.90 19.66 DFMed 42.56 33.34 22.53 29.31 14.21 22.48 22.84 22.66 IADDx ( Ours ) 43.17† 34.09† 23.33† 29.60† 14.37 21.81 22.90† 22.34 Methods B-1 B-2 B-4 R-1 R-2 Seq2Seq 23.52 18.56 12.13 23.56 8.67 HRED 26.75 21.08 13.91 22.93 7.80 VRBot 30.04 23.76 16.36 18.71 7.28 GPT-2 33.76 26.58 17.82 26.80 10.56 BART 33.62 26.43 17.64 27.91 11.43 DFMed 40.20 30.97 20.76 28.28 11.54 IADDx ( Ours ) 40.98† 31.69† 21.35† 28.31† 11.67† statistically significant differences ( p = 0.05 ) . in alignment with Liu et al . ( 2022b ) , we calculate the preci- sion , recall , and F1 of entities mentioned in the responses , denoted as E - P , E - R , and E - F1 respectively . We evaluate the accuracy of the differential diagnosis using the disease F1 score , denoted as D - F1 . Human Evaluation . We randomly selected 100 cases and engaged three physicians for manual evaluation . The per- formance of our IADDx is compared with various baseline models . Drawing upon previous studies ( Liu et al . 2022b ; Li et al . 2021 ) , we assess the generated responses using three metrics : sentence fluency ( FLU ) , knowledge accuracy ( KC ) , and overall quality ( EQ ) . Each metric is on a 5 - point Likert scale , ranging from 1 ( poorest ) to 5 ( excellent ) . Implementation Details We apply the MedBERT2 , a BERT - base model pre - trained on Chinese medical documents as the backbone for encoders in the differential diagnosis component . The disease corpus used for"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 12,
    "text": "Likert scale , ranging from 1 ( poorest ) to 5 ( excellent ) . Implementation Details We apply the MedBERT2 , a BERT - base model pre - trained on Chinese medical documents as the backbone for encoders in the differential diagnosis component . The disease corpus used for the intuitive association is extracted from an on- line medical encyclopedia website named baikemy3 , which contains medical knowledge certified by specialists . In the intuitive association stage , we selected the top 50 diseases as the preliminary diagnosis results . In the analytic refine- ment stage , we predict multiple diseases from the list . We set the weights for Ld and Lexpl to 1 and 0.5 . The threshold for predicting multiple diseases is set to 0.8 based on the disease F1 scores on the validation dataset . In the response genera- tion component , we retrieve knowledge from the same cor- pus and select the top 5 passages for generating responses . 2https://github.com/trueto/medbert 3https://www.baikemy.com/ Methods FLU KC EQ BART 3.77 1.87 3.12 BART - Entity 3.79 2.05 3.41 DFMed 3.91 2.26 3.59 IADDx ( Ours ) 4.08 2.41 3.83 The dialogue act predictor is also based on the MedBERT . We choose from 10 acts and adopt different thresholds to get the highest act F1 scores for each act . The generator is a pre - trained BART - base model4 with a six - layer encoder and a six - layer decoder . We adopt the AdamW optimizer ( Loshchilov and Hutter 2019 ) to train the above models and implement all experiments on a single RTX 3090 GPU . Fur- ther training details are provided in Appendix . Evaluation of Dialogue Generation Automatic Evaluation . The dialogue generation results for the MedDG dataset are presented in Table 1 , and re- sults for the KaMed dataset can be found in Table 2 . We ob- serve that our IADDx method outperforms baseline models on most evaluation metrics . In detail , on the MedDG dataset , IADDx surpasses the state - of - the - art method DFMed by 0.61 , 0.75 , and 0.8 in B-1 , B-2 , and B-4 metrics , as well as 0.29 and 0.16 in R-1 and R-2 metrics . Additionally , we achieve comparable entity accuracy even without employing entity flow learning like DFMed . The reason is that we use multiple differential diagnoses to retrieve pertinent knowl- edge and subsequently leverage them to enhance response generation . Similar advantages are evident in the experi- mental results on the KaMed dataset . IADDx outperforms DFMed by 0.78 , 0.72 , and 0.59 in B-1 , B-2 , and B-4 met- rics . It indicates that modeling and incorporating differential diagnosis aid in generating coherent , informative , and accu- rate responses . Human Evaluation . evaluation on the MedDG dataset . Our method outperforms baselines in all metrics . This suggests that by explicitly mod- eling the differential"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 13,
    "text": "B-4 met- rics . It indicates that modeling and incorporating differential diagnosis aid in generating coherent , informative , and accu- rate responses . Human Evaluation . evaluation on the MedDG dataset . Our method outperforms baselines in all metrics . This suggests that by explicitly mod- eling the differential diagnosis and using it to guide response 4https://huggingface.co/fnlp/bart-base-chinese \n Datasets Methods D - F1 B-1 B-4 R-2 MedDG IADDx 43.50 43.17 23.33 14.37 w/o DDx - 42.21 22.08 13.78 w/o Analytic 37.01 43.02 22.98 14.09 w/o DOG 42.61 43.13 23.22 14.23 KaMed IADDx 50.23 40.98 21.35 11.67 w/o DDx - 39.03 19.54 10.22 w/o Analytic 40.26 40.74 21.16 11.44 w/o DOG 49.52 40.86 21.22 11.52 generation , we can produce more informative and accurate responses . The Fleiss ’ kappa ( Fleiss 1971 ) score is 0.49 , in- dicating a moderate level of inter - annotator agreement . Analysis of Intuitive - then - Analytic Differential Diagnosis To delve deeper into the efficacy of our approach , we ex- amine multiple variants of our IADDx method as follows : ( 1 ) w/o DDx , which removes the entire differential diagno- sis component and generates responses conditioned solely on the dialogue history and dialogue acts . ( 2 ) w/o Ana- lytic , which removes the analytic refinement on the prelim- inary diagnosis and employs the top 5 ranked diseases to guide knowledge retrieval . ( 3 ) w/o DOG , which removes the diagnosis - oriented graph in the multi - disease classifier and solely adopts the mean representations of SOAP segments and a subsequent linear layer to classify diseases . reduction in performance across all metrics with the ab- lation variants , emphasizing the critical role of each mod- ule in our proposed method . Among these variants , the re- sponse quality of w/o DDx notably decreases due to the ab- sence of diagnosis - related knowledge . Such knowledge is crucial for providing essential disease information that en- hances the informativeness and accuracy of the response . Besides , the differential diagnosis performance of w/o Ana- lytic significantly drops compared with the complete DDx . It is because the retrieval - based intuitive association provides a fixed number of potential diseases and can not ensure that all relevant diseases rank high , inadvertently introducing unre- lated diseases . The results of w/o DOG demonstrate that the diagnosis - oriented graph plays a role in the differential diag- nosis and subsequent diagnosis - guided response generation . Case Study We present a case study of medical dialogue generation by different models in Table 5 . We observe that the response generated by our IADDx method offers more accurate di- agnoses , along with treatment plans tailored to those diag- noses , compared to responses from the three baseline mod- els . The prescribed medicine , Rabeprazole , Mosapride , and Magnesium aluminum carbonate , are suitable for treating the diagnosed disease GERD . This suggests that"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 14,
    "text": "more accurate di- agnoses , along with treatment plans tailored to those diag- noses , compared to responses from the three baseline mod- els . The prescribed medicine , Rabeprazole , Mosapride , and Magnesium aluminum carbonate , are suitable for treating the diagnosed disease GERD . This suggests that IADDx ef- fectively performs a reasonable differential diagnosis and leverages it to enhance response generation . Besides , al- though four diseases are included in the differential diag- Patient : Hello , doctor ! Over the past week , I ’ve been experienc- ing occasional subtle pain in the center of my upper abdomen , and sometimes this pain radiates to my back . Last night , I felt a tightness in my chest and a heavy sensation in my throat , almost like acid reflux . What could be the issue ? Doctor : Hello , are you currently experiencing acid reflux or heartburn ? Do you burp often ? Patient : I have n’t had noticeable acid reflux , but there ’s a slight burning sensation . I started burping since yesterday . Af- ter taking medicine today , the burping has become less frequent . Sometimes , there ’s also a subtle burning feeling in my back . ... Patient : I did n’t sleep well last night since I felt discomfort in my upper abdomen , and I woke up several times . Gold Response : It ’s likely that you have gastroesophageal reflux disease ( GERD ) . I ’d recommend trying Mosapride . For now , avoid greasy , fried , and spicy foods . BART : I suspect it ’s gastroesophageal reflux disease ( GERD ) . BART - Entity : Yes , it seems to be caused by an excessive se- cretion of stomach acid . DFMed : I believe you might have weak gastrointestinal motility and poor digestive function . I recommend taking Omeprazole , Domperidone , and Mosapride for treatment . Stick to a bland diet and avoid spicy , irritating , and fried foods . Differential Diagnosis : ( 1 ) reflux esophagitis , ( 2 ) gastro -esophageal reflux , ( 3 ) bile reflux gastritis , ( 4 ) duodeno -gastric reflux Attended Entities in DOG : [ digestive system ] −→[stomach ] −→[reflux esophagitis / gastroesophageal reflux ] −→[upper ab- dominal pain / poststernal burning sensation / acid reflux ] IADDx ( Ours ): I suspect you have gastroesophageal reflux disease ( GERD ) . I recommend taking Rabeprazole , Mosapr- ide , and Magnesium aluminum carbonate for treatment . Over the next few days , be mindful of your diet . Focus on easily di- gestible foods , and try to avoid acidic , spicy , or cold foods . ent models , where diseases are underlined . nosis , IADDx can implicitly select the most likely disease to generate a response . Interpretation of differential diagnosis . Our IADDx of- fers a comprehensive list"
  },
  {
    "source": "2401.06541v1.pdf",
    "chunk_index": 15,
    "text": "gestible foods , and try to avoid acidic , spicy , or cold foods . ent models , where diseases are underlined . nosis , IADDx can implicitly select the most likely disease to generate a response . Interpretation of differential diagnosis . Our IADDx of- fers a comprehensive list of differential diagnoses that can explain the generated response . In this dialogue , IADDx identifies four potential diseases that exhibit similar symp- toms . The first and second diseases are both related to GERD and have been utilized to generate the response . Beyond these two , physicians still need to consider the potential of the other diseases and seek to confirm or rule them out in subsequent conversations . IADDx also provides a diagno- sis path composed of systems , organs , diseases , and symp- toms to interpret the differential diagnosis . We select enti- ties with high attention weights to build the diagnosis path . We observe that the patient ’s condition predominantly per- tains to the digestive system and the stomach . Additionally , the attention to specific symptom entities lends further sup- port to the potential diseases : reflux esophagitis and gastroe- sophageal reflux . \n Conclusion In this work , we propose a medical dialogue generation framework with a differential diagnosis , IADDx , which ex- plicitly models the process of differential diagnosis through intuitive association followed by analytic refinement . More- over , we devise a diagnosis - oriented graph to interpret the differential diagnosis . The diagnosis results are utilized to guide medical knowledge retrieval and response generation . Experiments on two datasets demonstrate the efficacy of the proposed framework . Additionally , we illustrate how our framework aids clinicians and patients in understanding the diagnostic procedure , such as by generating intermediate outcomes and graph - based diagnostic paths ."
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 0,
    "text": "Dynamic Traceback Learning for Medical Report Generation Shuchang Ye School of Computer Science The University of Sydney Sydney , New South Wales , Australia shuchang.ye@sydney.edu.au Mingyuan Meng School of Computer Science The University of Sydney Sydney , New South Wales , Australia mmen2292@uni.sydney.edu.au Mingjian Li School of Computer Science The University of Sydney Sydney , New South Wales , Australia mili3287@uni.sydney.edu.au Dagan Feng School of Computer Science The University of Sydney Sydney , New South Wales , Australia dagan.feng@sydney.edu.au Usman Naseem School of Computing Macquarie University Sydney , New South Wales , Australia usman.naseem@mq.edu.au Jinman Kim School of Computer Science The University of Sydney Sydney , New South Wales , Australia jinman.kim@sydney.edu.au Abstract Automated medical report generation has the poten- tial to significantly reduce the workload associated with the time - consuming process of medical reporting . Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation . However , when trained end- to - end and applied directly to medical image - to - text gen- eration , they face two significant challenges : i ) difficulty in accurately capturing subtle yet crucial pathological de- tails , and ii ) reliance on both visual and textual inputs dur- ing inference , leading to performance degradation in zero- shot inference when only images are available . To address these challenges , this study proposes a novel multi - modal dynamic traceback learning framework ( DTrace)1 . Specif- ically , we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of im- age and text input , enabling text generation without strong reliance on the input from both modalities during infer- ence . The learning of cross - modal knowledge is enhanced by supervising the model to recover masked semantic infor- mation from a complementary counterpart . Extensive ex- 1Our code is provided in supplementary materials and will be publicly available on GitHub upon publication . periments conducted on two benchmark datasets , IU - Xray and MIMIC - CXR , demonstrate that the proposed DTrace framework outperforms state - of - the - art methods for medi- cal report generation . 1 . Introduction Medical report generation is a crucial component of the diagnostic process , providing detailed textual descriptions of medical images , which guide clinical decision - making and treatment planning [ 19 ] . However , the task of interpret- ing medical images and composing reports is both time- consuming and resource - intensive . Therefore , automating the report generation process has garnered significant at- tention as a potential solution to reduce radiologist work- load [ 39 ] . Despite advances in deep learning , medical re- port generation remains challenging due to the difficulty in accurately associating subtle yet critical diagnostic features in medical images with their corresponding textual reports . This is because medical images often contain subtle regions , such as tumor"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 1,
    "text": "[ 39 ] . Despite advances in deep learning , medical re- port generation remains challenging due to the difficulty in accurately associating subtle yet critical diagnostic features in medical images with their corresponding textual reports . This is because medical images often contain subtle regions , such as tumor areas , which are essential for diagnosis , while medical reports rely on a limited set of keywords to rep- resent this diagnostic information . Hence , it is crucial to develop a method that can capture and associate this nu- anced information to preserve the intrinsic medical mean- ings within medical images and reports . arXiv:2401.13267v3 [ cs . CV ] 7 Sep 2024 \n Visual Encoder Input Image Linguistic Decoder Gen. Text a ) Encoder - Decoder Architecture b ) Multi - modal Autoencoder c ) Proposed Framework Muti - modal Encoder Masked Image Masked Text Muti - modal Decoder Gen. Image Gen. Text Fixed Fixed Visual Decoder Linguistic Decoder Visual Encoder Linguistic Encoder Multi - modal Fusion Masked Image Masked Text Gen. Image Gen. Text Dynamic Dynamic Illustration of different generative frameworks . ( a ) Common uni - modal encoder - decoder framework , ( b ) Multi - modal masked encoder - decoder framework for generative representation learning ( GRL ) , and ( c ) Our proposed framework with dynamic traceback learning ( DTrace ) . Heart size is enlarged Ignored Heart size is enlarged Spurious normal enlarged a ) b ) tion framework . a ) The model make prediction based on spurious generation statistics , overlooking the radiology images ; b ) Ideally , the model should understand the pathological information in the image and generate a report accordingly . Existing medical report generation methods usually rely on an encoder - decoder framework ( Fig . 1a ) that performs uni - modal learning to build a uni - directional mapping from images to reports [ 4,29,34,38 ] . However , such a framework is limited in associating intrinsic medical meanings as it ig- nores the bi - directional mutual associations between images and reports . Models tend to adopt a “ lazy ” approach that re- lies on generative statistics ( the distribution of each class ) as a shortcut for report generation rather than learning cross- modal knowledge to perform diagnosis [ 33 ] , as depicted in Fig . 2 . Modeling of multi - modal information to build mutual associations is common in neural learning systems , and it has been demonstrated to facilitate understanding of cross - modal knowledge [ 18 ] . Recently , Generative Repre- sentation Learning ( GRL ) methods have exploited multi- modal learning and generation [ 6 , 8 , 9 ] , where image and report reconstruction are jointly performed in a multi - modal masked encoder - decoder framework ( Fig . 1b ) . These meth- ods aim to learn the latent space representations by masking inputs and subsequently"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 2,
    "text": "learning and generation [ 6 , 8 , 9 ] , where image and report reconstruction are jointly performed in a multi - modal masked encoder - decoder framework ( Fig . 1b ) . These meth- ods aim to learn the latent space representations by masking inputs and subsequently reconstructing the original inputs based on the unmasked information . Although these meth- ods have brought significant advancements in multi - modal learning , they exhibit considerable gaps when adopted for medical report generation due to two primary limitations : First , GRL methods prioritize capturing morphological information ( e.g. , organ shape and report structure ) , yet they struggle with subtle pathological semantics like lesion loca- tion and disease spread [ 14 ] . In image reconstruction , these methods often reduce the task to simple pixel matching by minimizing pixel - level discrepancies , neglecting the seman- tic and pathological nuances of images . For report recon- struction , GRL methods tend to predict frequently observed words to achieve a high overlap rate between the original and reconstructed reports . Due to the word imbalance in medical reports , where keywords signifying pathology ap- pear infrequently , this learning approach potentially leads to the generation of clinically - flawed templated reports . Fur- ther , there exists inherent variability in the report descrip- tions for the same medical images , such as the order of re- porting ( e.g. , starting from the disease sites or from image acquisition protocol ) and the lexical choices employed to convey the severity of symptoms . However , the loss func- tions of GRL methods typically operate at the word level , which can not measure the quality of generated reports from the sentence level with semantic contextual information . Second , GRL methods focus on predicting masked im- ages and text from their unmasked counterparts , which usu- ally require a large amount of unmasked information to achieve text reconstruction [ 6 , 8 ] , with the mask ratio for the text being limited to a low range , ≈15 % ( most textual information retained as the input ) . Whereas , the inference is based solely on images in medical report generation . Such a shift can lead to a performance drop ( see Section 5.3 ) , as GRL methods are not designed to handle inference from images alone without accompanying text . To address the above limitations , in this study , we pro- pose a novel report generation framework to overcome the limitations of GRL methods and introduce dynamic trace- back learning for medical report generation ( DTrace ) . Dur- ing inference , images are fully visible ( 0 % mask ratio ) , and text is completely masked ( 100 % mask ratio ) . During train- ing , image and text mask ratios fluctuate between 0 % and 100 % per batch . We demonstrate that this variable mask- ing"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 3,
    "text": ", images are fully visible ( 0 % mask ratio ) , and text is completely masked ( 100 % mask ratio ) . During train- ing , image and text mask ratios fluctuate between 0 % and 100 % per batch . We demonstrate that this variable mask- ing approach is the most effective way of learning ( see Sec- tion 5.3 ) . Our contributions are as follows : • We introduce DTrace , a multi - modal framework for medical report generation that jointly learns bi- directional image - to - report and report - to - image gener- ation , enhancing cross - modal knowledge by recover- ing masked semantic information . • We introduce a traceback mechanism in DTrace that ensures semantic validity by reintegrating gener- ated images and reports into their encoders for self- assessment . • We introduce a dynamic learning strategy in DTrace that adapts to any image - text ratio , enabling effective training with both modalities and supporting image- only inference by adjusting loss weights dynamically . • Extensive experiments on two benchmark datasets ( IU - Xray and MIMIC - CXR ) show that the proposed DTrace outperforms state - of - the - art medical report generation methods . \n 2 . Related Work 2.1 . Medical Report Generation Traditional medical report generation methods rely on rule- or template - based methods [ 1 ] . Rule - based methods often fall short in handling different scenarios and captur- ing language subtleties , while template - based methods are dependent on template quality and adaptability . With the paradigm shift in Computer Vision ( CV ) and Natural Lan- guage Processing ( NLP ) , deep learning - based medical re- port generation methods have achieved promising perfor- mance and attained wide attention [ 19 ] . Deep learning - based report generation can be traced back to the invention of encoder - decoder architecture . Within this framework , images were transmuted into repre- sentative vectors encapsulating salient information through a visual encoder , followed by a linguistic decoder to pre- dict text [ 31 ] . Subsequent studies primarily focused on enhancing the capabilities of the visual encoder and the linguistic decoder [ 25 ] , e.g. , from Convolutional Neural Networks ( CNN ) and Recurrent Neural Networks ( RNN ) to Vision Transformer ( ViT ) and Transformer [ 34 ] . Re- cently , the interaction and communication between the vi- sual encoder and linguistic decoder have also attracted wide attention . Visual language pretraining ( VLP ) has significantly advanced image - to - text generation tasks by effectively integrating visual and textual data . For in- stance , MCGN [ 35 ] employs CLIP [ 23 ] to harness con- trastive learning for refining the latent space , thus boost- ing semantic similarity between image - text pairs . How- ever , VLP approaches predominantly focus on training en- coders ,"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 4,
    "text": "textual data . For in- stance , MCGN [ 35 ] employs CLIP [ 23 ] to harness con- trastive learning for refining the latent space , thus boost- ing semantic similarity between image - text pairs . How- ever , VLP approaches predominantly focus on training en- coders , which , for tasks like report generation where de- coders are equally crucial , necessitates the incorporation of additional methodologies to train both network components effectively . In medical report generation , R2GenCMN [ 3 ] proposed to unify the visual and linguistic representations by sharing a vector pool across the visual encoder and linguistic decoder . XProNet [ 32 ] further improved the R2GenCMN by monitoring the vector pool with patholog- ical labels . These methods improved cross - modal commu- nication and achieved state - of - the - art medical report gen- eration performance . However , they still relied on a uni- directional image - to - report mapping and ignored the bi- directional mutual image - report associations . Recent stud- ies have introduced additional annotations to strengthen the relationship between images and their corresponding re- ports . COMG [ 28 ] incorporates segmentation masks of or- gans to guide the model ’s attention toward critical regions . Similarly , RGRG [ 26 ] integrates bounding boxes and links them to their associated sentences , enabling the model to fo- cus more effectively on specific anatomical regions during report generation . 2.2 . Generative Representation Learning GRL methods learn the latent space representations by training the model to reconstruct the masked inputs based on the unmasked information . Masked Image Modeling ( MAE ) [ 9 ] and Masked Language Modeling ( BERT ) [ 6 ] are prevalent pre - training techniques in CV and NLP . BERT learned the word latent representations by training the model to predict the masked content based on the sur- rounding words . Then , MAE employed a similar strat- egy in images , where the images were split into patches and the model was trained to reconstruct the randomly masked patches . Recently , M3AE [ 8 ] exploited multi- modal learning and generation , where image and text re- construction were jointly performed with a multi - modal masked encoder - decoder framework to enhance the com- prehension of cross - modal associations . These GRL meth- ods were widely adopted as a pre - training step to enhance the performance of downstream tasks , such as disease clas- sification [ 36 ] and medical visual question answering [ 2 ] . The capability of reconstructing masked text provides the potential for report generation . However , GRL meth- ods were seldom applied to medical report generation due to the two drawbacks that we identified above . Recently , MedViLL [ 20 ] applied GRL framework to medical report generation ( as one of the downstream tasks ) by progres- sively replacing mask tokens with predicted"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 5,
    "text": ", GRL meth- ods were seldom applied to medical report generation due to the two drawbacks that we identified above . Recently , MedViLL [ 20 ] applied GRL framework to medical report generation ( as one of the downstream tasks ) by progres- sively replacing mask tokens with predicted language to- kens . Unfortunately , its performance was limited ( 0.066 in BLEU4 in the MIMIC - CXR dataset ) as it was not trained to handle situations where text information is not available . In contrast , our proposed framework differs in several key aspects : i ) GRL methods are typically designed for pre - training on large datasets followed by fine - tuning for specific tasks , whereas our framework is trained end - to - end and can be directly applied to medical report generation ; ii ) GRL usually employs a fixed mask ratio , but we utilize a dy- namic mask ratio ; and iii ) while conventional methods pri- oritize morphological similarity , our approach emphasizes semantic similarity . 3 . Method 3.1 . Network Architecture DTrace consists of five key components : a visual en- coder , a visual decoder , a linguistic encoder , a linguistic de- coder , and a cross - modal fusion module ( Fig . 3 ) . The visual encoder processes partially masked images to extract in- complete pathological information . Simultaneously , the lin- guistic encoder also processes fragmented textual informa- tion to extract incomplete pathological information . Then , the extracted information is fed into the cross - modal fusion module , where a cross - attention mechanism is employed to foster the interaction between the visual and linguistic do- mains , thereby ameliorating the semantic deficits in both \n cardiac contour normal . cardiac contour normal . Visual Encoder cls MLP Linguistic Encoder cls MLP Cross - modal Prototype Q&R Cross - modal Attention Cross - modal Fusion Linguistic Decoder token Visual Decoder cardiac contour normal . The is modalities . After this , the enriched information is conveyed to the visual and linguistic decoders to restore the masked images and reports to their original unmasked states . Below , we discuss the key components of DTrace in detail : Visual Encoder is consistent with the common prac- tice of MAE ; medical images were split into patches and then randomly masked . The visual encoder was a standard ViT [ 7 ] , which mapped the unmasked patches into latent representations and then performed multi - label classification to predict the disease labels extracted via CheXbert [ 24 ] . In Linguistic Encoder , medical reports were mapped into embedded text tokens following the stan- dard pre - processing steps [ 6 ] . Then , these tokens were ran- domly masked and fed into the linguistic encoder . The lin- guistic encoder was a classic text transformer block [ 29 ] . For the Cross - Modal Fusion Module ,"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 6,
    "text": "tokens following the stan- dard pre - processing steps [ 6 ] . Then , these tokens were ran- domly masked and fed into the linguistic encoder . The lin- guistic encoder was a classic text transformer block [ 29 ] . For the Cross - Modal Fusion Module , the features ex- tracted from both the encoders were projected to a pre- defined dimension , which then was then concatenated and fed to a cross - modal attention module [ 8 ] for information interchange . The resultant features were subsequently sepa- rated and mapped into latent representations via Multi - layer Perceptrons ( MLPs ) [ 27 ] . In Visual Decoder , the masked tokens were reinstated to their original positions , aligning with the unmasked encoding tokens . Following this , a lite version of ViT [ 9 ] was used to restore the masked patches . For the Linguistic Decoder , We adopted a relational mem- ory Transformer [ 4 ] to perform report generation . Taking previously generated text as the value , the decoder treated the concatenation of image patches and text patches as the query and key of the self - attention module to predict the next subsequent word . 3.2 . Traceback Mechanism The traceback mechanism ( Fig . 4 ) was developed to en- sure the medical validity of generated outputs through two phases : forward and traceback . Initially , encoder capabili- ties are enhanced to identify pathological information dur- ing the forward phase . Subsequently , the decoder outputs are redirected to their corresponding encoders in the track- back stage to check their medical validity . Through repet- itive iterations between the two stages , the DTrace model enhances accuracy by refining its understanding of medi- cal content , ensuring reliable identification and validation of pathological information in image reconstruction and re- port generation . Forward Stage Traceback Stage and traceback stage ( right ) . The lock means that there is no gradi- ent descent in back - propagation . 3.3 . Dynamic Learning Strategy Existing multi - modal GRL methods incurred perfor- mance degradation when generating reports from images alone . To address this limitation , the dynamic learning strat- egy was proposed to enhance the generalizability of the model to perform multi - modal generation given any per- centage of text and image inputs . This was achieved through training with various complementary image and text mask ratios ( different mask ratios for each batch ) . This comple- mentary relation aimed to guarantee sufficient shared infor- mation and to ensure that each modality can consistently derive some information from the other . It also incorporated a self - adjustment mechanism for loss weights , dynamically adapting to changes in the mask ratios . The pseudo - code of the dynamic traceback learning is shown in Algorithm 1 . In the subsequent sections , the mathematical rationale be- hind the adjustment of loss weights during"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 7,
    "text": "incorporated a self - adjustment mechanism for loss weights , dynamically adapting to changes in the mask ratios . The pseudo - code of the dynamic traceback learning is shown in Algorithm 1 . In the subsequent sections , the mathematical rationale be- hind the adjustment of loss weights during the forward and traceback stages is presented . \n Algorithm 1 Dynamic Traceback Learning 1 : Generate random numbers : α as image mask ratio and β = 1 −α as text mask ratio 2 : while Not reach max epoch and early stop do 3 : if Forward then 4 : imasked , tmasked ←masking(image , text ) 5 : ifeats , tfeats ←encoders(imasked , tmasked ) 6 : compute ℓFVD and ℓFLD 7 : gradient descent encoders by ( 1 −α ) · ℓFVD and ( 1 −β ) · ℓFLD 8 : feats ←cross modal fusion(ifeats , tfeats ) 9 : igen , tgen ←decoders(feats ) 10 : compute ℓIR and ℓRG 11 : gradient descent all components by α · ℓIR and β · ℓRG 12 : end if 13 : if Traceback then 14 : disable the gradient descent of encoders 15 : igfeats , tgfeats ←encoders(igen , tgen ) 16 : compute ℓTVD and ℓTLD 17 : gradient descent all except encoders by α · e−ℓFVDℓTVD and β · e−ℓFLDℓTVD 18 : end if 19 : end while Assumed the mask ratio of images to be a random num- ber 0 ≤α ≤1 . The corresponding mask ratio of reports would then be β = 1 −α . Forward Stage The forward stage of the DTrace model performs three main tasks : 1 ) disease identification by the encoders , 2 ) image reconstruction by the visual decoder , and 3 ) report generation by the linguistic decoder , as shown in the left of Fig . 4 . For disease identification , we integrated a multi - label classification head for N classes into the encoders to predict disease labels as output . According to the dynamic learn- ing strategy , the image and reports are masked in the for- ward stage based on the dynamic complementary mask ra- tio . Subsequently , the remaining unmasked components are directed to their respective encoders . To optimize the gener- ation of their corresponding label outputs , both the encoders aim to minimize the diagnostic loss . The diagnostic loss is the binary cross - entropy ( BCE ) loss ( see Equation 1 ) be- tween the instances of disease label extracted by CheXbert , y and the encoders ’ corresponding predicted label , ˆy . As a diagnosis can only be meaningful when sufficient unmasked information is available , the visual and linguistic encoders ’ respective diagnostic losses complement their correspond- ing image and text mask ratios . The resulting forward vi- sual diagnostic ( FVD ) loss and forward linguistic diagnostic ( FLD ) loss are shown in Equation 2 and Equation"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 8,
    "text": "sufficient unmasked information is available , the visual and linguistic encoders ’ respective diagnostic losses complement their correspond- ing image and text mask ratios . The resulting forward vi- sual diagnostic ( FVD ) loss and forward linguistic diagnostic ( FLD ) loss are shown in Equation 2 and Equation 3 . ℓBCE(y , ˆy ) = −1 N N X i=1 yi·log(ˆyi)+(1−yi)·log(1−ˆyi ) ( 1 ) ℓFVD(y , ˆy ) = ( 1 −α ) · ℓBCE(y , ˆy ) ( 2 ) ℓFLD(y , ˆy ) = ( 1 −β ) · ℓBCE(y , ˆy ) ( 3 ) For image reconstruction , we follow the MAE ’s prac- tice [ 9 ] in that the visual decoder takes the visual features that gained knowledge from the cross - model fusion mod- ule and learned vector that indicated the presence of the masked patch as input . The image reconstruction process is regulated through minimizing the image reconstruction ( IR ) loss , which is the pixel - wise mean - square - error ( MSE ) weighted proportional to the image mask ratio to facilitate cross - modal communication , as shown in Equation 4 . In Equation 4 , g represents an instance of the origin image and ˆg represents the respective reconstructed image , and W and H represent the width and height of the image , respectively . ℓIR(g , ˆg ) = α · 1 W H W X i=1 H X j=1 ( gij −ˆgij)2 ( 4 ) The generated reports are refined by minimizing the re- port generation ( RG ) loss . The RG loss , as shown in Equa- tion 5 , is the word - level cross - entropy ( CE ) loss weighted corresponding to the text mask ratio to attend to instances where reports are generated solely from images , where r represents an instance of ground truth report , ˆr represents the corresponding generated report , V represents the vocab- ulary size and L represents the report length . ℓRG(r , ˆr ) = β · 1 V L V X i=1 L X j=1 rij · log ( eˆrij PV k=1 eˆrik ) ( 5 ) Traceback Stage As depicted in the right of Fig . 4 , the images reconstructed and reports generated by the decoders were traced back to be the input of locked encoders . The encoders inference on both the reconstructed items ( ˆˆy ) and unmasked items ( ˜y ) . However , this principle was heav- ily dependent on the accuracy of the encoders , which led to significant fluctuations in the early stages of training . This problem was addressed by introducing a compensa- tion mechanism : the weights of traceback losses were in- versely proportional to the losses of the generation process , as shown in Equation 6 and Equation 7 . ℓTVD(˜y , ˆˆy ) = α · e−ℓFVD(˜y,ˆˆy ) · ℓBCE(˜y , ˆˆy )"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 9,
    "text": "problem was addressed by introducing a compensa- tion mechanism : the weights of traceback losses were in- versely proportional to the losses of the generation process , as shown in Equation 6 and Equation 7 . ℓTVD(˜y , ˆˆy ) = α · e−ℓFVD(˜y,ˆˆy ) · ℓBCE(˜y , ˆˆy ) ( 6 ) ℓTLD(˜y , ˆˆy ) = α · e−ℓFLD(˜y,ˆˆy ) · ℓBCE(˜y , ˆˆy ) ( 7 ) \n 4 . Experimental Setup 4.1 . Dataset We conducted experiments on two well - benchmarked public datasets , Indiana University Chest X - ray ( IU- Xray ) [ 5 ] and MIMIC Chest X - ray ( MIMIC - CXR ) [ 12 ] . We split the data into training , validation , and testing subsets . For IU - Xray , we adopt the widely accepted 7:1:2 data split as suggested in prior studies [ 3 , 4 , 32 ] . For MIMIC - CXR , we adhere to the official data split . 4.2 . Evaluation Metrics To measure the quality of generated medical report , we follow the standard practice [ 3 , 4 , 32 , 34 ] to adopt natu- ral language generation ( NLG ) metrics : BLEU [ 21 ] , ME- TEOR [ 13 ] , ROUGE - L [ 15 ] , and CIDEr [ 30 ] as the eval- uation metrics . We further assess the clinical efficacy ( CE ) [ 3 , 4 , 10 , 11 ] of the generated reports by annotat- ing them with CheXbert and comparing the predicted and ground truth labels . 4.3 . Baselines We compared the performance of DTrace against the state - of - the - art medical report generation methods ( see Sec- tion 5.1 ): R2Gen [ 4 ] , R2GenCMN [ 3 ] , CMCL [ 16 ] , Align- Transformer [ 38 ] , XProNet [ 32 ] , MCTransformer [ 34 ] , M2KT [ 37 ] , ORGAN [ 10 ] and KiUT [ 11 ] . For fair evalua- tion , the released code from the baseline methods was used in the same settings as described in the papers . Please see appendix A.1 for implementation details . 5 . Result and Analysis 5.1 . Comparisons to Previous Methods of - the - art methods , highlighting the superior performance of the proposed DTrace across both NLG and clinical efficacy ( CE ) metrics on the IU - Xray and MIMIC - CXR datasets . DTrace consistently ranks among the top per- formers in NLG metrics , particularly excelling in BLEU-3 , BLEU-4 , and CIDEr , which underscores its strong capa- bility in generating morphologically accurate and linguis- tically coherent reports . In terms of CE metrics , DTrace achieves the best results in recall and F1 - score and the second - best in precision , demonstrating its effectiveness in capturing disease regions and generating semantically meaningful"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 10,
    "text": "its strong capa- bility in generating morphologically accurate and linguis- tically coherent reports . In terms of CE metrics , DTrace achieves the best results in recall and F1 - score and the second - best in precision , demonstrating its effectiveness in capturing disease regions and generating semantically meaningful reports . Even in comparison with models like COMG and RGRG , which leverage additional segmenta- tion masks and region - specific information , DTrace remains highly competitive . This superior performance suggests that DTrace is adept at capturing pathology - critical information , making it well- suited for scenarios where radiologists ’ descriptions of the same radiology image vary in writing style and terminology . For example , when comparing the ground truth sentence ” the heart size is normal ” with two variants , ” the heart size is enlarged ” and ” the heart size is within normal limits , ” the cross - entropy is smaller for the first variant . To address this potential bias towards common phrasing , which might sac- rifice semantic accuracy , we introduce a traceback mecha- nism . This mechanism evaluates generated reports using an encoder trained for accurate diagnosis , thus reinforcing se- mantic correctness and diluting the impact of cross - entropy loss . The proposed DTrace outperforms models such as R2GenCMN , AlighTransformer , and XProNet , which we attribute to the benefits of multi - modal learning , particu- larly in facilitating cross - modal communication . The trace- back mechanism further contributes to this performance im- provement . Even when a modality lacks information from another , it can reconstruct its form independently , while the traceback mechanism ensures that semantic information is still obtained from another modality . In the cross - modal fusion module , different modalities exchange and comple- ment each other ’s information , thereby establishing a robust communication protocol . This approach achieves effects similar to R2GenCMN and XProNet , with the added en- hancement of the traceback mechanism , leading to overall improved performance . 5.2 . Ablation Study The ablation study results presented in Table 2 demon- strate the contribution of each individual component to the overall performance of DTrace . Below , we provide a con- cise discussion of the impact of each component of DTrace . Bi - directional Multi - modal Generation : Incorporating bi - directional generation significantly improved the model ’s ability to capture mutual associations between medical im- ages and their corresponding reports . This enhancement led to a notable increase in CIDEr , from 0.143 to 0.241 , and a slight improvement in the F1 score to 0.280 . The bi- directional nature of the approach allows the model to better align the content of the generated reports with visual fea- tures , thereby improving both the relevance and accuracy of the textual output . Dynamic Learning : The introduction of dynamic learning resulted in a significant performance boost across all"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 11,
    "text": "directional nature of the approach allows the model to better align the content of the generated reports with visual fea- tures , thereby improving both the relevance and accuracy of the textual output . Dynamic Learning : The introduction of dynamic learning resulted in a significant performance boost across all met- rics , particularly in BLEU-4 , which increased from 0.107 to 0.120 , and the F1 score , which rose to 0.344 . This compo- nent ’s ability to adapt to varying proportions of image and text inputs enabled the model to generate more contextually accurate reports without over - reliance on one modality . Traceback Mechanism : The traceback mechanism pro- vided the most substantial improvements . By supervising the semantic validity of the generated content , this compo- nent enhanced the model ’s ability to produce clinically acbest results are highlighted in bold and the second best are underlined . BL , MTR , RG - L and CDr are the abbreviations of NLG evaluation metrics BLEU , METEOR , ROUGE - L and CIDEr . P , R , F are the abbreviations of CE metrics : Precision , Recall , F1 - score . Gray indicates the utilization of additional annotations . * denotes that the results are cited from their original papers . Dataset Model BL-1 BL-2 BL-3 BL-4 MTR RG - L CDr P R F IU - Xray R2Gen 0.470 0.304 0.211 0.157 0.197 0.364 0.342 - - - R2GenCMN 0.486 0.307 0.216 0.156 0.212 0.374 0.331 - - - CMCL * 0.473 0.305 0.217 0.162 0.186 0.378 - - - - AlignTransformer * 0.484 0.313 0.225 0.173 0.204 0.379 - - - - MCTransformer * 0.496 0.319 0.241 0.175 - 0.377 0.449 - - - M2KT * 0.497 0.319 0.230 0.174 - 0.399 0.407 - - - ORGAN * 0.494 0.335 0.247 0.190 0.203 0.395 - - - - KiUT 0.525 0.360 0.251 0.185 0.242 0.409 - - - - DTrace ( ours ) 0.516 0.353 0.278 0.204 0.233 0.386 0.469 - - - COMG * 0.482 0.316 0.233 0.184 0.191 0.382 - - - - MIMIC - CXR R2Gen 0.344 0.208 0.140 0.100 0.135 0.271 0.146 0.333 0.273 0.276 R2GenCMN 0.327 0.211 0.148 0.109 0.137 0.298 0.135 0.334 0.275 0.278 CMCL * 0.344 0.217 0.140 0.097 0.133 0.281 - - - - AlignTransformer * 0.378 0.235 0.156 0.112 0.158 0.283 - - - - MCTransformer * 0.351 0.223 0.157 0.118 - 0.287 0.281 - - - M2KT * 0.386 0.237 0.157 0.111 - 0.274 0.111 - - - ORGAN * 0.405 0.254 0.170 0.121 0.161 0.291 - 0.416 0.418 0.385 KiUT 0.393 0.243 0.159 0.113 0.160 0.285 - 0.371 0.318 0.321 DTrace ( ours ) 0.392 0.260 0.171 0.129 0.162 0.309 0.311 0.411 0.436 0.391 COMG * 0.346 0.216 0.145 0.104 0.137 0.279 - 0.424 0.291 0.345 RGRG 0.373 0.249 0.175 0.126 0.168 0.264 0.495 0.461 0.475 0.447 Model BL-1 BL-2 BL-3 BL-4 MTR RG - L CDr P R"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 12,
    "text": "0.321 DTrace ( ours ) 0.392 0.260 0.171 0.129 0.162 0.309 0.311 0.411 0.436 0.391 COMG * 0.346 0.216 0.145 0.104 0.137 0.279 - 0.424 0.291 0.345 RGRG 0.373 0.249 0.175 0.126 0.168 0.264 0.495 0.461 0.475 0.447 Model BL-1 BL-2 BL-3 BL-4 MTR RG - L CDr P R F Uni - modal Report Auto - completion 0.351 0.215 0.141 0.102 0.133 0.264 0.108 0.277 0.244 0.236 Encoder - Decoder ( Baseline ) 0.348 0.212 0.143 0.106 0.136 0.277 0.143 0.325 0.271 0.268 + Bi - directional Generation 0.346 0.220 0.144 0.107 0.142 0.285 0.241 0.345 0.280 0.280 + Dynamic Learning 0.371 0.243 0.165 0.120 0.155 0.281 0.279 0.358 0.355 0.344 + Traceback Mechanism 0.392 0.260 0.171 0.129 0.162 0.309 0.311 0.411 0.436 0.391 Multi - modal Masked Autoencoder 0.364 0.246 0.164 0.119 0.153 0.284 0.292 0.354 0.296 0.301 curate and coherent reports . The BLEU-4 score increased to 0.129 , while CIDEr reached 0.311 , indicating improved alignment with the true medical meaning of the reports . Clinically , the model achieved a precision of 0.411 , recall of 0.436 , and an F1 score of 0.391 . These improvements underscore the traceback mechanism ’s critical role in en- suring that the generated text not only follows the expected structure but also conveys accurate and meaningful clinical information . Understanding of Cross - modal Knowledge : The DTrace framework demonstrates clear superiority over both the baseline Encoder - Decoder model and the Multi - modal Masked Autoencoder . While the baseline model struggles to generate clinically accurate reports , often relying on sta- tistical regularities , DTrace ’s dynamic learning strategy and traceback mechanism ensure that the generated content is both semantically valid and clinically meaningful . Com- pared to the Multi - modal Masked Autoencoder , which inte- grates vision and language modalities but faces challenges in maintaining semantic precision , DTrace excels by dy- namically adapting to varying input proportions and super- vising the clinical relevance of the generated content . \n our method highlighted in bold . Strategy Mask Ratio BL-1 BL-2 BL-3 BL-4 MTR RG - L CDr Fixed 0 % 0.397 0.255 0.183 0.139 0.164 0.367 0.364 15 % 0.441 0.284 0.207 0.158 0.180 0.378 0.427 30 % 0.459 0.293 0.207 0.155 0.176 0.368 0.255 45 % 0.465 0.299 0.211 0.155 0.176 0.367 0.289 60 % 0.468 0.300 0.214 0.159 0.196 0.382 0.308 75 % 0.479 0.307 0.221 0.164 0.195 0.385 0.364 Dynamic varying 0.516 0.353 0.278 0.204 0.233 0.386 0.469 X - Ray Images Reports The heart size remains normal . The mediastinal and hilar contours are unremarkable . Pulmonary vascularity is normal . The lungs are clear without focal consolidation . No pleural effusion or pneumothorax is present . There are mild degenerative changes of the thoracic spine . pathological area Ground Truth R2Gen The lungs are clear without focal consolidation . No pleural effusion or pneumothorax is seen . The cardiac and mediastinal silhouettes are stable . Dual lead left-"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 13,
    "text": "No pleural effusion or pneumothorax is present . There are mild degenerative changes of the thoracic spine . pathological area Ground Truth R2Gen The lungs are clear without focal consolidation . No pleural effusion or pneumothorax is seen . The cardiac and mediastinal silhouettes are stable . Dual lead left- sided pacemaker is stable in position . The mediastinal and hilar contours are unremarkable . Pulmonary vascularity is normal . There are mild degenerative changes of the thoracic spine . pathological area The heart is of normal size with normal cardiomediastinal contours . The pulmonary vasculature is unremarkable . The lungs are clear without focal or diffuse abnormality . No radiopaque foreign body . Osseous structures are unremarkable . No radiopaque foreign bodies . No pleural effusion or pneumothorax is present . There are mild degenerative changes of the thoracic spine . Baseline pathological area R2GenCMN Left - sided AICD device is noted with leads terminating in the right atrium and right ventricle . Mild enlargement of the cardiac silhouette is unchanged . The mediastinal and hilar contours are similar . Pulmonary vasculature is not engorged . No focal consolidation pleural effusion or pneumothorax is present . No acute osseous abnormality is visualized . There are mild degenerative changes of the thoracic spine . pathological area The lungs are well expanded and clear without focal consolidation pleural effusion or pneumothorax . Heart size is normal . Mediastinal silhouette and hilar contours are normal . There is no free air under the diaphragm . There are moderate degenerative changes in the thoracic spine . Pulmonary vascularity is normal . pathological area PA and lateral views of the chest provided . Heart size is normal . No focal consolidation , pleural effusion or pneumothorax is seen . The pulmonary vasculature is not engorged . Median sternotomy wires and mediastinal clips again noted . The mediastinal and hilar contours are unremarkable . There are mild degenerative changes of the thoracic spine . XProNet pathological area framework . The information in the ground truth report is labeled from 1 to 6 and highlighted separately . The generated reports are labeled according to the ground truth report and highlighted with different colors to represent the differences between the generated sequences and the ground truth report : ( 1 ) Green - consistent ; ( 2 ) Blue - semantically similar but different in expression ; ( 3 ) Pink - incorrect information ; ( 4 ) Gray - missing sentences ; 5 ) Unhighlighted - not included in the ground truth . 5.3 . Impact of Mask Ratios To illustrate the significance of dynamic learning in med- ical report generation , we compared traditional methods that use a fixed mask ratio with our proposed method , which employs a varying mask ratio , as shown in Table 3 . Our comparison revealed that the model ’s performance im- proves as the mask ratio increases . Importantly , during training , the model is exposed to"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 14,
    "text": "that use a fixed mask ratio with our proposed method , which employs a varying mask ratio , as shown in Table 3 . Our comparison revealed that the model ’s performance im- proves as the mask ratio increases . Importantly , during training , the model is exposed to both image and text data , whereas , during inference , it processes only the image data . The greater the discrepancy between the information avail- able during training and inference , the worse the model ’s performance tends to be . Our dynamic learning approach effectively mitigates this issue by randomly varying the pro- portions of information from both modalities , thus enhanc- ing the model ’s robustness during inference . 5.4 . Qualitative Analysis and Visualization We conducted a qualitative analysis of DTrace compared to the baseline encoder - decoder framework . As illustrated in Fig.5 , when dealing with rare diseases , the baseline and existing methods frequently omit critical diagnostic state- ments , such as ” mild degenerative changes in the thoracic spine , ” despite having a high degree of textual overlap with the ground truth . In contrast , the DTrace - generated re- ports include most of the essential diagnostic statements and demonstrate a high level of consistency with the ground- truth reports . Additionally , Fig . A3 presents images re- constructed from 75%-masked images and unmasked re- ports by DTrace . These reconstructed images exhibit a high degree of consistency with the original unmasked im- ages , highlighting DTrace ’s effectiveness in image recon- struction . Further visualizations can be found in the supple- mentary materials . We further assessed the morphological and semantic similarities between the constructed images and the origi- nal images . To evaluate the efficacy of our dynamic trace- back learning in reducing pixel - level differences , we con- ducted a control experiment comparing the quality of im- ages generated with and without dynamic traceback learn- ing , as depicted in Fig . A1 . Our analysis revealed a distinct \n boundary between generated patches and original patches in images not reconstructed using the dynamic traceback learning strategy . Although dynamic traceback learning is not specifically designed to enhance morphological similar- ity , the images reconstructed using this method appear more cohesive and clearer . To quantify the quality of the generated images , we com- pared the Frechet Inception Distance ( FID ) scores . Af- ter implementing dynamic traceback learning , the FID of the constructed images decreased from 166.4 to 98.6 , in- dicating that dynamic traceback learning more accurately mimics the distribution of the original images and produces higher - quality , more realistic reconstructions . Additionally , we employed a classification evaluation method to assess the semantic correctness of the reconstructed images . Ta- ble A1 demonstrates that DTrace successfully reconstructs images from 75%-masked images and unmasked text input while preserving semantic information . 6 . Conclusion In this study ,"
  },
  {
    "source": "2401.13267v3.pdf",
    "chunk_index": 15,
    "text": "more realistic reconstructions . Additionally , we employed a classification evaluation method to assess the semantic correctness of the reconstructed images . Ta- ble A1 demonstrates that DTrace successfully reconstructs images from 75%-masked images and unmasked text input while preserving semantic information . 6 . Conclusion In this study , we introduced a novel medical report gen- eration framework , DTrace , which leverages multi - modal dynamic traceback learning . We incorporated a traceback mechanism to ensure semantic correctness during train- ing and a dynamic learning strategy to reduce the depen- dency of existing generative cross - modal frameworks on textual input . Our experimental results demonstrated that both the traceback mechanism and the dynamic learning strategy significantly enhance the multi - modal generation framework , enabling DTrace to achieve state - of - the - art per- formance on the well - benchmarked IU - Xray and MIMIC- CXR datasets ."
  }
]